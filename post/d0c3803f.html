<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/blog/crown.png"><link rel="icon" type="image/png" href="/img/blog/crown.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#2f4154"><meta name="description" content="Nothing is so common as the wish to be remarkables"><meta name="author" content="Bryce Huang"><meta name="keywords" content="blog,program,life"><title>kubernetes 安装 - Bryce&#39;s Club</title><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/dracula.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_fmb4a04yx8h.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css"><link rel="stylesheet" href="https://cdn.staticfile.org/gitalk/1.6.2/gitalk.css"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="Bryce's Club" type="application/atom+xml"></head><body><div id="dark" onclick="switchDarkMode()"></div><script>var isNight=18<=(new Date).getHours()||(new Date).getHours()<8;(matchMedia("(prefers-color-scheme: dark)").matches||isNight||"1"===localStorage.getItem("dark"))&&(isNight&&"1"===localStorage.getItem("noDark")||document.body.classList.add("dark")),document.getElementById("dark").innerHTML=document.querySelector("body").classList.contains("dark")?"🌙":"🌞"</script><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>Bryce's Club</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/links/"><i class="iconfont icon-link-fill"></i> 友链</a></li><li class="nav-item"><a class="nav-link" href="/messageboard/"><i class="iconfont icon-speakernotes"></i> 留言</a></li><li class="nav-item"><a class="nav-link" href="/features/"><i class="iconfont icon-exp-fill"></i> 功能</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i class="iconfont icon-search"></i>&nbsp;&nbsp;</a></li></ul></div></div></nav><div class="view intro-2" id="background" parallax="true" style="background:url(/img/star-guardian/champion-pajama-group-splash.webp) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="container text-center white-text fadeInUp"><span class="h2" id="subtitle"></span><div class="mt-3 post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2020-05-01 17:23">星期五, 五月 1日 2020, 5:23 下午</time></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 22.3k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 384 分钟</span></div></div></div></div></div></header><main><div class="container-fluid"><div class="row"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-md"><div class="container nopadding-md" id="board-ctn"><div class="py-5" id="board"><div class="post-content mx-auto" id="post"><article class="markdown-body"><p><strong>kubernetes ubuntu安装部署</strong></p><h2 id="安装基础环境"><a href="#安装基础环境" class="headerlink" title="安装基础环境"></a><strong>安装基础环境</strong></h2><h3 id="配置系统hosts文件"><a href="#配置系统hosts文件" class="headerlink" title="配置系统hosts文件"></a>配置系统hosts文件</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt;&gt; /etc/hosts &lt;&lt;EOF

10.10.34.92   ukm01
10.10.34.93   ukm02
10.10.34.94   ukm03
10.10.34.95   uki01
10.10.34.96   uki02
10.10.34.97   uki03
10.10.34.98   ukn01
10.10.34.99   ukn02
10.10.34.100  ukn03
EOF</code></pre></div><h3 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h3><div class="hljs"><pre><code class="hljs bash">hostnamectl <span class="hljs-built_in">set</span>-hostname ukm01</code></pre></div><h3 id="配置sshkey"><a href="#配置sshkey" class="headerlink" title="配置sshkey"></a>配置sshkey</h3><h4 id="创建sshkey"><a href="#创建sshkey" class="headerlink" title="创建sshkey"></a>创建sshkey</h4><div class="hljs"><pre><code class="hljs bash"> ssh-keygen -t dsa -b 1024 -C <span class="hljs-string">"seanzhau@mokr.cn"</span>
Generating public/private dsa key pair.
Enter file <span class="hljs-keyword">in</span> <span class="hljs-built_in">which</span> to save the key (/root/.ssh/id_dsa):
Enter passphrase (empty <span class="hljs-keyword">for</span> no passphrase):
Enter same passphrase again:
Your identification has been saved <span class="hljs-keyword">in</span> /root/.ssh/id_dsa.
Your public key has been saved <span class="hljs-keyword">in</span> /root/.ssh/id_dsa.pub.
The key fingerprint is:
SHA256:LaO+CdmwvXlHaIPXvZf0s/XjKxPjVUdtvoHC0PCGEUs seanzhau@mokr.cn
The key<span class="hljs-string">'s randomart image is:</span>
<span class="hljs-string">+---[DSA 1024]----+</span>
<span class="hljs-string">|         E+     .|</span>
<span class="hljs-string">|        ..=.    +|</span>
<span class="hljs-string">|         ooo  .+ |</span>
<span class="hljs-string">|         ..o . .+|</span>
<span class="hljs-string">|    .  .So...   =|</span>
<span class="hljs-string">|     *..=oo .o.o |</span>
<span class="hljs-string">|    + +o o  .o+o.|</span>
<span class="hljs-string">|     o +. . .+oo+|</span>
<span class="hljs-string">|      *o .   .++*|</span>
<span class="hljs-string">+----[SHA256]-----+</span></code></pre></div><h4 id="分发key"><a href="#分发key" class="headerlink" title="分发key"></a>分发key</h4><div class="hljs"><pre><code class="hljs bash"> ssh-copy-id ukm01
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: <span class="hljs-string">"/root/.ssh/id_dsa.pub"</span>
/usr/bin/ssh-copy-id: INFO: attempting to <span class="hljs-built_in">log</span> <span class="hljs-keyword">in</span> with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- <span class="hljs-keyword">if</span> you are prompted now it is to install the new keys

Number of key(s) added: 1
...

 ssh-copy-id ukn03
...</code></pre></div><h4 id="校验"><a href="#校验" class="headerlink" title="校验"></a>校验</h4><div class="hljs"><pre><code class="hljs bash"> ssh ukm01
Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-88-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

 System information disabled due to load higher than 4.0
...

 ssh ukn03
...</code></pre></div><h3 id="配置阿里云-repo"><a href="#配置阿里云-repo" class="headerlink" title="配置阿里云 repo"></a>配置阿里云 repo</h3><div class="hljs"><pre><code class="hljs bash"> mv /etc/apt/sources.list /etc/apt/sources.list.bak

 cat &gt; /etc/apt/sources.list &lt;&lt;EOF
deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse

deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse

deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse

deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse

deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse

EOF</code></pre></div><h3 id="安装基础包"><a href="#安装基础包" class="headerlink" title="安装基础包"></a>安装基础包</h3><p>ubuntu 18.04 默认未安装python，需要每台服务器手动安装。</p><div class="hljs"><pre><code class="hljs bash">apt update &amp;&amp; apt install -y software-properties-common apt-transport-https ca-certificates gnupg2 python curl</code></pre></div><h2 id="安装-ansible"><a href="#安装-ansible" class="headerlink" title="安装 ansible"></a><strong>安装 ansible</strong></h2><p>只需在 master 节点安装</p><div class="hljs"><pre><code class="hljs bash">apt-add-repository --yes --update ppa:ansible/ansible

apt install -y ansible</code></pre></div><h3 id="配置-ansible-cfg"><a href="#配置-ansible-cfg" class="headerlink" title="配置 ansible.cfg"></a>配置 ansible.cfg</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; /etc/ansible/ansible.cfg &lt;&lt;EOF
[defaults]
log_path = /var/<span class="hljs-built_in">log</span>/ansible.log

forks = 20
host_key_checking = False
retry_files_enabled = False
deprecation_warnings = False
nocows = True
remote_user = root
roles_path = roles/
gathering = smart
fact_caching = jsonfile
fact_caching_connection = /etc/ansible/facts
fact_caching_timeout = 600
callback_whitelist = profile_tasks
inventory_ignore_extensions = secrets.py, .pyc, .cfg, .crt, .ini
timeout = 30

[inventory]
unparsed_is_failed=<span class="hljs-literal">true</span>

[ssh_connection]
pipelining = True
ssh_args = -o ControlMaster=auto -o ControlPersist=600s
timeout = 10
control_path = %(directory)s/%%h-%%r
EOF</code></pre></div><h3 id="配置-inventory-hosts"><a href="#配置-inventory-hosts" class="headerlink" title="配置 inventory hosts"></a>配置 inventory hosts</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; /etc/ansible/hosts &lt;&lt;EOF
[ins]
10.10.34.92  hostname=<span class="hljs-string">'ukm01'</span>
10.10.34.93  hostname=<span class="hljs-string">'ukm02'</span>
10.10.34.94  hostname=<span class="hljs-string">'ukm03'</span>
10.10.34.95  hostname=<span class="hljs-string">'uki01'</span>
10.10.34.96  hostname=<span class="hljs-string">'uki02'</span>
10.10.34.97  hostname=<span class="hljs-string">'uki03'</span>
10.10.34.98  hostname=<span class="hljs-string">'ukn01'</span>
10.10.34.99  hostname=<span class="hljs-string">'ukn02'</span>
10.10.34.100 hostname=<span class="hljs-string">'ukn03'</span> ansible_ssh_user=<span class="hljs-string">'root'</span> ansible_ssh_pass=<span class="hljs-string">'xxxxxxx'</span>

[ans]
10.10.34.92

[master]
10.10.34.9[2:4]

[infra]
10.10.34.9[5:7]

[worker]
10.10.34.9[8:9]
10.10.34.100
EOF</code></pre></div><h3 id="验证-ansible-配置"><a href="#验证-ansible-配置" class="headerlink" title="验证 ansible 配置"></a>验证 ansible 配置</h3><div class="hljs"><pre><code class="hljs bash"> ansible ins -m ping
10.10.34.92 | SUCCESS =&gt; &#123;
    <span class="hljs-string">"ansible_facts"</span>: &#123;
        <span class="hljs-string">"discovered_interpreter_python"</span>: <span class="hljs-string">"/usr/bin/python"</span>
    &#125;,
    <span class="hljs-string">"changed"</span>: <span class="hljs-literal">false</span>,
    <span class="hljs-string">"ping"</span>: <span class="hljs-string">"pong"</span>
&#125;
...</code></pre></div><h3 id="集群节点绑定主机名"><a href="#集群节点绑定主机名" class="headerlink" title="集群节点绑定主机名"></a>集群节点绑定主机名</h3><div class="hljs"><pre><code class="hljs bash"> ansible ins:\!ans -m shell -a <span class="hljs-string">'cat &gt;&gt; /etc/hosts &lt;&lt;EOF</span>
<span class="hljs-string">10.10.34.92   ukm01</span>
<span class="hljs-string">10.10.34.93   ukm02</span>
<span class="hljs-string">10.10.34.94   ukm03</span>
<span class="hljs-string">10.10.34.95   uki01</span>
<span class="hljs-string">10.10.34.96   uki02</span>
<span class="hljs-string">10.10.34.97   uki03</span>
<span class="hljs-string">10.10.34.98   ukn01</span>
<span class="hljs-string">10.10.34.99   ukn02</span>
<span class="hljs-string">10.10.34.100  ukn03</span>
<span class="hljs-string">EOF'</span>

 ansible ins:\!ans -m shell -a <span class="hljs-string">'cat /etc/hosts'</span></code></pre></div><h3 id="分发配置"><a href="#分发配置" class="headerlink" title="分发配置"></a>分发配置</h3><div class="hljs"><pre><code class="hljs bash">分发私钥
ansible master -m copy -a <span class="hljs-string">'src=/root/.ssh/id_rsa dest=/root/.ssh/id_rsa mode=0600 owner=root'</span>

分发 ansible 配置
ansible master -m copy -a <span class="hljs-string">'src=/etc/ansible/hosts dest=/etc/ansible/hosts'</span>

ansible master -m copy -a <span class="hljs-string">'src=/etc/ansible/ansible.cfg dest=/etc/ansible/ansible.cfg'</span></code></pre></div><h2 id="安装-Docker-CE"><a href="#安装-Docker-CE" class="headerlink" title="安装 Docker CE"></a><strong>安装 Docker CE</strong></h2><p>添加 Docker GPG key</p><div class="hljs"><pre><code class="hljs bash">ansible ins -m shell -a <span class="hljs-string">'curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -'</span></code></pre></div><h3 id="配置-Docker-源"><a href="#配置-Docker-源" class="headerlink" title="配置 Docker 源"></a>配置 Docker 源</h3><ul><li>动态识别操作系统版本 <code>add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;</code></li></ul><div class="hljs"><pre><code class="hljs bash">ansible ins -m shell -a <span class="hljs-string">'add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic stable"'</span></code></pre></div><h3 id="安装-Docker"><a href="#安装-Docker" class="headerlink" title="安装 Docker"></a>安装 Docker</h3><div class="hljs"><pre><code class="hljs bash">ansible ins -m shell -a <span class="hljs-string">'apt update &amp;&amp; apt install -y docker-ce docker-ce-cli containerd.io'</span></code></pre></div><h3 id="配置-docker-daemon"><a href="#配置-docker-daemon" class="headerlink" title="配置 docker daemon"></a>配置 docker daemon</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; /etc/docker/daemon.json &lt;&lt;EOF
&#123;
  <span class="hljs-string">"exec-opts"</span>: [<span class="hljs-string">"native.cgroupdriver=systemd"</span>],
  <span class="hljs-string">"log-driver"</span>: <span class="hljs-string">"json-file"</span>,
  <span class="hljs-string">"log-opts"</span>: &#123;
    <span class="hljs-string">"max-size"</span>: <span class="hljs-string">"100m"</span>,
    <span class="hljs-string">"max-file"</span>: <span class="hljs-string">"5"</span>
  &#125;,
  <span class="hljs-string">"registry-mirrors"</span>: [<span class="hljs-string">"https://docker.mirrors.ustc.edu.cn"</span>],
  <span class="hljs-string">"storage-driver"</span>: <span class="hljs-string">"overlay2"</span>
&#125;
EOF</code></pre></div><h3 id="分发-daemon-json"><a href="#分发-daemon-json" class="headerlink" title="分发 daemon.json"></a>分发 daemon.json</h3><div class="hljs"><pre><code class="hljs bash">ansible ins -m copy -a <span class="hljs-string">'src=/etc/docker/daemon.json dest=/etc/docker/daemon.json'</span>

ansible ins -m shell -a <span class="hljs-string">'mkdir -p /etc/systemd/system/docker.service.d'</span></code></pre></div><h3 id="重启-docker-服务"><a href="#重启-docker-服务" class="headerlink" title="重启 docker 服务"></a>重启 docker 服务</h3><div class="hljs"><pre><code class="hljs bash">ansible ins -m shell -a <span class="hljs-string">'systemctl daemon-reload'</span>

ansible ins -m shell -a <span class="hljs-string">'systemctl restart docker'</span>

ansible ins -m shell -a <span class="hljs-string">'docker info|grep -i cgroup'</span>
Cgroup Driver: systemd</code></pre></div><h3 id="daemon-json-配置详解"><a href="#daemon-json-配置详解" class="headerlink" title="daemon.json 配置详解"></a>daemon.json 配置详解</h3><div class="hljs"><pre><code class="hljs bash"> cat /etc/docker/daemon.json
&#123;
  <span class="hljs-string">"authorization-plugins"</span>: [],
  <span class="hljs-string">"data-root"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"dns"</span>: [],
  <span class="hljs-string">"dns-opts"</span>: [],
  <span class="hljs-string">"dns-search"</span>: [],
  <span class="hljs-string">"exec-opts"</span>: [],
  <span class="hljs-string">"exec-root"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"experimental"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"features"</span>: &#123;&#125;,
  <span class="hljs-string">"storage-driver"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"storage-opts"</span>: [],
  <span class="hljs-string">"labels"</span>: [],
  <span class="hljs-string">"live-restore"</span>: <span class="hljs-literal">true</span>,
  <span class="hljs-string">"log-driver"</span>: <span class="hljs-string">"json-file"</span>,
  <span class="hljs-string">"log-opts"</span>: &#123;
    <span class="hljs-string">"max-size"</span>: <span class="hljs-string">"10m"</span>,
    <span class="hljs-string">"max-file"</span>:<span class="hljs-string">"5"</span>,
    <span class="hljs-string">"labels"</span>: <span class="hljs-string">"somelabel"</span>,
    <span class="hljs-string">"env"</span>: <span class="hljs-string">"os,customer"</span>
  &#125;,
  <span class="hljs-string">"mtu"</span>: 0,
  <span class="hljs-string">"pidfile"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"cluster-store"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"cluster-store-opts"</span>: &#123;&#125;,
  <span class="hljs-string">"cluster-advertise"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"max-concurrent-downloads"</span>: 3,
  <span class="hljs-string">"max-concurrent-uploads"</span>: 5,
  <span class="hljs-string">"default-shm-size"</span>: <span class="hljs-string">"64M"</span>,
  <span class="hljs-string">"shutdown-timeout"</span>: 15,
  <span class="hljs-string">"debug"</span>: <span class="hljs-literal">true</span>,
  <span class="hljs-string">"hosts"</span>: [],
  <span class="hljs-string">"log-level"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"tls"</span>: <span class="hljs-literal">true</span>,
  <span class="hljs-string">"tlsverify"</span>: <span class="hljs-literal">true</span>,
  <span class="hljs-string">"tlscacert"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"tlscert"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"tlskey"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"swarm-default-advertise-addr"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"api-cors-header"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"selinux-enabled"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"userns-remap"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"group"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"cgroup-parent"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"default-ulimits"</span>: &#123;
    <span class="hljs-string">"nofile"</span>: &#123;
      <span class="hljs-string">"Name"</span>: <span class="hljs-string">"nofile"</span>,
      <span class="hljs-string">"Hard"</span>: 64000,
      <span class="hljs-string">"Soft"</span>: 64000
    &#125;
  &#125;,
  <span class="hljs-string">"init"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"init-path"</span>: <span class="hljs-string">"/usr/libexec/docker-init"</span>,
  <span class="hljs-string">"ipv6"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"iptables"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"ip-forward"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"ip-masq"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"userland-proxy"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"userland-proxy-path"</span>: <span class="hljs-string">"/usr/libexec/docker-proxy"</span>,
  <span class="hljs-string">"ip"</span>: <span class="hljs-string">"0.0.0.0"</span>,
  <span class="hljs-string">"bridge"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"bip"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"fixed-cidr"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"fixed-cidr-v6"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"default-gateway"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"default-gateway-v6"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"icc"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"raw-logs"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"allow-nondistributable-artifacts"</span>: [],
  <span class="hljs-string">"registry-mirrors"</span>: [],
  <span class="hljs-string">"seccomp-profile"</span>: <span class="hljs-string">""</span>,
  <span class="hljs-string">"insecure-registries"</span>: [],
  <span class="hljs-string">"no-new-privileges"</span>: <span class="hljs-literal">false</span>,
  <span class="hljs-string">"default-runtime"</span>: <span class="hljs-string">"runc"</span>,
  <span class="hljs-string">"oom-score-adjust"</span>: -500,
  <span class="hljs-string">"node-generic-resources"</span>: [<span class="hljs-string">"NVIDIA-GPU=UUID1"</span>, <span class="hljs-string">"NVIDIA-GPU=UUID2"</span>],
  <span class="hljs-string">"runtimes"</span>: &#123;
    <span class="hljs-string">"cc-runtime"</span>: &#123;
      <span class="hljs-string">"path"</span>: <span class="hljs-string">"/usr/bin/cc-runtime"</span>
    &#125;,
    <span class="hljs-string">"custom"</span>: &#123;
      <span class="hljs-string">"path"</span>: <span class="hljs-string">"/usr/local/bin/my-runc-replacement"</span>,
      <span class="hljs-string">"runtimeArgs"</span>: [
        <span class="hljs-string">"--debug"</span>
      ]
    &#125;
  &#125;,
  <span class="hljs-string">"default-address-pools"</span>:[
    &#123;<span class="hljs-string">"base"</span>:<span class="hljs-string">"172.80.0.0/16"</span>,<span class="hljs-string">"size"</span>:24&#125;,
    &#123;<span class="hljs-string">"base"</span>:<span class="hljs-string">"172.90.0.0/16"</span>,<span class="hljs-string">"size"</span>:24&#125;
  ]
&#125;</code></pre></div><h3 id="格式化查看-images"><a href="#格式化查看-images" class="headerlink" title="格式化查看 images"></a>格式化查看 images</h3><ul><li>.ID Image ID</li><li>.Repository Image 仓库地址和名称</li><li>.Tag Image 标签</li><li>.Digest Image hash 值信息</li><li>.CreatedSince Image 创建到现在的时间</li><li>.CreatedAt Image 创建的具体时间</li><li>.Size Image 大小</li></ul><div class="hljs"><pre><code class="hljs bash">docker images --format <span class="hljs-string">"table &#123;&#123; .Repository &#125;&#125; \t &#123;&#123; .Tag &#125;&#125; \t &#123;&#123; .ID &#125;&#125; \t &#123;&#123; .CreatedSince &#125;&#125; \t &#123;&#123; .CreatedAt &#125;&#125; \t &#123;&#123; .Size &#125;&#125;"</span> --digests</code></pre></div><h3 id="docker-pull-过程"><a href="#docker-pull-过程" class="headerlink" title="docker pull 过程"></a>docker pull 过程</h3><ul><li>Docker 客户端发送 REPOSITORY 和 TAG（或者 DIGEST）给 Registry；</li><li>Registry 根据 REPOSITORY 和 TAG（或者 DIGEST）找到相应 image manifest；</li><li>Registry 将 manifest 返回给 Docker 客户端；</li><li>Docker 客户端根据 manifest 读取 image 的 digest(sha256) ， sha256 就是 IMAGE ID；</li><li>Docker 客户端根据 ID 查找本地是否存在对应的 image；</li><li>如果 image 存在则创建对应的 image REPOSITORY 和 TAG；</li><li>如果 image 不存在，则继续请求 Registry 获取 image 的配置文件；</li><li>根据 image 配置文件中的 diff_ids 在本地找对应的 layer 是否存在；</li><li>如果 layer 存在，则返回 Already exists；</li><li>如果 layer 不存在，则根据 manifest 中 layer 的 sha256 和 media type 去服务器下载对应的 layer；</li><li>下载解压之后，校验 tar 包的 sha256 是否和 Image Config 中的 diff_id 一致。</li></ul><h3 id="image本地存放信息"><a href="#image本地存放信息" class="headerlink" title="image本地存放信息"></a>image本地存放信息</h3><div class="hljs"><pre><code class="hljs bash"> 查看 image DIGEST 信息
 docker images --digests
REPOSITORY                    TAG                 DIGEST                                                                    IMAGE ID            CREATED             SIZE
busybox                       latest              sha256:a8cf7ff6367c2afa2a90acd081b484cbded349a7076e7bdf37a05279f276bc12   be5888e67be6        2 weeks ago         1.22MB

 如果是其他的文件系统的话，名字会是其他的，比如btrfs、overlay2、devicemapper等。
 cat /var/lib/docker/image/overlay2/repositories.json | python -m json.tool
&#123;
    <span class="hljs-string">"Repositories"</span>: &#123;
        <span class="hljs-string">"busybox"</span>: &#123;
            <span class="hljs-string">"busybox:latest"</span>: <span class="hljs-string">"sha256:be5888e67be651f1fbb59006f0fd791b44ed3fceaa6323ab4e37d5928874345a"</span>,
            <span class="hljs-string">"busybox@sha256:a8cf7ff6367c2afa2a90acd081b484cbded349a7076e7bdf37a05279f276bc12"</span>: <span class="hljs-string">"sha256:be5888e67be651f1fbb59006f0fd791b44ed3fceaa6323ab4e37d5928874345a"</span>
        &#125;
    &#125;
&#125;

 校验 image config 的 sha256 值
 sha256sum /var/lib/docker/image/overlay2/imagedb/content/sha256/be5888e67be651f1fbb59006f0fd791b44ed3fceaa6323ab4e37d5928874345a
be5888e67be651f1fbb59006f0fd791b44ed3fceaa6323ab4e37d5928874345a  /var/lib/docker/image/overlay2/imagedb/content/sha256/be5888e67be651f1fbb59006f0fd791b44ed3fceaa6323ab4e37d5928874345a

 查看 image config
 cat /var/lib/docker/image/overlay2/imagedb/content/sha256/be5888e67be651f1fbb59006f0fd791b44ed3fceaa6323ab4e37d5928874345a | python -m json.tool
&#123;
    <span class="hljs-string">"architecture"</span>: <span class="hljs-string">"amd64"</span>,
    <span class="hljs-string">"config"</span>: &#123;
        <span class="hljs-string">"ArgsEscaped"</span>: <span class="hljs-literal">true</span>,
        <span class="hljs-string">"AttachStderr"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"AttachStdin"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"AttachStdout"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"Cmd"</span>: [
            <span class="hljs-string">"sh"</span>
        ],
        <span class="hljs-string">"Domainname"</span>: <span class="hljs-string">""</span>,
        <span class="hljs-string">"Entrypoint"</span>: null,
        <span class="hljs-string">"Env"</span>: [
            <span class="hljs-string">"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"</span>
        ],
        <span class="hljs-string">"Hostname"</span>: <span class="hljs-string">""</span>,
        <span class="hljs-string">"Image"</span>: <span class="hljs-string">"sha256:b0acc7ebf5092fcdd0fe097448529147e6619bd051f03ccf25b29bcae87e783f"</span>,
        <span class="hljs-string">"Labels"</span>: null,
        <span class="hljs-string">"OnBuild"</span>: null,
        <span class="hljs-string">"OpenStdin"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"StdinOnce"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"Tty"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"User"</span>: <span class="hljs-string">""</span>,
        <span class="hljs-string">"Volumes"</span>: null,
        <span class="hljs-string">"WorkingDir"</span>: <span class="hljs-string">""</span>
    &#125;,
    <span class="hljs-string">"container"</span>: <span class="hljs-string">"f7e67f16a539f8bbf53aae18cdb5f8c53e6a56930e7660010d9396ae77f7acfa"</span>,
    <span class="hljs-string">"container_config"</span>: &#123;
        <span class="hljs-string">"ArgsEscaped"</span>: <span class="hljs-literal">true</span>,
        <span class="hljs-string">"AttachStderr"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"AttachStdin"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"AttachStdout"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"Cmd"</span>: [
            <span class="hljs-string">"/bin/sh"</span>,
            <span class="hljs-string">"-c"</span>,
            <span class="hljs-string">"(nop) "</span>,
            <span class="hljs-string">"CMD [\"sh\"]"</span>
        ],
        <span class="hljs-string">"Domainname"</span>: <span class="hljs-string">""</span>,
        <span class="hljs-string">"Entrypoint"</span>: null,
        <span class="hljs-string">"Env"</span>: [
            <span class="hljs-string">"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"</span>
        ],
        <span class="hljs-string">"Hostname"</span>: <span class="hljs-string">"f7e67f16a539"</span>,
        <span class="hljs-string">"Image"</span>: <span class="hljs-string">"sha256:b0acc7ebf5092fcdd0fe097448529147e6619bd051f03ccf25b29bcae87e783f"</span>,
        <span class="hljs-string">"Labels"</span>: &#123;&#125;,
        <span class="hljs-string">"OnBuild"</span>: null,
        <span class="hljs-string">"OpenStdin"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"StdinOnce"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"Tty"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"User"</span>: <span class="hljs-string">""</span>,
        <span class="hljs-string">"Volumes"</span>: null,
        <span class="hljs-string">"WorkingDir"</span>: <span class="hljs-string">""</span>
    &#125;,
    <span class="hljs-string">"created"</span>: <span class="hljs-string">"2020-04-14T19:19:53.590635493Z"</span>,
    <span class="hljs-string">"docker_version"</span>: <span class="hljs-string">"18.09.7"</span>,
    <span class="hljs-string">"history"</span>: [
        &#123;
            <span class="hljs-string">"created"</span>: <span class="hljs-string">"2020-04-14T19:19:53.444488372Z"</span>,
            <span class="hljs-string">"created_by"</span>: <span class="hljs-string">"/bin/sh -c (nop) ADD file:09a89925137e1b768ef1f0e7d1d7325eb2b4f1a0895b3aa8dfc98b0c75f3f507 in / "</span>
        &#125;,
        &#123;
            <span class="hljs-string">"created"</span>: <span class="hljs-string">"2020-04-14T19:19:53.590635493Z"</span>,
            <span class="hljs-string">"created_by"</span>: <span class="hljs-string">"/bin/sh -c (nop)  CMD [\"sh\"]"</span>,
            <span class="hljs-string">"empty_layer"</span>: <span class="hljs-literal">true</span>
        &#125;
    ],
    <span class="hljs-string">"os"</span>: <span class="hljs-string">"linux"</span>,
    <span class="hljs-string">"rootfs"</span>: &#123;
        <span class="hljs-string">"diff_ids"</span>: [
            <span class="hljs-string">"sha256:5b0d2d635df829f65d0ffb45eab2c3124a470c4f385d6602bda0c21c5248bcab"</span>
        ],
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"layers"</span>
    &#125;
&#125;

 layer diff_id 和 digest 的对应关系
 diffid-by-digest： 存放 digest 到 diffid 的对应关系
 v2metadata-by-diffid： 存放 diffid 到 digest 的对应关系
 tree /var/lib/docker/image/overlay2/distribution/
/var/lib/docker/image/overlay2/distribution/
├── diffid-by-digest
│   └── sha256
│       ├── e2334dd9fee4b77e48a8f2d793904118a3acf26f1f2e72a3d79c6cae993e07f0
│       └── ec237e1426f0de497da8c0951d5db1d2b7f155d207cfc0df02f3fb0508a3036a
└── v2metadata-by-diffid
    └── sha256
        ├── 5b0d2d635df829f65d0ffb45eab2c3124a470c4f385d6602bda0c21c5248bcab
        └── f53db92a80829925f8ec12396ab68569d7a769d9ab94983b86dbe6b26ae3d1fa

 根据 diffid 获取 digest
~ cat /var/lib/docker/image/overlay2/distribution/v2metadata-by-diffid/sha256/5b0d2d635df829f65d0ffb45eab2c3124a470c4f385d6602bda0c21c5248bcab | python -m json.tool
[
    &#123;
        <span class="hljs-string">"Digest"</span>: <span class="hljs-string">"sha256:e2334dd9fee4b77e48a8f2d793904118a3acf26f1f2e72a3d79c6cae993e07f0"</span>,
        <span class="hljs-string">"HMAC"</span>: <span class="hljs-string">""</span>,
        <span class="hljs-string">"SourceRepository"</span>: <span class="hljs-string">"docker.io/library/busybox"</span>
    &#125;
]

 根据 digest 获取 diffid
 cat /var/lib/docker/image/overlay2/distribution/diffid-by-digest/sha256/e2334dd9fee4b77e48a8f2d793904118a3acf26f1f2e72a3d79c6cae993e07f0
sha256:5b0d2d635df829f65d0ffb45eab2c3124a470c4f385d6602bda0c21c5248bcab

 根据 digest 查看 layer 元数据
 <span class="hljs-built_in">cd</span> /var/lib/docker/image/overlay2/layerdb/sha256/5b0d2d635df829f65d0ffb45eab2c3124a470c4f385d6602bda0c21c5248bcab/ &amp;&amp; ls -l
total 24
-rw-r--r-- 1 root root    64 Apr 30 15:21 cache-id
-rw-r--r-- 1 root root    71 Apr 30 15:21 diff
-rw-r--r-- 1 root root     7 Apr 30 15:21 size
-rw-r--r-- 1 root root 12159 Apr 30 15:21 tar-split.json.gz

 cache-id 是 docker 下载 layer 的时候随机生成 uuid，指向真正存放 layer 文件的位置。
 cat cache-id
486d3aacd2ee026b1e3db99b23e32a9f60d0972f7224fd53657a41943c871872

 diff 文件存放 layer 的 diffid
 cat diff
sha256:5b0d2d635df829f65d0ffb45eab2c3124a470c4f385d6602bda0c21c5248bcab

 size 当前 layer 的大小，单位是字节。
 tar-split.json.gz 是 layer 的 split 文件，通过这个文件可以还原 layer 的 tar 包，在 docker save 导出 image 时会用到。

 layer 数据，image 存储数据，使用 cache-id 访问。
 <span class="hljs-built_in">cd</span> /var/lib/docker/overlay2/486d3aacd2ee026b1e3db99b23e32a9f60d0972f7224fd53657a41943c871872
 tree -d diff/
diff/
├── bin
├── dev
├── etc
│   └── network
│       ├── <span class="hljs-keyword">if</span>-down.d
│       ├── <span class="hljs-keyword">if</span>-post-down.d
│       ├── <span class="hljs-keyword">if</span>-pre-up.d
│       └── <span class="hljs-keyword">if</span>-up.d
├── home
├── root
├── tmp
├── usr
│   └── sbin
└── var
    ├── spool
    │   └── mail
    └── www

 manifest 存储对 config 和 layer 的 sha256 + media <span class="hljs-built_in">type</span> 描述，目的就是为了下载 config 和 layer。下载完成后，manifest 的使命就完成了，所以在本地没有存储 manifest 文件。</code></pre></div><h3 id="docker-空间管理"><a href="#docker-空间管理" class="headerlink" title="docker 空间管理"></a>docker 空间管理</h3><div class="hljs"><pre><code class="hljs bash"> 查看空间
 docker system df
TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE
Images              11                  9                   1.084GB             202.9MB (18%)
Containers          18                  12                  10.61kB             0B (0%)
Local Volumes       0                   0                   0B                  0B
Build Cache         0                   0                   0B                  0B

 清理文件系统
 docker system prune
WARNING! This will remove:
  - all stopped containers
  - all networks not used by at least one container
  - all dangling images
  - all dangling build cache

Are you sure you want to <span class="hljs-built_in">continue</span>? [y/N]

 强制清理，忽略依赖关系。
 docker system prune -a -f

 清理所有状态为 &lt;none&gt; 的 image
 docker image prune
WARNING! This will remove all images without at least one container associated to them.
Are you sure you want to <span class="hljs-built_in">continue</span>? [y/N]

 强制清理 image，忽略依赖关系。
 docker image prune -a
WARNING! This will remove all images without at least one container associated to them.
Are you sure you want to <span class="hljs-built_in">continue</span>? [y/N]

 清理无用数据卷
 docker volume ls
 docker volume prune
WARNING! This will remove all <span class="hljs-built_in">local</span> volumes not used by at least one container.
Are you sure you want to <span class="hljs-built_in">continue</span>? [y/N]

 清理所有停止的容器
 docker container prune
WARNING! This will remove all stopped containers.
Are you sure you want to <span class="hljs-built_in">continue</span>? [y/N]</code></pre></div><h2 id="配置系统"><a href="#配置系统" class="headerlink" title="配置系统"></a><strong>配置系统</strong></h2><p>内核关闭 swap</p><div class="hljs"><pre><code class="hljs bash">ansible ins -m shell -a <span class="hljs-string">'echo "vm.swappiness = 0" &gt;&gt; /etc/sysctl.conf &amp;&amp; sysctl -p'</span></code></pre></div><p>零时关闭 swap</p><div class="hljs"><pre><code class="hljs bash">ansible ins -m shell -a <span class="hljs-string">'swapoff -a'</span></code></pre></div><p>系统 fstab 关闭配置</p><div class="hljs"><pre><code class="hljs bash">ansible ins -m shell -a <span class="hljs-string">"sed -i 's/^[^#].*swap*/#&amp;/g'  /etc/fstab &amp;&amp; chattr +i /etc/fstab"</span></code></pre></div><ul><li>WARNING: No swap limit support</li></ul><div class="hljs"><pre><code class="hljs bash">sed -i <span class="hljs-string">'/GRUB_CMDLINE_LINUX=/cGRUB_CMDLINE_LINUX="net.ifnames=0 biosdevname=0 console=tty0 console=ttyS0,115200 cgroup_enable=memory swapaccount=1"'</span> /etc/default/grub

ansible ins -m copy -a <span class="hljs-string">'src=/etc/default/grub dest=/etc/default/grub'</span>

ansible ins -m shell -a <span class="hljs-string">'update-grub &amp;&amp; shutdown -r +1 "reboot for turnoff swap"'</span></code></pre></div><h2 id="安装-kubernetes"><a href="#安装-kubernetes" class="headerlink" title="安装 kubernetes"></a><strong>安装 kubernetes</strong></h2><ul><li>激活旧版本的防火墙配置（19年之后的操作系统需要修改）</li></ul><div class="hljs"><pre><code class="hljs bash">apt install -y iptables arptables ebtables

update-alternatives --<span class="hljs-built_in">set</span> iptables /usr/sbin/iptables-legacy

update-alternatives --<span class="hljs-built_in">set</span> ip6tables /usr/sbin/ip6tables-legacy

update-alternatives --<span class="hljs-built_in">set</span> arptables /usr/sbin/arptables-legacy

update-alternatives --<span class="hljs-built_in">set</span> ebtables /usr/sbin/ebtables-legacy</code></pre></div><h2 id="配置-kubernetes-repo"><a href="#配置-kubernetes-repo" class="headerlink" title="配置 kubernetes repo"></a>配置 kubernetes repo</h2><div class="hljs"><pre><code class="hljs bash">批量配置
ansible ins -m shell -a <span class="hljs-string">'curl -fsSL https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -'</span>

ansible ins -m shell -a <span class="hljs-string">'add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main"'</span></code></pre></div><h2 id="安装-kubernetes-基础工具"><a href="#安装-kubernetes-基础工具" class="headerlink" title="安装 kubernetes 基础工具"></a>安装 kubernetes 基础工具</h2><div class="hljs"><pre><code class="hljs bash">查看当前支持的版本信息
apt-cache policy kubeadm

ansible ins -m shell -a <span class="hljs-string">'apt update &amp;&amp; apt install -y kubelet=1.17.3-00 kubeadm=1.17.3-00 kubectl=1.17.3-00'</span></code></pre></div><h2 id="配置-driver"><a href="#配置-driver" class="headerlink" title="配置 driver"></a>配置 driver</h2><div class="hljs"><pre><code class="hljs bash"> cat &gt; /etc/default/kubelet &lt;&lt;EOF
KUBELET_EXTRA_ARGS=--cgroup-driver=systemd
EOF

 cat &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf &lt;&lt;EOF
[Service]
Environment=<span class="hljs-string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroup-driver=systemd"</span>
Environment=<span class="hljs-string">"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"</span>
<span class="hljs-comment"># 这是 "kubeadm init" 和 "kubeadm join" 运行时生成的文件，动态地填充 KUBELET_KUBEADM_ARGS 变量</span>
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
<span class="hljs-comment"># 这是一个文件，用户在不得已下可以将其用作替代 kubelet args。</span>
<span class="hljs-comment"># 用户最好使用 .NodeRegistration.KubeletExtraArgs 对象在配置文件中替代。</span>
<span class="hljs-comment"># KUBELET_EXTRA_ARGS 应该从此文件中获取。</span>
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet <span class="hljs-variable">$KUBELET_KUBECONFIG_ARGS</span> <span class="hljs-variable">$KUBELET_CONFIG_ARGS</span> <span class="hljs-variable">$KUBELET_KUBEADM_ARGS</span> <span class="hljs-variable">$KUBELET_EXTRA_ARGS</span>

EOF

ansible ins -m copy -a <span class="hljs-string">'src=/etc/default/kubelet dest=/etc/default/kubelet'</span>
ansible ins -m copy -a <span class="hljs-string">'src=/etc/systemd/system/kubelet.service.d/10-kubeadm.conf dest=/etc/systemd/system/kubelet.service.d/10-kubeadm.conf'</span></code></pre></div><h3 id="重启-kubelet"><a href="#重启-kubelet" class="headerlink" title="重启 kubelet"></a>重启 kubelet</h3><div class="hljs"><pre><code class="hljs bash">ansible ins -m shell -a <span class="hljs-string">'systemctl daemon-reload &amp;&amp; systemctl restart kubelet'</span></code></pre></div><h2 id="开启-ipvs-支持"><a href="#开启-ipvs-支持" class="headerlink" title="开启 ipvs 支持"></a>开启 ipvs 支持</h2><h3 id="配置内核支持-IP-转发"><a href="#配置内核支持-IP-转发" class="headerlink" title="配置内核支持 IP 转发"></a>配置内核支持 IP 转发</h3><div class="hljs"><pre><code class="hljs bash">ansible ins -m shell -a <span class="hljs-string">'grep net.ipv4.ip_forward /etc/sysctl.conf &amp;&amp; sed -i "/net.ipv4.ip_forward/c\net.ipv4.ip_forward=1" /etc/sysctl.conf || echo "net.ipv4.ip_forward=1" &gt;&gt; /etc/sysctl.conf'</span>
ansible ins -m shell -a <span class="hljs-string">'grep net.bridge.bridge-nf-call-iptables /etc/sysctl.conf &amp;&amp; sed -i "/net.bridge.bridge-nf-call-iptables/c\net.bridge.bridge-nf-call-iptables=1" /etc/sysctl.conf || echo "net.bridge.bridge-nf-call-iptables=1" &gt;&gt; /etc/sysctl.conf'</span>
ansible ins -m shell -a <span class="hljs-string">'grep net.bridge.bridge-nf-call-ip6tables /etc/sysctl.conf &amp;&amp; sed -i "/net.bridge.bridge-nf-call-ip6tables/c\net.bridge.bridge-nf-call-ip6tables=1" /etc/sysctl.conf || echo "net.bridge.bridge-nf-call-ip6tables=1" &gt;&gt; /etc/sysctl.conf'</span>

ansible ins -m shell -a <span class="hljs-string">'sysctl -p'</span></code></pre></div><h3 id="配置内核加载-ipvs-模块"><a href="#配置内核加载-ipvs-模块" class="headerlink" title="配置内核加载 ipvs 模块"></a>配置内核加载 ipvs 模块</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; /tmp/ipvs.modules &lt;&lt;EOF
<span class="hljs-meta">#!/bin/bash</span>
ipvs_modules=<span class="hljs-string">"ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4"</span>
<span class="hljs-keyword">for</span> kernel_module <span class="hljs-keyword">in</span> \<span class="hljs-variable">$&#123;ipvs_modules&#125;</span>; <span class="hljs-keyword">do</span>
    /sbin/modinfo -F filename \<span class="hljs-variable">$&#123;kernel_module&#125;</span> &gt; /dev/null 2&gt;&amp;1
    <span class="hljs-keyword">if</span> [ $? -eq 0 ]; <span class="hljs-keyword">then</span>
        /sbin/modprobe \<span class="hljs-variable">$&#123;kernel_module&#125;</span>
    <span class="hljs-keyword">fi</span>
<span class="hljs-keyword">done</span>
EOF

 ansible ins -m copy -a <span class="hljs-string">'src=/tmp/ipvs.modules dest=/tmp/ipvs.modules mode=0755'</span>
 ansible ins -m shell -a <span class="hljs-string">'/tmp/ipvs.modules'</span>

 验证 ipvs 支持
 ansible ins -m shell -a <span class="hljs-string">'lsmod | grep ip_vs'</span></code></pre></div><h3 id="安装-ipvsadm"><a href="#安装-ipvsadm" class="headerlink" title="安装 ipvsadm"></a>安装 ipvsadm</h3><div class="hljs"><pre><code class="hljs bash">ansible ins -m yum -a <span class="hljs-string">'name=ipvsadm state=present'</span></code></pre></div><h2 id="创建-kubernetes-集群"><a href="#创建-kubernetes-集群" class="headerlink" title="创建 kubernetes 集群"></a><strong>创建 kubernetes 集群</strong></h2><p>ukm01 上执行</p><ul><li>内网负载均衡地址 10.10.34.89</li></ul><div class="hljs"><pre><code class="hljs bash"> 配置文件创建，可以修改较多的参数。
 cat &gt; kubeadm-config.yaml &lt;&lt;EOF
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 0.0.0.0
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  kubeletExtraArgs:
    cgroup-driver: <span class="hljs-string">"systemd"</span>
  ignorePreflightErrors:
  - IsPrivilegedUser
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
controlPlaneEndpoint: 192.168.0.113:6443
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
dns:
  <span class="hljs-built_in">type</span>: CoreDNS
apiServer:
  timeoutForControlPlane: 5m0s
  extraArgs:
    authorization-mode: <span class="hljs-string">"Node,RBAC"</span>
  certSANs:
  - <span class="hljs-string">"192.168.0.113"</span>
  - <span class="hljs-string">"192.168.0.110"</span>
  - <span class="hljs-string">"192.168.0.111"</span>
  - <span class="hljs-string">"192.168.0.112"</span>
  - <span class="hljs-string">"node1"</span>
  - <span class="hljs-string">"node2"</span>
  - <span class="hljs-string">"node3"</span>
  - <span class="hljs-string">"kubernetes"</span>
  - <span class="hljs-string">"kubernetes.default"</span>
  - <span class="hljs-string">"kubernetes.default.svc"</span>
  - <span class="hljs-string">"kubernetes.default.svc.cluster"</span>
  - <span class="hljs-string">"kubernetes.default.svc.cluster.local"</span>
controllerManager:
  extraArgs:
    <span class="hljs-string">"node-cidr-mask-size"</span>: <span class="hljs-string">"20"</span>
scheduler:
  extraArgs:
    address: <span class="hljs-string">"0.0.0.0"</span>
dns:
  <span class="hljs-built_in">type</span>: CoreDNS
etcd:
  <span class="hljs-built_in">local</span>:
    dataDir: /var/lib/etcd
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kubernetesVersion: v1.20.1
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
  podSubnet: 192.168.0.0/16
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: 0.0.0.0
mode: ipvs   
ipvs:
  scheduler: wlc
  syncPeriod: 30s
  minSyncPeriod: 5s
  tcpTimeout: 0s
  tcpFinTimeout: 0s
  udpTimeout: 0s
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: systemd
rotateCertificates: <span class="hljs-literal">true</span>
EOF


 验证 kubeadm-config 配置
 kubeadm init --config=kubeadm-config.yaml --upload-certs

 创建集群
 kubeadm init --config=kubeadm-config.yaml --upload-certs

 参数化创建集群
 kubeadm init \
  --apiserver-bind-port 8443 \
  --control-plane-endpoint <span class="hljs-string">"10.10.34.89:8443"</span> \
  --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
  --kubernetes-version=v1.17.3 \
  --upload-certs \
  --pod-network-cidr=10.100.0.0/16 \
  --service-cidr=10.96.0.0/12 \

...
kubeadm join 10.10.34.89:8443 --token 7nf8wp.cefgemzgn5xcdmln \
    --discovery-token-ca-cert-hash sha256:36b328e0052c7f8e7cac826d70b21bef0cdc4e0505fddc8200c52b8b01605da3 \
    --control-plane --certificate-key bd9fc1600d2db4f2b323a17b2b7be4f4234d7ed8705bc64976dcd43d929a24a8
...
kubeadm join 10.10.34.89:8443 --token 7nf8wp.cefgemzgn5xcdmln \
    --discovery-token-ca-cert-hash sha256:36b328e0052c7f8e7cac826d70b21bef0cdc4e0505fddc8200c52b8b01605da3</code></pre></div><h3 id="kubeadm-init-常用参数"><a href="#kubeadm-init-常用参数" class="headerlink" title="kubeadm init 常用参数"></a>kubeadm init 常用参数</h3><div class="hljs"><pre><code class="hljs bash">--apiserver-advertise-address string
API 服务器所公布的其正在监听的 IP 地址。如果未设置，则使用默认网络接口。
--apiserver-bind-port int32     默认值：6443
API 服务器绑定的端口。
--apiserver-cert-extra-sans stringSlice
用于 API Server 服务证书的可选附加主题备用名称（SAN）。可以是 IP 地址和 DNS 名称。
--cert-dir string     默认值：<span class="hljs-string">"/etc/kubernetes/pki"</span>
保存和存储证书的路径。
--certificate-key string
用于加密 kubeadm-certs Secret 中的控制平面证书的密钥。
--config string
kubeadm 配置文件的路径。
--control-plane-endpoint string
为控制平面指定一个稳定的 IP 地址或 DNS 名称。
--cri-socket string
要连接的 CRI 套接字的路径。如果为空，则 kubeadm 将尝试自动检测此值；仅当安装了多个 CRI 或具有非标准 CRI 插槽时，才使用此选项。
--dry-run
不要应用任何更改；只是输出将要执行的操作。
-k, --experimental-kustomize string
用于存储 kustomize 为静态 pod 清单所提供的补丁的路径。
--feature-gates string
一组用来描述各种功能特性的键值（key=value）对。选项是：
IPv6DualStack=<span class="hljs-literal">true</span>|<span class="hljs-literal">false</span> (ALPHA - default=<span class="hljs-literal">false</span>)
-h, --<span class="hljs-built_in">help</span>
init 操作的帮助命令
--ignore-preflight-errors stringSlice
错误将显示为警告的检查列表；例如：<span class="hljs-string">'IsPrivilegedUser,Swap'</span>。取值为 <span class="hljs-string">'all'</span> 时将忽略检查中的所有错误。
--image-repository string     默认值：<span class="hljs-string">"k8s.gcr.io"</span>
选择用于拉取控制平面镜像的容器仓库
--kubernetes-version string     默认值：<span class="hljs-string">"stable-1"</span>
为控制平面选择一个特定的 Kubernetes 版本。
--node-name string
指定节点的名称。
--pod-network-cidr string
指明 pod 网络可以使用的 IP 地址段。如果设置了这个参数，控制平面将会为每一个节点自动分配 CIDRs。
--service-cidr string     默认值：<span class="hljs-string">"10.96.0.0/12"</span>
为服务的虚拟 IP 地址另外指定 IP 地址段
--service-dns-domain string     默认值：<span class="hljs-string">"cluster.local"</span>
为服务另外指定域名，例如：<span class="hljs-string">"myorg.internal"</span>。
--skip-certificate-key-print
不要打印用于加密控制平面证书的密钥。
--skip-phases stringSlice
要跳过的阶段列表
--skip-token-print
跳过打印 <span class="hljs-string">'kubeadm init'</span> 生成的默认引导令牌。
--token string
这个令牌用于建立控制平面节点与工作节点间的双向通信。格式为 [a-z0-9]&#123;6&#125;\.[a-z0-9]&#123;16&#125; - 示例：abcdef.0123456789abcdef
--token-ttl duration     默认值：24h0m0s
令牌被自动删除之前的持续时间（例如 1 s，2 m，3 h）。如果设置为 <span class="hljs-string">'0'</span>，则令牌将永不过期
--upload-certs
将控制平面证书上传到 kubeadm-certs Secret。</code></pre></div><h3 id="master-节点扩容"><a href="#master-节点扩容" class="headerlink" title="master 节点扩容"></a>master 节点扩容</h3><p>在 ukm02 ukm03 上执行</p><div class="hljs"><pre><code class="hljs bash">ansible master:\!ans -m shell -a <span class="hljs-string">'kubeadm join 10.10.34.89:8443 \</span>
<span class="hljs-string">   --apiserver-bind-port 8443 \</span>
<span class="hljs-string">   --control-plane \</span>
<span class="hljs-string">   --token 7nf8wp.cefgemzgn5xcdmln \</span>
<span class="hljs-string">   --discovery-token-ca-cert-hash sha256:36b328e0052c7f8e7cac826d70b21bef0cdc4e0505fddc8200c52b8b01605da3 \</span>
<span class="hljs-string">   --control-plane --certificate-key bd9fc1600d2db4f2b323a17b2b7be4f4234d7ed8705bc64976dcd43d929a24a8'</span></code></pre></div><h3 id="kubeadm-join-常用参数"><a href="#kubeadm-join-常用参数" class="headerlink" title="kubeadm join 常用参数"></a>kubeadm join 常用参数</h3><div class="hljs"><pre><code class="hljs bash">--apiserver-advertise-address string
如果该节点托管一个新的控制平面实例，则 API 服务器将公布其正在侦听的 IP 地址。如果未设置，则使用默认网络接口。
--apiserver-bind-port int32     默认值: 6443
如果节点应该托管新的控制平面实例，则为 API 服务器要绑定的端口。
--certificate-key string
使用此密钥可以解密由 init 上传的证书 secret。
--config string
kubeadm 配置文件的路径。
--control-plane
在此节点上创建一个新的控制平面实例
--cri-socket string
要连接的 CRI 套接字的路径。如果为空，则 kubeadm 将尝试自动检测此值；仅当安装了多个 CRI 或具有非标准 CRI 插槽时，才使用此选项。
--discovery-file string
对于基于文件的发现，给出用于加载集群信息的文件或者 URL。
--discovery-token string
对于基于令牌的发现，该令牌用于验证从 API 服务器获取的集群信息。
--discovery-token-ca-cert-hash stringSlice
对基于令牌的发现，验证根 CA 公钥是否与此哈希匹配 (格式: <span class="hljs-string">"&lt;type&gt;:&lt;value&gt;"</span>)。
--discovery-token-unsafe-skip-ca-verification
对于基于令牌的发现，允许在未关联 --discovery-token-ca-cert-hash 参数的情况下添加节点。
-k, --experimental-kustomize string
用于存储 kustomize 为静态 pod 清单所提供的补丁的路径。
-h, --<span class="hljs-built_in">help</span>
join 操作的帮助命令
--ignore-preflight-errors stringSlice
错误将显示为警告的检查列表；例如：<span class="hljs-string">'IsPrivilegedUser,Swap'</span>。取值为 <span class="hljs-string">'all'</span> 时将忽略检查中的所有错误。
--node-name string
指定节点的名称
--skip-phases stringSlice
要跳过的阶段列表
--tls-bootstrap-token string
指定在加入节点时用于临时通过 Kubernetes 控制平面进行身份验证的令牌。
--token string
如果未提供这些值，则将它们用于 discovery-token 令牌和 tls-bootstrap 令牌。</code></pre></div><p>配置 master 管理命令</p><div class="hljs"><pre><code class="hljs bash">ansible master -m shell -a <span class="hljs-string">'echo "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bashrc'</span>

ansible master -m shell -a <span class="hljs-string">'mkdir -p /root/.kube'</span>

ansible master -m copy -a <span class="hljs-string">'src=/etc/kubernetes/admin.conf dest=/root/.kube/config mode=0600 owner=root remote_src=yes'</span></code></pre></div><h3 id="验证-master"><a href="#验证-master" class="headerlink" title="验证 master"></a>验证 master</h3><div class="hljs"><pre><code class="hljs bash"> kubectl get nodes -l node-role.kubernetes.io/master
NAME    STATUS     ROLES    AGE     VERSION
ukm01   NotReady   master   3m26s   v1.17.3
ukm02   NotReady   master   1m56s   v1.17.3
ukm03   NotReady   master   2m14s   v1.17.3

 kubectl get pods --all-namespaces
NAMESPACE     NAME                            READY   STATUS    RESTARTS   AGE
kube-system   coredns-7f9c544f75-h9hmm        0/1     Pending   0          3m55s
kube-system   coredns-7f9c544f75-k9pb7        0/1     Pending   0          3m55s
kube-system   etcd-ukm01                      1/1     Running   1          20s
kube-system   etcd-ukm02                      1/1     Running   0          40s
kube-system   etcd-ukm03                      1/1     Running   0          20s
kube-system   kube-apiserver-ukm01            1/1     Running   0          20s
kube-system   kube-apiserver-ukm02            1/1     Running   0          20s
kube-system   kube-apiserver-ukm03            1/1     Running   0          20s
kube-system   kube-controller-manager-ukm01   1/1     Running   3          5m17s
kube-system   kube-controller-manager-ukm02   1/1     Running   0          74s
kube-system   kube-controller-manager-ukm03   1/1     Running   2          3m27s
kube-system   kube-proxy-4fkkx                1/1     Running   0          3m27s
kube-system   kube-proxy-5vjpc                1/1     Running   0          2m9s
kube-system   kube-proxy-plx2x                1/1     Running   0          3m55s
kube-system   kube-scheduler-ukm01            1/1     Running   2          5m17s
kube-system   kube-scheduler-ukm02            1/1     Running   0          75s
kube-system   kube-scheduler-ukm03            1/1     Running   0          3m26s</code></pre></div><h3 id="确认-kube-proxy-启用了-ipvs-模式"><a href="#确认-kube-proxy-启用了-ipvs-模式" class="headerlink" title="确认 kube-proxy 启用了 ipvs 模式"></a>确认 kube-proxy 启用了 ipvs 模式</h3><div class="hljs"><pre><code class="hljs bash"> kubectl get configmaps kube-proxy -n kube-system -o yaml
...
    iptables:
      masqueradeAll: <span class="hljs-literal">false</span>
      masqueradeBit: null
      minSyncPeriod: 0s
      syncPeriod: 0s
    ipvs:
      excludeCIDRs: null
      minSyncPeriod: 5s
      scheduler: wlc
      strictARP: <span class="hljs-literal">false</span>
      syncPeriod: 30s
      tcpFinTimeout: 0s
      tcpTimeout: 0s
      udpTimeout: 0s
    kind: KubeProxyConfiguration
    metricsBindAddress: <span class="hljs-string">""</span>
    mode: ipvs
...</code></pre></div><h3 id="查看-ipvs-转发情况"><a href="#查看-ipvs-转发情况" class="headerlink" title="查看 ipvs 转发情况"></a>查看 ipvs 转发情况</h3><div class="hljs"><pre><code class="hljs bash"> ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.96.0.1:443 wlc
  -&gt; 10.10.34.92:8443             Masq    1      0          0
  -&gt; 10.10.34.93:8443             Masq    1      0          0
  -&gt; 10.10.34.94:8443             Masq    1      0          0
TCP  10.96.0.10:53 wlc
TCP  10.96.0.10:9153 wlc
UDP  10.96.0.10:53 wlc</code></pre></div><h3 id="查看网卡信息"><a href="#查看网卡信息" class="headerlink" title="查看网卡信息"></a>查看网卡信息</h3><div class="hljs"><pre><code class="hljs bash"> ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether fa:16:3e:cf:28:7f brd ff:ff:ff:ff:ff:ff
    inet 10.10.34.92/24 brd 10.10.34.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::f816:3eff:fecf:287f/64 scope link
       valid_lft forever preferred_lft forever
3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:81:5a:e2:7c brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:81ff:fe5a:e27c/64 scope link
       valid_lft forever preferred_lft forever
6: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
46: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default
    link/ether 6a:10:8f:81:3e:61 brd ff:ff:ff:ff:ff:ff
    inet 10.96.0.10/32 brd 10.96.0.10 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.0.1/32 brd 10.96.0.1 scope global kube-ipvs0
       valid_lft forever preferred_lft forever</code></pre></div><h3 id="更新-etcd-集群配置"><a href="#更新-etcd-集群配置" class="headerlink" title="更新 etcd 集群配置"></a>更新 etcd 集群配置</h3><div class="hljs"><pre><code class="hljs bash">ansible master -m shell -a <span class="hljs-string">'sed -i "/--initial-cluster=/c\    - --initial-cluster=ukm01=https://10.10.34.92:2380,ukm02=https://10.10.34.93:2380,ukm03=https://10.10.34.94:2380" /etc/kubernetes/manifests/etcd.yaml'</span>

apiserver 默认访问本地的 etcd 节点，如果需要访问集群所有节点可以修改配置。
ansible master -m shell -a <span class="hljs-string">'sed -i "/- --etcd-servers/c\    - --etcd-servers=https://10.10.34.92:2379,https://10.10.34.93:2379,https://10.10.34.94:2379" /etc/kubernetes/manifests/kube-apiserver.yaml'</span></code></pre></div><h3 id="查看-etcd-成员状态"><a href="#查看-etcd-成员状态" class="headerlink" title="查看 etcd 成员状态"></a>查看 etcd 成员状态</h3><p>成员管理命令：</p><ul><li>add - 添加集群成员</li><li>list - 查看集群成员信息</li><li>promote - 提升集群成员权限</li><li>remove - 从集群中删除成员ID</li><li>update - 更新集群成员信息</li></ul><div class="hljs"><pre><code class="hljs bash"> kubectl <span class="hljs-built_in">exec</span> -n kube-system -it $(kubectl get pod  -n kube-system -l component=etcd -o jsonpath=<span class="hljs-string">'&#123;.items[0].metadata.name&#125;'</span>) sh

 或者下载 etcdctl
 curl -L -O https://github.com/etcd-io/etcd/releases/download/v3.4.3/etcd-v3.4.3-linux-amd64.tar.gz
 tar xzvf etcd-v3.4.3-linux-amd64.tar.gz
 mv etcd-v3.4.3-linux-amd64/etcd* /usr/<span class="hljs-built_in">local</span>/bin/

 ETCDCTL=3 etcdctl --write-out=table \
  --cert /etc/kubernetes/pki/etcd/peer.crt \
  --key /etc/kubernetes/pki/etcd/peer.key \
  --cacert /etc/kubernetes/pki/etcd/ca.crt \
  --endpoints https://ukm01:2379,https://ukm02:2379,https://ukm03:2379 \
  member list

+------------------+---------+-------+--------------------------+--------------------------+------------+
|        ID        | STATUS  | NAME  |        PEER ADDRS        |       CLIENT ADDRS       | IS LEARNER |
+------------------+---------+-------+--------------------------+--------------------------+------------+
| 7a212ff0c04f1e82 | started | ukm02 | https://10.10.34.93:2380 | https://10.10.34.93:2379 |      <span class="hljs-literal">false</span> |
| ae758756c0c6b09c | started | ukm01 | https://10.10.34.92:2380 | https://10.10.34.92:2379 |      <span class="hljs-literal">false</span> |
| ff90415307bdc341 | started | ukm03 | https://10.10.34.94:2380 | https://10.10.34.94:2379 |      <span class="hljs-literal">false</span> |
+------------------+---------+-------+--------------------------+--------------------------+------------+</code></pre></div><h3 id="验证-etcd-集群状态"><a href="#验证-etcd-集群状态" class="headerlink" title="验证 etcd 集群状态"></a>验证 etcd 集群状态</h3><div class="hljs"><pre><code class="hljs bash"> ETCDCTL=3 etcdctl --cluster --write-out=table \
  --cert /etc/kubernetes/pki/etcd/peer.crt \
  --key /etc/kubernetes/pki/etcd/peer.key \
  --cacert /etc/kubernetes/pki/etcd/ca.crt \
  --endpoints https://ukm01:2379,https://ukm02:2379,https://ukm03:2379 \
  endpoint status

+--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT         |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| https://10.10.34.94:2379 | 4677521bc1465d7e |   3.4.3 |  1.5 MB |     <span class="hljs-literal">false</span> |      <span class="hljs-literal">false</span> |         9 |       1671 |               1671 |        |
| https://10.10.34.92:2379 | ae758756c0c6b09c |   3.4.3 |  1.6 MB |     <span class="hljs-literal">false</span> |      <span class="hljs-literal">false</span> |         9 |       1671 |               1671 |        |
| https://10.10.34.93:2379 | ff3c0a769521e2c2 |   3.4.3 |  1.5 MB |      <span class="hljs-literal">true</span> |      <span class="hljs-literal">false</span> |         9 |       1671 |               1671 |        |
+--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</code></pre></div><h3 id="查看-etcd-节点健康状况"><a href="#查看-etcd-节点健康状况" class="headerlink" title="查看 etcd 节点健康状况"></a>查看 etcd 节点健康状况</h3><div class="hljs"><pre><code class="hljs bash"> ETCDCTL=3 etcdctl --cluster --write-out=table \
  --cert /etc/kubernetes/pki/etcd/peer.crt \
  --key /etc/kubernetes/pki/etcd/peer.key \
  --cacert /etc/kubernetes/pki/etcd/ca.crt \
  --endpoints https://ukm01:2379,https://ukm02:2379,https://ukm03:2379 \
  endpoint health

+--------------------------+--------+-------------+-------+
|         ENDPOINT         | HEALTH |    TOOK     | ERROR |
+--------------------------+--------+-------------+-------+
| https://10.10.34.93:2379 |   <span class="hljs-literal">true</span> | 18.384799ms |       |
| https://10.10.34.94:2379 |   <span class="hljs-literal">true</span> | 18.649393ms |       |
| https://10.10.34.92:2379 |   <span class="hljs-literal">true</span> | 18.831792ms |       |
+--------------------------+--------+-------------+-------+</code></pre></div><h3 id="增加-node-节点"><a href="#增加-node-节点" class="headerlink" title="增加 node 节点"></a>增加 node 节点</h3><p>在 uki01 uki02 uki03 ukn01 ukn02 ukn03 上执行</p><div class="hljs"><pre><code class="hljs bash">ansible infra:worker -m shell -a <span class="hljs-string">'kubeadm join 10.10.34.89:8443 \</span>
<span class="hljs-string">   --token 7nf8wp.cefgemzgn5xcdmln \</span>
<span class="hljs-string">   --discovery-token-ca-cert-hash sha256:36b328e0052c7f8e7cac826d70b21bef0cdc4e0505fddc8200c52b8b01605da3'</span></code></pre></div><h2 id="SDN"><a href="#SDN" class="headerlink" title="SDN"></a><strong>SDN</strong></h2><h3 id="安装-calico"><a href="#安装-calico" class="headerlink" title="安装 calico"></a>安装 calico</h3><div class="hljs"><pre><code class="hljs bash">curl -L https://docs.projectcalico.org/manifests/calico.yaml -o calico.yaml

kubectl apply -f calico.yaml</code></pre></div><h3 id="查看服务状态"><a href="#查看服务状态" class="headerlink" title="查看服务状态"></a>查看服务状态</h3><ul><li>node 节点处于 Ready 状态</li><li>每个节点上都有一个 pod：calico-node</li></ul><div class="hljs"><pre><code class="hljs bash"> kubectl get nodes
NAME    STATUS   ROLES    AGE     VERSION
uki01   Ready    infra    5m      v1.17.3
uki02   Ready    infra    5m      v1.17.3
uki03   Ready    infra    5m      v1.17.3
ukm01   Ready    master   8m      v1.17.3
ukm02   Ready    master   8m      v1.17.3
ukm03   Ready    master   8m      v1.17.3
ukn01   Ready    worker   5m      v1.17.3
ukn02   Ready    worker   5m      v1.17.3
ukn03   Ready    worker   5m      v1.17.3

 watch kubectl get pods -n kube-system -o wide
NAME                                       READY   STATUS    RESTARTS   AGE   IP                NODE    NOMINATED NODE   READINESS GATES
...
calico-kube-controllers-5b644bc49c-7xjzr   1/1     Running   0          30s   192.168.144.138   uki01   &lt;none&gt;           &lt;none&gt;
calico-node-57rns                          1/1     Running   0          30s   10.10.34.98       ukn01   &lt;none&gt;           &lt;none&gt;
calico-node-6p8z7                          1/1     Running   0          30s   10.10.34.100      ukn03   &lt;none&gt;           &lt;none&gt;
calico-node-77jwm                          1/1     Running   0          30s   172.17.0.1        ukn02   &lt;none&gt;           &lt;none&gt;
calico-node-c4gxf                          1/1     Running   0          30s   10.10.34.95       uki01   &lt;none&gt;           &lt;none&gt;
calico-node-gdb5j                          1/1     Running   0          30s   10.10.34.96       uki02   &lt;none&gt;           &lt;none&gt;
calico-node-gmzmf                          1/1     Running   0          30s   10.10.34.93       ukm02   &lt;none&gt;           &lt;none&gt;
calico-node-jdd2f                          1/1     Running   0          30s   10.10.34.97       uki03   &lt;none&gt;           &lt;none&gt;
calico-node-mrr49                          1/1     Running   0          30s   10.10.34.92       ukm01   &lt;none&gt;           &lt;none&gt;
calico-node-w5b7r                          1/1     Running   0          30s   10.10.34.94       ukm03   &lt;none&gt;           &lt;none&gt;
...

 kubectl get ippool -o wide
NAME                  AGE
default-ipv4-ippool   2m</code></pre></div><h3 id="验证-calico"><a href="#验证-calico" class="headerlink" title="验证 calico"></a>验证 calico</h3><div class="hljs"><pre><code class="hljs bash"> 下载 calicoctl
 curl -L -o /usr/<span class="hljs-built_in">local</span>/bin/calicoctl https://github.com/projectcalico/calicoctl/releases/download/v3.11.2/calicoctl-linux-amd64
 chmod +x /usr/<span class="hljs-built_in">local</span>/bin/calicoctl

 使用 kubernetes api 存储数据
 mkdir -p /etc/calico
 cat &gt; /etc/calico/calicoctl.cfg &lt;&lt;EOF
apiVersion: projectcalico.org/v3
kind: CalicoAPIConfig
metadata:
spec:
  datastoreType: kubernetes
  kubeconfig: /etc/kubernetes/admin.conf   使用 admin 配置
  k8sAPIEndpoint: https://10.10.34.89:8443
  k8sCertFile: /etc/kubernetes/pki/apiserver-kubelet-client.crt
  k8sKeyFile: /etc/kubernetes/pki/apiserver-kubelet-client.key
  k8sCAFile: /etc/kubernetes/pki/ca.crt
EOF

 使用独立的 etcd 存储数据
 mkdir -p /etc/calico
 cat &gt; /etc/calico/calicoctl.cfg &lt;&lt;EOF
apiVersion: projectcalico.org/v3
kind: CalicoAPIConfig
metadata:
spec:
  datastoreType: etcdv3
  etcdEndpoints: https://10.10.34.92:2379,https://10.10.34.93:2379,https://10.10.34.94:2379
  etcdKeyFile: /etc/kubernetes/pki/etcd/peer.key
  etcdCertFile: /etc/kubernetes/pki/etcd/peer.crt
  etcdCACertFile: /etc/kubernetes/pki/etcd/ca.crt
EOF

 查看 node
 calicoctl node status
Calico process is running.

IPv4 BGP status
+--------------+-------------------+-------+------------+-------------+
| PEER ADDRESS |     PEER TYPE     | STATE |   SINCE    |    INFO     |
+--------------+-------------------+-------+------------+-------------+
| 10.10.34.95  | node-to-node mesh | up    | 2020-04-11 | Established |
| 10.10.34.96  | node-to-node mesh | up    | 2020-04-11 | Established |
| 10.10.34.97  | node-to-node mesh | up    | 2020-04-11 | Established |
| 10.10.34.93  | node-to-node mesh | up    | 2020-04-11 | Established |
| 10.10.34.94  | node-to-node mesh | up    | 2020-04-11 | Established |
| 10.10.34.98  | node-to-node mesh | up    | 2020-04-11 | Established |
| 10.10.34.99  | node-to-node mesh | up    | 2020-04-11 | Established |
| 10.10.34.100 | node-to-node mesh | up    | 2020-04-11 | Established |
+--------------+-------------------+-------+------------+-------------+

IPv6 BGP status
No IPv6 peers found.


 calicoctl get node -o wide
NAME    ASN       IPV4              IPV6
uki01   (64512)   10.10.34.95/24
uki02   (64512)   10.10.34.96/24
uki03   (64512)   10.10.34.97/24
ukm01   (64512)   10.10.34.92/24
ukm02   (64512)   10.10.34.93/24
ukm03   (64512)   10.10.34.94/24
ukn01   (64512)   10.10.34.98/24
ukn02   (64512)   10.10.34.99/24
ukn03   (64512)   10.10.34.100/24

 查看 ippool
 calicoctl get ippool -o wide
NAME                  CIDR             NAT    IPIPMODE   VXLANMODE   DISABLED   SELECTOR
default-ipv4-ippool   192.168.0.0/16   <span class="hljs-literal">true</span>   Always     Never       <span class="hljs-literal">false</span>      all()

 查看 ip 状态
 calicoctl ipam show
+----------+----------------+-----------+------------+--------------+
| GROUPING |      CIDR      | IPS TOTAL | IPS IN USE |   IPS FREE   |
+----------+----------------+-----------+------------+--------------+
| IP Pool  | 192.168.0.0/16 |     65536 | 5  (0%)    | 65531 (100%) |
+----------+----------------+-----------+------------+--------------+

 查看已分配的服务节点信息
 calicoctl get workloadendpoints -o wide -n kube-system
NAMESPACE     NAME                                                          WORKLOAD                                   NODE    NETWORKS             INTERFACE         PROFILES                                                  NATS
kube-system   ukn02-k8s-calico--kube--controllers--75d56dfc47--xfdp9-eth0   calico-kube-controllers-75d56dfc47-xfdp9   ukn02   192.168.33.65/32     calif1d8798ea15   kns.kube-system,ksa.kube-system.calico-kube-controllers
kube-system   uki02-k8s-coredns--546565776c--mkx9n-eth0                     coredns-546565776c-mkx9n                   uki02   192.168.225.1/32     calib399f7859db   kns.kube-system,ksa.kube-system.coredns
kube-system   ukn03-k8s-coredns--546565776c--v9wgw-eth0                     coredns-546565776c-v9wgw                   ukn03   192.168.188.129/32   cali4fffc74a7b2   kns.kube-system,ksa.kube-system.coredns</code></pre></div><h3 id="calico-网络资源"><a href="#calico-网络资源" class="headerlink" title="calico 网络资源"></a>calico 网络资源</h3><p>官方文档： <code>https://docs.projectcalico.org/reference/resources/overview</code></p><ul><li>BGPConfiguration</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">BGPConfiguration</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">default</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">logSeverityScreen:</span> <span class="hljs-string">Info</span>
  <span class="hljs-attr">nodeToNodeMeshEnabled:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">asNumber:</span> <span class="hljs-number">63400</span>
  <span class="hljs-attr">serviceClusterIPs:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">cidr:</span> <span class="hljs-number">10.96</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/12</span>
  <span class="hljs-attr">serviceExternalIPs:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">cidr:</span> <span class="hljs-number">104.244</span><span class="hljs-number">.42</span><span class="hljs-number">.129</span><span class="hljs-string">/32</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">cidr:</span> <span class="hljs-number">172.217</span><span class="hljs-number">.3</span><span class="hljs-number">.0</span><span class="hljs-string">/24</span></code></pre></div><ul><li>BGPPeer</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">BGPPeer</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">some.name</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">node:</span> <span class="hljs-string">rack1-host1</span>
  <span class="hljs-attr">peerIP:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.1</span><span class="hljs-number">.1</span>
  <span class="hljs-attr">asNumber:</span> <span class="hljs-number">63400</span></code></pre></div><ul><li>FelixConfiguration</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">FelixConfiguration</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">default</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">ipv6Support:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">ipipMTU:</span> <span class="hljs-number">1400</span>
  <span class="hljs-attr">chainInsertMode:</span> <span class="hljs-string">Append</span></code></pre></div><ul><li>GlobalNetworkPolicy</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">GlobalNetworkPolicy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">allow-tcp-6379</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">selector:</span> <span class="hljs-string">role</span> <span class="hljs-string">==</span> <span class="hljs-string">'database'</span>
  <span class="hljs-attr">types:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Ingress</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Egress</span>
  <span class="hljs-attr">ingress:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">action:</span> <span class="hljs-string">Allow</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">annotations:</span>
        <span class="hljs-attr">from:</span> <span class="hljs-string">frontend</span>
        <span class="hljs-attr">to:</span> <span class="hljs-string">database</span>
    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
    <span class="hljs-attr">source:</span>
      <span class="hljs-attr">selector:</span> <span class="hljs-string">role</span> <span class="hljs-string">==</span> <span class="hljs-string">'frontend'</span>
    <span class="hljs-attr">destination:</span>
      <span class="hljs-attr">ports:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-number">6379</span>
  <span class="hljs-attr">egress:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">action:</span> <span class="hljs-string">Allow</span></code></pre></div><ul><li>GlobalNetworkSet</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">GlobalNetworkSet</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">a-name-for-the-set</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">role:</span> <span class="hljs-string">external-database</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">nets:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-number">198.51</span><span class="hljs-number">.100</span><span class="hljs-number">.0</span><span class="hljs-string">/28</span>
  <span class="hljs-bullet">-</span> <span class="hljs-number">203.0</span><span class="hljs-number">.113</span><span class="hljs-number">.0</span><span class="hljs-string">/24</span></code></pre></div><ul><li>HostEndpoint</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">HostEndpoint</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">some.name</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">type:</span> <span class="hljs-string">production</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">interfaceName:</span> <span class="hljs-string">eth0</span>
  <span class="hljs-attr">node:</span> <span class="hljs-string">myhost</span>
  <span class="hljs-attr">expectedIPs:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-number">192.168</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>
  <span class="hljs-bullet">-</span> <span class="hljs-number">192.168</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span>
  <span class="hljs-attr">profiles:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">profile1</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">profile2</span>
  <span class="hljs-attr">ports:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">some-port</span>
    <span class="hljs-attr">port:</span> <span class="hljs-number">1234</span>
    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">another-port</span>
    <span class="hljs-attr">port:</span> <span class="hljs-number">5432</span>
    <span class="hljs-attr">protocol:</span> <span class="hljs-string">UDP</span></code></pre></div><ul><li>IPPool</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">IPPool</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">my.ippool-1</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">cidr:</span> <span class="hljs-number">10.1</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/16</span>
  <span class="hljs-attr">ipipMode:</span> <span class="hljs-string">CrossSubnet</span>
  <span class="hljs-attr">natOutgoing:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>
  <span class="hljs-attr">nodeSelector:</span> <span class="hljs-string">all()</span></code></pre></div><ul><li>NetworkPolicy</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">allow-tcp-6379</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">production</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">selector:</span> <span class="hljs-string">role</span> <span class="hljs-string">==</span> <span class="hljs-string">'database'</span>
  <span class="hljs-attr">types:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Ingress</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Egress</span>
  <span class="hljs-attr">ingress:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">action:</span> <span class="hljs-string">Allow</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">annotations:</span>
        <span class="hljs-attr">from:</span> <span class="hljs-string">frontend</span>
        <span class="hljs-attr">to:</span> <span class="hljs-string">database</span>
    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
    <span class="hljs-attr">source:</span>
      <span class="hljs-attr">selector:</span> <span class="hljs-string">role</span> <span class="hljs-string">==</span> <span class="hljs-string">'frontend'</span>
    <span class="hljs-attr">destination:</span>
      <span class="hljs-attr">ports:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-number">6379</span>
  <span class="hljs-attr">egress:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">action:</span> <span class="hljs-string">Allow</span></code></pre></div><ul><li>NetworkSet</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkSet</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">external-database</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">staging</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">role:</span> <span class="hljs-string">db</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">nets:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-number">198.51</span><span class="hljs-number">.100</span><span class="hljs-number">.0</span><span class="hljs-string">/28</span>
  <span class="hljs-bullet">-</span> <span class="hljs-number">203.0</span><span class="hljs-number">.113</span><span class="hljs-number">.0</span><span class="hljs-string">/24</span></code></pre></div><ul><li>Node</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Node</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">node-hostname</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">bgp:</span>
    <span class="hljs-attr">asNumber:</span> <span class="hljs-number">64512</span>
    <span class="hljs-attr">ipv4Address:</span> <span class="hljs-number">10.244</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span><span class="hljs-string">/24</span>
    <span class="hljs-attr">ipv6Address:</span> <span class="hljs-number">2001</span><span class="hljs-string">:db8:85a3::8a2e:370:7334/120</span>
    <span class="hljs-attr">ipv4IPIPTunnelAddr:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span></code></pre></div><ul><li>Profile</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Profile</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">dev-apps</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">ingress:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">action:</span> <span class="hljs-string">Deny</span>
    <span class="hljs-attr">source:</span>
      <span class="hljs-attr">nets:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.20</span><span class="hljs-number">.0</span><span class="hljs-string">/24</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">action:</span> <span class="hljs-string">Allow</span>
    <span class="hljs-attr">source:</span>
      <span class="hljs-attr">selector:</span> <span class="hljs-string">stage</span> <span class="hljs-string">==</span> <span class="hljs-string">'development'</span>
  <span class="hljs-attr">egress:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">action:</span> <span class="hljs-string">Allow</span>
  <span class="hljs-attr">labelsToApply:</span>
    <span class="hljs-attr">stage:</span> <span class="hljs-string">development</span></code></pre></div><ul><li>WorkloadEndpoint</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">WorkloadEndpoint</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">node1-k8s-my--nginx--b1337a-eth0</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">frontend</span>
    <span class="hljs-attr">projectcalico.org/namespace:</span> <span class="hljs-string">default</span>
    <span class="hljs-attr">projectcalico.org/orchestrator:</span> <span class="hljs-string">k8s</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">node:</span> <span class="hljs-string">node1</span>
  <span class="hljs-attr">orchestrator:</span> <span class="hljs-string">k8s</span>
  <span class="hljs-attr">endpoint:</span> <span class="hljs-string">eth0</span>
  <span class="hljs-attr">containerID:</span> <span class="hljs-number">1337495556942031415926535</span>
  <span class="hljs-attr">pod:</span> <span class="hljs-string">my-nginx-b1337a</span>
  <span class="hljs-attr">endpoint:</span> <span class="hljs-string">eth0</span>
  <span class="hljs-attr">interfaceName:</span> <span class="hljs-string">cali0ef24ba</span>
  <span class="hljs-attr">mac:</span> <span class="hljs-string">ca:fe:1d:52:bb:e9</span>
  <span class="hljs-attr">ipNetworks:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-number">192.168</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/32</span>
  <span class="hljs-attr">profiles:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">profile1</span>
  <span class="hljs-attr">ports:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">some-port</span>
    <span class="hljs-attr">port:</span> <span class="hljs-number">1234</span>
    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">another-port</span>
    <span class="hljs-attr">port:</span> <span class="hljs-number">5432</span>
    <span class="hljs-attr">protocol:</span> <span class="hljs-string">UDP</span></code></pre></div><h3 id="卸载-calico"><a href="#卸载-calico" class="headerlink" title="卸载 calico"></a>卸载 calico</h3><div class="hljs"><pre><code class="hljs bash">kubectl delete -f calico.yaml</code></pre></div><h3 id="删除-calico-配置"><a href="#删除-calico-配置" class="headerlink" title="删除 calico 配置"></a>删除 calico 配置</h3><div class="hljs"><pre><code class="hljs bash">ansible ins -m shell -a <span class="hljs-string">'rm -rf /etc/cni /opt/cni/ /var/lib/calico/'</span></code></pre></div><h2 id="集群校验"><a href="#集群校验" class="headerlink" title="集群校验"></a>集群校验</h2><h3 id="查看集群信息"><a href="#查看集群信息" class="headerlink" title="查看集群信息"></a>查看集群信息</h3><div class="hljs"><pre><code class="hljs bash"> kubectl config get-clusters
NAME
shenzhen

 kubectl cluster-info
Kubernetes master is running at https://10.10.34.89:8443
KubeDNS is running at https://10.10.34.89:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</code></pre></div><h3 id="查看-namespace"><a href="#查看-namespace" class="headerlink" title="查看 namespace"></a>查看 namespace</h3><div class="hljs"><pre><code class="hljs bash"> kubectl get namespaces
NAME              STATUS   AGE
default           Active   15m
kube-node-lease   Active   15m
kube-public       Active   15m
kube-system       Active   15m</code></pre></div><h3 id="查看节点状态"><a href="#查看节点状态" class="headerlink" title="查看节点状态"></a>查看节点状态</h3><div class="hljs"><pre><code class="hljs bash"> kubectl get nodes
NAME    STATUS     ROLES    AGE    VERSION
uki01   NotReady   &lt;none&gt;   7m     v1.17.3
uki02   NotReady   &lt;none&gt;   7m    v1.17.3
uki03   NotReady   &lt;none&gt;   7m    v1.17.3
ukm01   NotReady   master   10m    v1.17.3
ukm02   NotReady   master   14m    v1.17.3
ukm03   NotReady   master   13m    v1.17.3
ukn01   NotReady   &lt;none&gt;   7m     v1.17.3
ukn02   NotReady   &lt;none&gt;   7m     v1.17.3
ukn03   NotReady   &lt;none&gt;   7m     v1.17.3</code></pre></div><h3 id="添加-infra-标签"><a href="#添加-infra-标签" class="headerlink" title="添加 infra 标签"></a>添加 infra 标签</h3><div class="hljs"><pre><code class="hljs bash"> kubectl label node uki01 uki02 uki03 node-role.kubernetes.io/infra=<span class="hljs-literal">true</span>
node/uki01 labeled
node/uki02 labeled
node/uki03 labeled

 kubectl get nodes -l node-role.kubernetes.io/infra
NAME    STATUS      ROLES   AGE   VERSION
uki01   NotReady    infra   8m    v1.17.3
uki02   NotReady    infra   8m    v1.17.3
uki03   NotReady    infra   8m    v1.17.3</code></pre></div><h3 id="添加-worker-标签"><a href="#添加-worker-标签" class="headerlink" title="添加 worker 标签"></a>添加 worker 标签</h3><div class="hljs"><pre><code class="hljs bash"> kubectl label node ukn01 ukn02 ukn03 node-role.kubernetes.io/worker=<span class="hljs-literal">true</span>
node/ukn01 labeled
node/ukn02 labeled
node/ukn03 labeled

 kubectl get nodes -l node-role.kubernetes.io/worker
NAME    STATUS      ROLES    AGE   VERSION
ukn01   NotReady    worker   8m    v1.17.3
ukn02   NotReady    worker   8m    v1.17.3
ukn03   NotReady    worker   8m    v1.17.3</code></pre></div><h3 id="查看所有节点标签"><a href="#查看所有节点标签" class="headerlink" title="查看所有节点标签"></a>查看所有节点标签</h3><div class="hljs"><pre><code class="hljs bash"> kubectl get nodes --show-labels
NAME    STATUS     ROLES    AGE     VERSION   LABELS
uki01   NotReady   infra    10m     v1.17.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=uki01,kubernetes.io/os=linux,node-role.kubernetes.io/infra=<span class="hljs-literal">true</span>
uki02   NotReady   infra    10m     v1.17.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=uki02,kubernetes.io/os=linux,node-role.kubernetes.io/infra=<span class="hljs-literal">true</span>
uki03   NotReady   infra    10m     v1.17.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=uki03,kubernetes.io/os=linux,node-role.kubernetes.io/infra=<span class="hljs-literal">true</span>
ukm01   NotReady   master   19m     v1.17.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ukm01,kubernetes.io/os=linux,node-role.kubernetes.io/master=
ukm02   NotReady   master   17m     v1.17.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ukm02,kubernetes.io/os=linux,node-role.kubernetes.io/master=
ukm03   NotReady   master   15m      v1.17.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ukm03,kubernetes.io/os=linux,node-role.kubernetes.io/master=
ukn01   NotReady   worker   10m     v1.17.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ukn01,kubernetes.io/os=linux,node-role.kubernetes.io/worker=<span class="hljs-literal">true</span>
ukn02   NotReady   worker   10m     v1.17.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ukn02,kubernetes.io/os=linux,node-role.kubernetes.io/worker=<span class="hljs-literal">true</span>
ukn03   NotReady   worker   10m     v1.17.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ukn03,kubernetes.io/os=linux,node-role.kubernetes.io/worker=<span class="hljs-literal">true</span></code></pre></div><h3 id="删除节点标签"><a href="#删除节点标签" class="headerlink" title="删除节点标签"></a>删除节点标签</h3><div class="hljs"><pre><code class="hljs bash">kubectl label node uki01 uki02 uki03 node-role.kubernetes.io/infra-</code></pre></div><h3 id="kubectl-简介"><a href="#kubectl-简介" class="headerlink" title="kubectl 简介"></a>kubectl 简介</h3><div class="hljs"><pre><code class="hljs docs">基础命令:
  create        从文件或 stdin 创建资源
  expose        为 deployment、pod 创建 Service
  run           使用 image 去创建一个服务
  set           更新resource ,比如更新env环境变量，image，resources 资源限制，selector  subject等。

管理命令:
  explain       查看资源定义（文档）。如 kubectl explain replicaset
  get           最基本的对象查询命令。如 kubectl get nodes&#x2F;pods&#x2F;deploy&#x2F;rs&#x2F;ns&#x2F;secret等等， 加-o wide查看详细信息，-o yaml 或-o json 输出具体格式。
  edit          编辑资源配置，完成对象的更新。
  delete        删除指定资源，支持文件名、资源名、label selector。

部署命令:
  rollout       Deployment, Daemonset的升级过程管理（查看状态status、操作历史history、暂停升级、恢复升级、回滚等）
  scale         修改Deployment, ReplicaSet, Replication Controller, Job的实例数,实现一个副本集的手工扩展
  autoscale     为Deploy, RS, RC配置自动伸缩规则（依赖heapster和hpa）

集群管理命令:
  certificate   修改证书资源
  cluster-info  查看集群信息
  top           查看资源占用率 (CPU&#x2F;Memory&#x2F;Storage)
  cordon        标记节点为不可调度状态
  uncordon      标记节点为可调度状态
  drain         驱逐节点上的 pod ，准备下线维护
  taint         设置节点污点隔离标记

故障诊断命令:
  describe      查看资源详情
  logs          查看 pod 内容器的日志
  attach        Attach 到 pod 内的一个容器
  exec          进入指定容器执行命令
  port-forward  为 pod 创建本地端口映射
  proxy         为 Kubernetes API server 创建代理
  cp            容器内外&#x2F;容器间文件拷贝
  auth          授权校验

高级命令:
  diff          比较当前版本和要部署的版本
  apply         从文件或stdin创建&#x2F;更新资源
  patch         使用strategic merge patch语法更新对象的某些字段
  replace       从文件或stdin更新资源
  wait          实验功能：等待一种或多种资源的特定条件。
  convert       在不同API版本之间转换对象定义
  kustomize     从指定目录或远程URL构建Kustomization目标。

配置命令:
  label         给资源设置label
  annotate      给资源设置annotation
  completion    获取shell自动补全脚本 (bash or zsh)

其他命令:
  alpha         Alpha中功能的命令
  api-resources 打印当前 kubernetes API 所支持的命令资源
  api-versions  打印当前 kubernetes API 所支持的 api 接口以及版本信息
  config        修改kubectl配置（kubeconfig文件），如context。
  plugin        提供用于与插件交互的插件程序。
  version       打印当前 kubernetes 的服务端和客户端版本</code></pre></div><h3 id="调度策略-Scheduling-Policies"><a href="#调度策略-Scheduling-Policies" class="headerlink" title="调度策略 Scheduling Policies"></a>调度策略 Scheduling Policies</h3><p>kube-scheduler 给一个 pod 做调度选择包含两个步骤：</p><ul><li><p>过滤 Filtering<br>过滤策略会检查候选 Node 是否满足 Pod 的创建需求；<br>在过滤之后得出一个 Node 列表，里面包含了所有可调度节点；<br>如果为空，代表这个 Pod 不可调度。</p></li><li><p>打分 Scoring<br>根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。</p></li></ul><p><strong><em>过滤策略</em></strong></p><ul><li>PodFitsHostPorts：检查 Pod 请求的端口是否空闲（网络协议类型）。</li><li>PodFitsHost：检查 Pod 是否通过其主机名指定了特定的 Node。</li><li>PodFitsResources：检查节点空闲资源（例如 CPU 和内存）是否满足 Pod 的要求。</li><li>PodMatchNodeSelector：检查 Pod label 是否与 Node label 匹配。</li><li>NoVolumeZoneConflict：检查 Volume 存储卷是否可用。</li><li>NoDiskConflict：检查节点 Disk 磁盘是否冲突。</li><li>MaxCSIVolumeCount：检查节点挂载的 CSI 卷是否超出最大限制。</li><li>CheckNodeMemoryPressure：节点正在报告内存压力，并且没有配置的异常，则不会在此处安排 Pod。</li><li>CheckNodePIDPressure：节点正在报告进程 PID 稀缺，并且没有配置的异常，则不会在此处安排 Pod。</li><li>CheckNodeDiskPressure：节点正在报告存储压力，并且没有配置的异常，则不会在此处安排Pod。</li><li>CheckNodeCondition：检查节点的文件系统、网络状态或者 kubelet 是否准备好运行 Pod。</li><li>PodToleratesNodeTaints：检查 Pod 的 Tolerates 是否可以容忍节点的 Taints。</li><li>CheckVolumeBinding：检查 PVC 状态是否满足 Pod 创建需求。</li></ul><p><strong><em>打分策略</em></strong></p><ul><li>SelectorSpreadPriority：将属于同一 StatefulSet、 ReplicaSet 或 Service 的 Pod 分散在不同的主机 。</li><li>InterPodAffinityPriority：根据 Pod 亲和条目打分，匹配到给定节点的条目权重相加，结果值越大的节点得分越高。</li><li>LeastRequestedPriority：计算Pods需要的CPU和内存在当前节点可用资源的百分比，具有最小百分比的节点就是最优。</li><li>MostRequestedPriority：适用动态伸缩集群环境，会优先调度pod到使用率最高的主机节点。当伸缩集群时，就会腾出空闲机器，从而可以停机处理。</li><li>RequestedToCapacityRatioPriority：为节点上每个资源占用比例设定得分值，给资源打分函数在打分时使用。</li><li>BalancedResourceAllocation：优选那些使得资源利用率更为均衡的节点。</li><li>NodePreferAvoidPodsPriority：根据节点是否包含 scheduler.alpha.kubernetes.io/preferAvoidPods 来计算其优先级，将两个不同 Pod 运行在不同的节点上。</li><li>NodeAffinityPriority：基于 PreferredDuringSchedulingIgnoredDuringExecution 来进行节点亲和调度。</li><li>TaintTolerationPriority：基于 Pod 中对节点的污点容忍程度进行优先级评估，这个策略能够调整待选节点的排名。</li><li>ImageLocalityPriority：已经拥有 Pod 需要的 image 的节点会有较高的优先级。</li><li>ServiceSpreadingPriority：确保归属于同一给 Service 的 Pod 调度到不同的节点上，确保节点宕机之后 Service 也具有很强容灾能力。</li><li>EqualPriority：将所有的 Node 设置成相同的权重为 1。</li><li>EvenPodsSpreadPriority：实现首选的Pod拓扑扩展约束。</li></ul><h3 id="调度配置-Scheduling-Profiles"><a href="#调度配置-Scheduling-Profiles" class="headerlink" title="调度配置 Scheduling Profiles"></a>调度配置 Scheduling Profiles</h3><p>官方文档 <code>https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/</code></p><p>每次调度一个Pod的尝试都分为两个阶段，调度周期 Scheduling Cycle 和 绑定周期 Binding Cycle。</p><ul><li>调度周期为Pod选择一个节点，并且绑定周期将该决定应用于集群。调度周期和绑定周期一起被称为“调度上下文”。</li><li>调度周期是串行运行的，而绑定周期可能是同时运行的。</li><li>如果确定Pod不可调度或存在内部错误，则可以中止调度或绑定周期。Pod将返回队列并重试。</li></ul><p><strong><em>扩展点 Extension points</em></strong></p><p>QueueSort - 对 Pod 的待调度队列进行排序，比较两个 Pod 谁更优先调度，同一时间点只能有一个 QueueSort 插件生效。</p><p>调度周期 Scheduling Cycle</p><ul><li>PreFilter - 对 Pod 条件信息进行预处理，如果 PreFilter 返回了 error 则调度过程终止。</li><li>Filter - 标记不符合调度条件的</li><li>PreScore - 对 Pod 进行预评分，为 Score 插件使用提供可共享的状态。如果PreScore插件返回错误，则调度周期将中止。</li><li>Score - 对符合过滤条件的节点进行计分排名，在 NormalizeScore 阶段之后，调度程序将根据权重合并所有的节点分值。</li><li>NormalizeScore - 在调度程序计算节点的最终排名之前修改分数。</li><li>Reserve - 在符合条件的节点上为 Pod 保留资源。</li><li>Permit - 用于阻止或者延迟 Pod 与节点的绑定。<ul><li>approve - 批准</li><li>deny - 拒绝</li><li>wait - 等待</li></ul></li></ul><p>绑定周期 Binding Cycle</p><ul><li>PreBind - 用于执行绑定 Pod 之前所需的任何工作。如果任何 PreBind 插件返回错误，则 Pod 被拒绝并返回到调度队列。</li><li>Bind - 用于 Pod 绑定。如果绑定插件选择处理 Pod，则会跳过其余的绑定插件。</li><li>PostBind - 成功绑定Pod后，将调用后绑定插件。绑定周期到此结束，可以用来清理关联的资源。</li></ul><p>Unreserve - 如果节点为 Pod 预留了资源，Pod 又在被绑定过程中被拒绝绑定，则执行 unreserve 释放为 Pod 预留的资源。</p><p><strong><em>调度插件 Scheduling plugins</em></strong></p><p>默认情况下启用以下插件，实现这些扩展点中的一个或多个：</p><ul><li>DefaultTopologySpread：基于 Service、ReplicaSets 和 StatefulSets 分散部署。扩展点：PreScore， Score。</li><li>ImageLocality：倾向于已经具有 Pod 运行的 image 的节点。扩展点：Score。</li><li>TaintToleration：实施污点和宽容。扩展点：Filter，Prescore，Score。</li><li>NodeName：Pod 配置的节点与节点是否匹配。扩展点：Filter。</li><li>NodePorts：节点端口是否被占用。扩展点：PreFilter，Filter。</li><li>NodePreferAvoidPods：根据节点 annotation 进行评分 scheduler.alpha.kubernetes.io/preferAvoidPods。扩展点：Score。</li><li>NodeAffinity：节点的亲和与反亲和。扩展点：Filter，Score。</li><li>PodTopologySpread：实现 Pod 拓扑传播。扩展点：PreFilter，Filter，PreScore，Score。</li><li>NodeUnschedulable：过滤掉 .spec.unschedulable 为 true 的节点。扩展点：Filter。</li><li>NodeResourcesFit：检查节点是否具有 Pod 请求的资源。扩展点：PreFilter，Filter。</li><li>NodeResourcesBallancedAllocation：使用资源利用均衡的节点。扩展点：Score。</li><li>NodeResourcesLeastAllocated：使用资源分配少的节点。扩展点：Score。</li><li>VolumeBinding：节点是否具有或可以绑定所请求卷。扩展点：Filter。</li><li>VolumeRestrictions：节点挂载的卷是否满足限制条件。扩展点：Filter。</li><li>VolumeZone：检查卷是否满足任何区域要求。扩展点：Filter。</li><li>NodeVolumeLimits：检查节点是否可以符合 CSI 限制。扩展点：Filter。</li><li>EBSLimits：检查节点是否满足 AWS EBS 限制。扩展点：Filter。</li><li>GCEPDLimits：检查节点是否满足 GCP-PD 限制。扩展点：Filter。</li><li>AzureDiskLimits：检查节点是否满足 Azure Disk 限制。扩展点：Filter。</li><li>InterPodAffinity：基于 Pod 亲和与反亲和。扩展点：PreFilter，Filter，PreScore，Score。</li><li>PrioritySort：基于默认优先级的排序。扩展点：QueueSort。</li><li>DefaultBinder：基于默认的绑定机制。扩展点：Bind。</li></ul><p>您还可以通过组件配置API启用以下默认未启用的插件：</p><ul><li>NodeResourcesMostAllocated：倾向于资源分配高的节点。扩展点：Score。</li><li>RequestedToCapacityRatio：基于节点的资源可用性比例。扩展点：Score。</li><li>NodeResourceLimits：支持满足 Pod 资源限制的节点。扩展点：PreScore，Score。</li><li>CinderVolume：检查节点是否可以满足 OpenStack Cinder 限制。扩展点：Filter。</li><li>NodeLabel：根据节点的 label 进行评分。扩展点：Filter，Score。</li><li>ServiceAffinity：检查 Service label 匹配的节点，该插件有利于在节点之间部署服务。扩展点：PreFilter，Filter，Score。</li></ul><h3 id="亲和-Affinity-与-反亲和-Anti-Affinity"><a href="#亲和-Affinity-与-反亲和-Anti-Affinity" class="headerlink" title="亲和 Affinity 与 反亲和 Anti-Affinity"></a>亲和 Affinity 与 反亲和 Anti-Affinity</h3><h4 id="亲和规则"><a href="#亲和规则" class="headerlink" title="亲和规则"></a>亲和规则</h4><ul><li><p>requiredDuringSchedulingRequiredDuringExecution<br>该规则还未正式上线；<br>在调度期间必须满足亲和或者反亲和规则，如果不能满足规则，则 pod 不能被调度到对应的主机上;<br>在之后的运行过程中，系统会持续监规则是否满足。</p></li><li><p>RequiredDuringSchedulingIgnoredDuringExecution<br>在调度期间必须满足亲和或者反亲和规则，如果不能满足规则，则 pod 不能被调度到对应的主机上;<br>在之后的运行过程中，系统不再检查这些规则是否满足。</p></li><li><p>PreferredDuringSchedulingIgnoredDuringExecution<br>在调度期间尽量满足亲和或者反亲和规则，如果不能满足规则，pod 也有可能被调度到对应的主机上；<br>在之后的运行过程中，系统不再检查这些规则是否满足。</p></li></ul><h4 id="nodeAffinity"><a href="#nodeAffinity" class="headerlink" title="nodeAffinity"></a>nodeAffinity</h4><p>nodeAffinity - 节点亲和，使用场景 ：</p><ul><li>将服务的所有Pod部署到指定的符合标签规则的主机上。</li><li>将服务的所有Pod部署到除部分主机外的其他主机上。</li><li>支持的操作符： In，NotIn，Exists，DoesNotExist，Gt，Lt。</li></ul><p>节点亲和：</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">with-node-affinity</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">affinity:</span>
    <span class="hljs-attr">nodeAffinity:</span>
      <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span>
        <span class="hljs-attr">nodeSelectorTerms:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">matchExpressions:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">kubernetes.io/e2e-az-name</span>
            <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span>
            <span class="hljs-attr">values:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">e2e-az1</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">e2e-az2</span>
      <span class="hljs-attr">preferredDuringSchedulingIgnoredDuringExecution:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">weight:</span> <span class="hljs-number">1</span>
        <span class="hljs-attr">preference:</span>
          <span class="hljs-attr">matchExpressions:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">another-node-label-key</span>
            <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span>
            <span class="hljs-attr">values:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">another-node-label-value</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">with-node-affinity</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">k8s.gcr.io/pause:2.0</span></code></pre></div><h4 id="podAffinity-与-podAntiAffinity"><a href="#podAffinity-与-podAntiAffinity" class="headerlink" title="podAffinity 与 podAntiAffinity"></a>podAffinity 与 podAntiAffinity</h4><p>注意：<br>Pod 间亲和与反亲和需要大量的处理，这可能会显著减慢大规模集群中的调度。我们不建议在超过数百个节点的集群中使用它们。</p><p>podAffinity - pod 亲和，使用场景 ：</p><ul><li>将某一特定的 pod 部署在同一拓扑域中，不用指定具体的拓扑域。</li><li>为了减少关联的2个服务之间的网络延迟（或其它原因），将他们部署在同一拓扑域中。</li><li>支持的操作符： In，NotIn，Exists，DoesNotExist。</li></ul><p>podAntiAffinity - pod 反亲和，使用场景 ：</p><ul><li>将一个服务的 pod 分散在不同的主机或者拓扑域中，提高服务本身的稳定性。</li><li>为某一个 pod 提供节点的资源独占权限。</li><li>把可能会相互影响的 pod 分散在不同的主机上。</li><li>支持的操作符： In，NotIn，Exists，DoesNotExist。</li></ul><p>pod 亲和与反亲和：</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">with-pod-affinity</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">affinity:</span>
    <span class="hljs-attr">podAffinity:</span>
      <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">labelSelector:</span>
          <span class="hljs-attr">matchExpressions:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">security</span>
            <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span>
            <span class="hljs-attr">values:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">S1</span>
        <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">failure-domain.beta.kubernetes.io/zone</span>
    <span class="hljs-attr">podAntiAffinity:</span>
      <span class="hljs-attr">preferredDuringSchedulingIgnoredDuringExecution:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">weight:</span> <span class="hljs-number">100</span>
        <span class="hljs-attr">podAffinityTerm:</span>
          <span class="hljs-attr">labelSelector:</span>
            <span class="hljs-attr">matchExpressions:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">security</span>
              <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span>
              <span class="hljs-attr">values:</span>
              <span class="hljs-bullet">-</span> <span class="hljs-string">S2</span>
          <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">failure-domain.beta.kubernetes.io/zone</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">with-pod-affinity</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">k8s.gcr.io/pause:2.0</span></code></pre></div><h3 id="污点驱逐-Taint"><a href="#污点驱逐-Taint" class="headerlink" title="污点驱逐 Taint"></a>污点驱逐 Taint</h3><ul><li>NoSchedule：表示k8s将不会将Pod调度到具有该污点的Node上</li><li>PreferNoSchedule：表示k8s将尽量避免将Pod调度到具有该污点的Node上</li><li>NoExecute：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去</li></ul><div class="hljs"><pre><code class="hljs bash">删除污点，开启 master 节点 pod 调度功能。
kubectl taint node ukm03 node-role.kubernetes.io/master=:NoSchedule-

添加污点，关闭 master 节点 pod 调度功能。
kubectl taint node ukm03 node-role.kubernetes.io/master=:NoSchedule</code></pre></div><h3 id="容忍-Toleration"><a href="#容忍-Toleration" class="headerlink" title="容忍 Toleration"></a>容忍 Toleration</h3><p>设置了污点的节点，Pod 将在一定程度上不会被调度到节点上。 通过设置 pod 容忍(Toleration)，将 pod 调度到存在污点的Node上。通过在Pod的spec中设置tolerations字段，给Pod设置上容忍点：</p><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-attr">tolerations:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">"key1"</span>
  <span class="hljs-attr">operator:</span> <span class="hljs-string">"Equal"</span>
  <span class="hljs-attr">value:</span> <span class="hljs-string">"value1"</span>
  <span class="hljs-attr">effect:</span> <span class="hljs-string">"NoSchedule"</span>
  <span class="hljs-attr">tolerationSeconds:</span> <span class="hljs-number">3600</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">"key1"</span>
  <span class="hljs-attr">operator:</span> <span class="hljs-string">"Equal"</span>
  <span class="hljs-attr">value:</span> <span class="hljs-string">"value1"</span>
  <span class="hljs-attr">effect:</span> <span class="hljs-string">"NoExecute"</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">"key2"</span>
  <span class="hljs-attr">operator:</span> <span class="hljs-string">"Exists"</span>
  <span class="hljs-attr">effect:</span> <span class="hljs-string">"NoSchedule"</span></code></pre></div><ul><li>key、vaule、effect要与Node上设置的taint保持一致。</li><li>当不指定key值时，表示容忍所有的污点key；当不指定effect值时，表示容忍所有的污点作用。</li><li>operator的值为Exists将会忽略value值。</li><li>tolerationSeconds用于描述当Pod需要被驱逐时可以在Pod上继续保留运行的时间。</li></ul><h3 id="create-和-apply"><a href="#create-和-apply" class="headerlink" title="create 和 apply"></a>create 和 apply</h3><p>kubectl create 命令可创建新资源。 因此再次运行该命令，则会抛出错误，因为资源名称在名称空间中应该是唯一的。</p><div class="hljs"><pre><code class="hljs bash"> kubectl create -f test.yml
pod/myapp-pod created

 kubectl create -f test.yml
Error from server (AlreadyExists): error when creating <span class="hljs-string">"pod.xml"</span>: pods <span class="hljs-string">"myapp-pod"</span> already exists</code></pre></div><p>kubectl apply 命令将配置应用于资源。 如果资源不在那里，那么它将被创建。 apply 命令可以二次运行，因为它只是应用配置。 在这种情况下，配置没有改变, 所以 pod 没有改变。</p><div class="hljs"><pre><code class="hljs bash"> kubectl apply -f test.yml
pod/myapp-pod created

 kubectl apply -f test.yml
pod/myapp-pod unchanged</code></pre></div><h3 id="kubernetes-控制器"><a href="#kubernetes-控制器" class="headerlink" title="kubernetes 控制器"></a><strong>kubernetes 控制器</strong></h3><h4 id="ReplicationController"><a href="#ReplicationController" class="headerlink" title="ReplicationController"></a>ReplicationController</h4><ul><li>确保在任何时候都有特定数量的 pod 副本处于运行状态；</li><li>类似于进程管理器， ReplicationController 监控跨多个节点的多个 pod；</li><li>通常缩写为 rc ，通过 replicas 控制副本的数量。</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ReplicationController</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-string">...</span></code></pre></div><h4 id="ReplicaSet"><a href="#ReplicaSet" class="headerlink" title="ReplicaSet"></a>ReplicaSet</h4><ul><li>ReplicaSet 是下一代的 ReplicationController；</li><li>ReplicaSet 和 ReplicationController 的唯一区别是选择器的支持，ReplicaSet 支持新的基于集合的选择器需求，而 Replication Controller 仅支持基于相等选择器的需求。</li><li>通常缩写为 rs 。</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ReplicaSet</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-string">...</span></code></pre></div><h4 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h4><ul><li>Deployment 管理和描述 Pods 和 ReplicaSets 部署方式；</li><li>清理较旧的、不再需要的 ReplicaSets 。</li><li>如果 Deployment 的当前状态不稳定，回滚到较早的 Deployment 版本。</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-string">...</span></code></pre></div><h4 id="StatefulSet"><a href="#StatefulSet" class="headerlink" title="StatefulSet"></a>StatefulSet</h4><ul><li>是用来管理有状态应用的对象；</li><li>稳定的、唯一的网络标识符；</li><li>稳定的、持久的存储；</li><li>有序的、优雅的部署和缩放；</li><li>有序的、自动的滚动更新。</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">StatefulSet</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-string">...</span></code></pre></div><h4 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h4><ul><li>DaemonSet 确保全部或者某个 label 的节点上运行一个 Pod 的实例；</li><li>在每个节点上运行集群存储 DaemonSet，例如 glusterd、ceph；</li><li>在每个节点上运行日志收集 DaemonSet，例如 fluentd、logstash；</li><li>在每个节点上运行监控 DaemonSet，例如 Prometheus Node Exporter；</li><li>在每个 infra 节点上面运行一个 ingress 服务。</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">DaemonSet</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-string">...</span></code></pre></div><h4 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h4><ul><li>Job负责处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个Pod成功结束;</li><li>运行示例作业</li><li>编写工作规范</li><li>处理Pod和容器故障</li><li>作业终止和清理</li><li>自动清理完成的作业</li><li>CronJob</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">batch/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Job</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">pi</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pi</span>
        <span class="hljs-attr">image:</span> <span class="hljs-string">perl</span>
        <span class="hljs-attr">command:</span> <span class="hljs-string">["perl",</span>  <span class="hljs-string">"-Mbignum=bpi"</span><span class="hljs-string">,</span> <span class="hljs-string">"-wle"</span><span class="hljs-string">,</span> <span class="hljs-string">"print bpi(2000)"</span><span class="hljs-string">]</span>
      <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">Never</span>
  <span class="hljs-attr">backoffLimit:</span> <span class="hljs-number">4</span></code></pre></div><h4 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a>CronJob</h4><ul><li>执行基于时间调度的任务，类似于 linux 系统的 crontab 任务。</li></ul><div class="hljs"><pre><code class="hljs yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">batch/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">CronJob</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hello</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">schedule:</span> <span class="hljs-string">"*/1 * * * *"</span>
  <span class="hljs-attr">jobTemplate:</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">template:</span>
        <span class="hljs-attr">spec:</span>
          <span class="hljs-attr">containers:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">hello</span>
            <span class="hljs-attr">image:</span> <span class="hljs-string">busybox</span>
            <span class="hljs-attr">args:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/sh</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">-c</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">date;</span> <span class="hljs-string">echo</span> <span class="hljs-string">Hello</span> <span class="hljs-string">from</span> <span class="hljs-string">the</span> <span class="hljs-string">Kubernetes</span> <span class="hljs-string">cluster</span>
          <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">OnFailure</span></code></pre></div><h3 id="备份-images"><a href="#备份-images" class="headerlink" title="备份 images"></a>备份 images</h3><div class="hljs"><pre><code class="hljs bash"> cat images.sh
!/bin/bash

imagePath=<span class="hljs-string">"/root/images"</span>
mkdir -p <span class="hljs-variable">$&#123;imagePath&#125;</span>

<span class="hljs-function"><span class="hljs-title">save_images</span></span>() &#123;
    docker images|awk <span class="hljs-string">'!/REPOSITORY/ &amp;&amp; !/&lt;none&gt;/ &amp;&amp; !/docker-registry.default.svc/ &amp;&amp; !/jenkins/&#123;print $1,$2&#125;'</span> &gt; /tmp/images.list
    <span class="hljs-keyword">while</span> <span class="hljs-built_in">read</span> Name Version
    <span class="hljs-keyword">do</span>
        fileName=$(<span class="hljs-built_in">echo</span> <span class="hljs-variable">$Name</span> | awk -F\/ <span class="hljs-string">'&#123;print $NF&#125;'</span>)
        <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">"<span class="hljs-variable">$&#123;imagePath&#125;</span>/<span class="hljs-variable">$&#123;fileName&#125;</span>_<span class="hljs-variable">$&#123;Version&#125;</span>.tgz"</span> ];<span class="hljs-keyword">then</span>
            <span class="hljs-built_in">echo</span> <span class="hljs-string">"save image <span class="hljs-variable">$Name</span>:<span class="hljs-variable">$Version</span>"</span>
            docker save <span class="hljs-variable">$Name</span>:<span class="hljs-variable">$Version</span> | gzip &gt; <span class="hljs-variable">$&#123;imagePath&#125;</span>/<span class="hljs-variable">$&#123;fileName&#125;</span>_<span class="hljs-variable">$&#123;Version&#125;</span>.tgz
        <span class="hljs-keyword">fi</span>
    <span class="hljs-keyword">done</span> &lt; /tmp/images.list
&#125;

<span class="hljs-function"><span class="hljs-title">load_images</span></span>() &#123;
    <span class="hljs-keyword">for</span> images <span class="hljs-keyword">in</span> $(ls <span class="hljs-variable">$&#123;imagePath&#125;</span>/*.tgz)
    <span class="hljs-keyword">do</span>
        <span class="hljs-built_in">echo</span> <span class="hljs-string">"load image <span class="hljs-variable">$images</span>"</span>
        docker load &lt; <span class="hljs-variable">$images</span>
    <span class="hljs-keyword">done</span>
&#125;

<span class="hljs-function"><span class="hljs-title">push_images</span></span>() &#123;
    newRegistry=<span class="hljs-string">"https://docker-registry-default.pase-hrx-stg1.zhi-niao.com/"</span>

    docker images|awk <span class="hljs-string">'!/REPOSITORY/ &amp;&amp; !/&lt;none&gt;/&#123;print $1,$2&#125;'</span> &gt; /tmp/images.list
    <span class="hljs-keyword">while</span> <span class="hljs-built_in">read</span> Name Version
    <span class="hljs-keyword">do</span>
        fileName=$(<span class="hljs-built_in">echo</span> <span class="hljs-variable">$Name</span> | awk -F/ <span class="hljs-string">'sub($1,"")'</span>)
        <span class="hljs-built_in">echo</span> <span class="hljs-string">"<span class="hljs-variable">$&#123;Name&#125;</span>:<span class="hljs-variable">$&#123;Version&#125;</span> | new tag | <span class="hljs-variable">$&#123;newRegistry&#125;</span><span class="hljs-variable">$&#123;fileName&#125;</span>:<span class="hljs-variable">$&#123;Version&#125;</span>"</span>
        docker tag <span class="hljs-variable">$&#123;Name&#125;</span>:<span class="hljs-variable">$&#123;Version&#125;</span> <span class="hljs-variable">$&#123;newRegistry&#125;</span><span class="hljs-variable">$&#123;fileName&#125;</span>:<span class="hljs-variable">$&#123;Version&#125;</span>
        docker push <span class="hljs-variable">$&#123;newRegistry&#125;</span><span class="hljs-variable">$&#123;fileName&#125;</span>:<span class="hljs-variable">$&#123;Version&#125;</span>
        docker rmi <span class="hljs-variable">$&#123;newRegistry&#125;</span><span class="hljs-variable">$&#123;fileName&#125;</span>:<span class="hljs-variable">$&#123;Version&#125;</span>
    <span class="hljs-keyword">done</span> &lt; /tmp/images.list
&#125;

<span class="hljs-keyword">case</span> <span class="hljs-variable">$1</span> <span class="hljs-keyword">in</span>
    save|s)
        save_images
        ;;
    load|l)
        load_images
        ;;
    push|p)
        push_images
        ;;
    *)
        <span class="hljs-built_in">echo</span> <span class="hljs-string">"Usage: <span class="hljs-variable">$0</span> &#123;save|load&#125;"</span>
        ;;
<span class="hljs-keyword">esac</span>

 分发 images.sh
 ansible master:worker:infra -m copy -a <span class="hljs-string">'src=images.sh dest=/root/images.sh mode=0755'</span>

 ansible master:worker:infra -m shell -a <span class="hljs-string">'./images.sh save'</span>
 <span class="hljs-keyword">for</span> host <span class="hljs-keyword">in</span> ukm02 ukm03 uki01 uki02 uki03 ukn01 ukn02 ukn03;<span class="hljs-keyword">do</span> rsync -vzrtopg --progress -e ssh <span class="hljs-variable">$&#123;host&#125;</span>:/root/images/* /root/images/;<span class="hljs-keyword">done</span></code></pre></div><h3 id="切换-namespace"><a href="#切换-namespace" class="headerlink" title="切换 namespace"></a>切换 namespace</h3><div class="hljs"><pre><code class="hljs bash">kubectl config <span class="hljs-built_in">set</span>-context --current --namespace=kube-system</code></pre></div><h3 id="获取加入集群-token"><a href="#获取加入集群-token" class="headerlink" title="获取加入集群 token"></a>获取加入集群 token</h3><div class="hljs"><pre><code class="hljs bash"> kubeadm token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS
gnc91d.aav2px3kb3021uxl   8h          2020-02-20T20:20:00+08:00   authentication,signing   The default bootstrap token generated by <span class="hljs-string">'kubeadm init'</span>.   system:bootstrappers:kubeadm:default-node-token</code></pre></div><h3 id="获取加入集群-token-hash"><a href="#获取加入集群-token-hash" class="headerlink" title="获取加入集群 token hash"></a>获取加入集群 token hash</h3><div class="hljs"><pre><code class="hljs bash"> openssl x509 -pubkey -<span class="hljs-keyword">in</span> /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed <span class="hljs-string">'s/^.* //'</span>
ae56bc3c9cabaaf04878be16436348419a0772342d5cc71c371fb5a79157b5a8</code></pre></div><h3 id="重新生成-kubeadm-certs"><a href="#重新生成-kubeadm-certs" class="headerlink" title="重新生成 kubeadm-certs"></a>重新生成 kubeadm-certs</h3><ul><li>kubeadm-certs 密钥和解密密钥会在两个小时后失效</li></ul><div class="hljs"><pre><code class="hljs bash"> kubeadm init phase upload-certs --upload-certs
[upload-certs] Storing the certificates <span class="hljs-keyword">in</span> Secret <span class="hljs-string">"kubeadm-certs"</span> <span class="hljs-keyword">in</span> the <span class="hljs-string">"kube-system"</span> Namespace
[upload-certs] Using certificate key:
4379fa5fdc142d5e98d01a290b8490fc9a169e2a4c3a94695ec3745573b22fb3</code></pre></div><h3 id="查看-kubernetes-证书状态"><a href="#查看-kubernetes-证书状态" class="headerlink" title="查看 kubernetes 证书状态"></a>查看 kubernetes 证书状态</h3><div class="hljs"><pre><code class="hljs bash"> kubeadm alpha certs check-expiration
[check-expiration] Reading configuration from the cluster...
[check-expiration] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>

CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Apr 22, 2021 09:03 UTC   364d                                    no
apiserver                  Apr 22, 2021 09:03 UTC   364d            ca                      no
apiserver-etcd-client      Apr 22, 2021 09:03 UTC   364d            etcd-ca                 no
apiserver-kubelet-client   Apr 22, 2021 09:03 UTC   364d            ca                      no
controller-manager.conf    Apr 22, 2021 09:03 UTC   364d                                    no
etcd-healthcheck-client    Apr 22, 2021 09:03 UTC   364d            etcd-ca                 no
etcd-peer                  Apr 22, 2021 09:03 UTC   364d            etcd-ca                 no
etcd-server                Apr 22, 2021 09:03 UTC   364d            etcd-ca                 no
front-proxy-client         Apr 22, 2021 09:03 UTC   364d            front-proxy-ca          no
scheduler.conf             Apr 22, 2021 09:03 UTC   364d                                    no

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Apr 14, 2030 11:18 UTC   9y              no
etcd-ca                 Apr 14, 2030 11:18 UTC   9y              no
front-proxy-ca          Apr 14, 2030 11:18 UTC   9y              no</code></pre></div><h3 id="重新签发所有-kubernetes-证书"><a href="#重新签发所有-kubernetes-证书" class="headerlink" title="重新签发所有 kubernetes 证书"></a>重新签发所有 kubernetes 证书</h3><div class="hljs"><pre><code class="hljs bash"> kubeadm alpha certs renew all
[renew] Reading configuration from the cluster...
[renew] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>

certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the admin to use and <span class="hljs-keyword">for</span> kubeadm itself renewed
certificate <span class="hljs-keyword">for</span> serving the Kubernetes API renewed
certificate the apiserver uses to access etcd renewed
certificate <span class="hljs-keyword">for</span> the API server to connect to kubelet renewed
certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the controller manager to use renewed
certificate <span class="hljs-keyword">for</span> liveness probes to healthcheck etcd renewed
certificate <span class="hljs-keyword">for</span> etcd nodes to communicate with each other renewed
certificate <span class="hljs-keyword">for</span> serving etcd renewed
certificate <span class="hljs-keyword">for</span> the front proxy client renewed
certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the scheduler manager to use renewed</code></pre></div><h3 id="更新证书签名"><a href="#更新证书签名" class="headerlink" title="更新证书签名"></a>更新证书签名</h3><div class="hljs"><pre><code class="hljs bash"> 创建更新配置
 cat &gt; ca-sign.yaml &lt;&lt;EOF
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
apiServer:
  certSANs:
  - <span class="hljs-string">"10.10.34.89"</span>
  - <span class="hljs-string">"10.10.34.92"</span>
  - <span class="hljs-string">"10.10.34.93"</span>
  - <span class="hljs-string">"10.10.34.94"</span>
  - <span class="hljs-string">"113.108.71.77"</span>
  - <span class="hljs-string">"kubernetes"</span>
  - <span class="hljs-string">"kubernetes.default"</span>
  - <span class="hljs-string">"kubernetes.default.svc"</span>
  - <span class="hljs-string">"kubernetes.default.svc.cluster"</span>
  - <span class="hljs-string">"kubernetes.default.svc.cluster.local"</span>
controllerManager:
  extraArgs:
    cluster-signing-cert-file: /etc/kubernetes/pki/ca.crt
    cluster-signing-key-file: /etc/kubernetes/pki/ca.key
EOF

 更新 kubernetes 配置
 kubeadm config upload from-file --config=ca-sign.yaml

 确认更新配置生效
 kubeadm config view
apiServer:
  certSANs:
  - 10.10.34.89
  - 10.10.34.92
  - 10.10.34.93
  - 10.10.34.94
  - 113.108.71.77
  - kubernetes
  - kubernetes.default
  - kubernetes.default.svc
  - kubernetes.default.svc.cluster
  - kubernetes.default.svc.cluster.local
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager:
  extraArgs:
    cluster-signing-cert-file: /etc/kubernetes/pki/ca.crt
    cluster-signing-key-file: /etc/kubernetes/pki/ca.key
dns:
  <span class="hljs-built_in">type</span>: CoreDNS
etcd:
  <span class="hljs-built_in">local</span>:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.18.2
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: &#123;&#125;

 删除原 apiserver 证书
 rm -rf /etc/kubernetes/pki/apiserver.*

 重新生成 apiserver 证书
 kubeadm init phase certs apiserver --config=ca-sign.yaml
W0429 15:59:26.534066   10196 configset.go:202] WARNING: kubeadm cannot validate component configs <span class="hljs-keyword">for</span> API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[certs] Generating <span class="hljs-string">"apiserver"</span> certificate and key
[certs] apiserver serving cert is signed <span class="hljs-keyword">for</span> DNS names [ukm01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local ukm01 ukm02 ukm03 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.10.34.92 10.10.34.89 10.10.34.92 10.10.34.93 10.10.34.94 113.108.71.77]


 确认 apiserver 证书更新情况
 openssl x509 -text -noout -<span class="hljs-keyword">in</span> /etc/kubernetes/pki/apiserver.crt
...
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage:
                TLS Web Server Authentication
            X509v3 Subject Alternative Name:
                DNS:ukm01, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, DNS:ukm01, DNS:ukm02, DNS:ukm03, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:10.10.34.92, IP Address:10.10.34.89, IP Address:10.10.34.92, IP Address:10.10.34.93, IP Address:10.10.34.94, IP Address:113.108.71.77
...

 更新所有证书
 kubeadm alpha certs renew all --config=ca-sign.yaml
W0429 15:50:57.792294    2944 configset.go:202] WARNING: kubeadm cannot validate component configs <span class="hljs-keyword">for</span> API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the admin to use and <span class="hljs-keyword">for</span> kubeadm itself renewed
certificate <span class="hljs-keyword">for</span> serving the Kubernetes API renewed
certificate the apiserver uses to access etcd renewed
certificate <span class="hljs-keyword">for</span> the API server to connect to kubelet renewed
certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the controller manager to use renewed
certificate <span class="hljs-keyword">for</span> liveness probes to healthcheck etcd renewed
certificate <span class="hljs-keyword">for</span> etcd nodes to communicate with each other renewed
certificate <span class="hljs-keyword">for</span> serving etcd renewed
certificate <span class="hljs-keyword">for</span> the front proxy client renewed
certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the scheduler manager to use renewed

 通过 api 更新所有证书，需要执行确认命令。
 kubeadm alpha certs renew all --use-api --config=ca-sign.yaml &amp;
...
[certs] Certificate request <span class="hljs-string">"kubeadm-cert-kubernetes-admin-8pvf8"</span> created
...

 批准更新
 kubectl get csr | awk <span class="hljs-string">'!/Approved/ &amp;&amp; !/NAME/&#123;print "kubectl certificate approve "$1&#125;'</span> | bash
...
certificatesigningrequest.certificates.k8s.io/kubeadm-cert-kubernetes-admin-8pvf8 approved
...

 查看证书更新状态
 kubectl get csr
NAME                                                AGE     SIGNERNAME                     REQUESTOR          CONDITION
kubeadm-cert-front-proxy-client-tq62p               101s    kubernetes.io/legacy-unknown   kubernetes-admin   Approved,Issued
kubeadm-cert-kube-apiserver-etcd-client-rnnjn       2m15s   kubernetes.io/legacy-unknown   kubernetes-admin   Approved,Issued
kubeadm-cert-kube-apiserver-fk62g                   2m16s   kubernetes.io/legacy-unknown   kubernetes-admin   Approved,Issued
kubeadm-cert-kube-apiserver-kubelet-client-5cmpv    2m11s   kubernetes.io/legacy-unknown   kubernetes-admin   Approved,Issued
kubeadm-cert-kube-etcd-healthcheck-client-7lcw9     118s    kubernetes.io/legacy-unknown   kubernetes-admin   Approved,Issued
kubeadm-cert-kubernetes-admin-8pvf8                 3m24s   kubernetes.io/legacy-unknown   kubernetes-admin   Approved,Issued
kubeadm-cert-system:kube-controller-manager-bgsmt   2m8s    kubernetes.io/legacy-unknown   kubernetes-admin   Approved,Issued
kubeadm-cert-system:kube-scheduler-dhj6b            96s     kubernetes.io/legacy-unknown   kubernetes-admin   Approved,Issued
kubeadm-cert-ukm01-2bwbg                            115s    kubernetes.io/legacy-unknown   kubernetes-admin   Approved,Issued
kubeadm-cert-ukm01-ftrgn                            104s    kubernetes.io/legacy-unknown   kubernetes-admin   Approved,Issued</code></pre></div><h2 id="卸载-kubernetes-集群"><a href="#卸载-kubernetes-集群" class="headerlink" title="卸载 kubernetes 集群"></a><strong>卸载 kubernetes 集群</strong></h2><h3 id="先隔离再删除节点"><a href="#先隔离再删除节点" class="headerlink" title="先隔离再删除节点"></a>先隔离再删除节点</h3><div class="hljs"><pre><code class="hljs bash">kubectl drain uki01 uki02 uki03 ukm01 ukm02 ukm03 ukn01 ukn02 ukn03 --delete-local-data --force --ignore-daemonsets

kubectl delete node uki01 uki02 uki03 ukm01 ukm02 ukm03 ukn01 ukn02 ukn03</code></pre></div><h3 id="重置节点状态"><a href="#重置节点状态" class="headerlink" title="重置节点状态"></a>重置节点状态</h3><div class="hljs"><pre><code class="hljs bash">ansible ins -m shell -a <span class="hljs-string">'kubeadm reset --force'</span>

ansible ins -m shell -a <span class="hljs-string">'rm -rf /etc/cni /opt/cni/ /var/lib/calico/ ~/.kube/'</span></code></pre></div><h2 id="kubernetes-集群升级"><a href="#kubernetes-集群升级" class="headerlink" title="kubernetes 集群升级"></a><strong>kubernetes 集群升级</strong></h2><h3 id="检查-kubeadm-可用版本"><a href="#检查-kubeadm-可用版本" class="headerlink" title="检查 kubeadm 可用版本"></a>检查 kubeadm 可用版本</h3><div class="hljs"><pre><code class="hljs bash">apt update &amp;&amp; apt-cache policy kubeadm</code></pre></div><h3 id="升级-kubeadm"><a href="#升级-kubeadm" class="headerlink" title="升级 kubeadm"></a>升级 kubeadm</h3><p>建议一台一台升级，批量操作存在无法预估的错误。</p><div class="hljs"><pre><code class="hljs bash">apt-mark unhold kubeadm &amp;&amp; \
 apt-get update &amp;&amp; apt-get install -y kubeadm=1.18.0-00 &amp;&amp; \
 apt-mark hold kubeadm

或者
apt-get update &amp;&amp; \
 apt-get install -y --allow-change-held-packages kubeadm=1.18.0-00</code></pre></div><h3 id="验证-kubeadm-版本"><a href="#验证-kubeadm-版本" class="headerlink" title="验证 kubeadm 版本"></a>验证 kubeadm 版本</h3><div class="hljs"><pre><code class="hljs bash"> kubeadm version
kubeadm version: &amp;version.Info&#123;Major:<span class="hljs-string">"1"</span>, Minor:<span class="hljs-string">"18"</span>, GitVersion:<span class="hljs-string">"v1.18.0"</span>, GitCommit:<span class="hljs-string">"9e991415386e4cf155a24b1da15becaa390438d8"</span>, GitTreeState:<span class="hljs-string">"clean"</span>, BuildDate:<span class="hljs-string">"2020-03-25T14:56:30Z"</span>, GoVersion:<span class="hljs-string">"go1.13.8"</span>, Compiler:<span class="hljs-string">"gc"</span>, Platform:<span class="hljs-string">"linux/amd64"</span>&#125;</code></pre></div><h3 id="隔离-ukm01"><a href="#隔离-ukm01" class="headerlink" title="隔离 ukm01"></a>隔离 ukm01</h3><div class="hljs"><pre><code class="hljs bash"> kubectl drain ukm01 --ignore-daemonsets
node/ukm01 cordoned
node/ukm01 drained</code></pre></div><h3 id="查看可升级版本"><a href="#查看可升级版本" class="headerlink" title="查看可升级版本"></a>查看可升级版本</h3><div class="hljs"><pre><code class="hljs bash"> kubeadm upgrade plan
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.17.3
[upgrade/versions] kubeadm version: v1.18.0
[upgrade/versions] Latest stable version: v1.18.0
[upgrade/versions] Latest stable version: v1.18.0
[upgrade/versions] Latest version <span class="hljs-keyword">in</span> the v1.17 series: v1.17.4
[upgrade/versions] Latest version <span class="hljs-keyword">in</span> the v1.17 series: v1.17.4

Components that must be upgraded manually after you have upgraded the control plane with <span class="hljs-string">'kubeadm upgrade apply'</span>:
COMPONENT   CURRENT       AVAILABLE
Kubelet     9 x v1.17.3   v1.17.4

Upgrade to the latest version <span class="hljs-keyword">in</span> the v1.17 series:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.17.3   v1.17.4
Controller Manager   v1.17.3   v1.17.4
Scheduler            v1.17.3   v1.17.4
Kube Proxy           v1.17.3   v1.17.4
CoreDNS              1.6.5     1.6.7
Etcd                 3.4.3     3.4.3-0

You can now apply the upgrade by executing the following <span class="hljs-built_in">command</span>:

  kubeadm upgrade apply v1.17.4

_____________________________________________________________________

Components that must be upgraded manually after you have upgraded the control plane with <span class="hljs-string">'kubeadm upgrade apply'</span>:
COMPONENT   CURRENT       AVAILABLE
Kubelet     9 x v1.17.3   v1.18.0

Upgrade to the latest stable version:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.17.3   v1.18.0
Controller Manager   v1.17.3   v1.18.0
Scheduler            v1.17.3   v1.18.0
Kube Proxy           v1.17.3   v1.18.0
CoreDNS              1.6.5     1.6.7
Etcd                 3.4.3     3.4.3-0

You can now apply the upgrade by executing the following <span class="hljs-built_in">command</span>:

  kubeadm upgrade apply v1.18.0

_____________________________________________________________________</code></pre></div><h3 id="升级-kubernetes"><a href="#升级-kubernetes" class="headerlink" title="升级 kubernetes"></a>升级 kubernetes</h3><div class="hljs"><pre><code class="hljs bash"> kubeadm upgrade apply v1.18.0
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span>
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade/version] You have chosen to change the cluster version to <span class="hljs-string">"v1.18.0"</span>
[upgrade/versions] Cluster version: v1.17.3
[upgrade/versions] kubeadm version: v1.18.0
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
[upgrade/prepull] Will prepull images <span class="hljs-keyword">for</span> components [kube-apiserver kube-controller-manager kube-scheduler etcd]
[upgrade/prepull] Prepulling image <span class="hljs-keyword">for</span> component etcd.
[upgrade/prepull] Prepulling image <span class="hljs-keyword">for</span> component kube-apiserver.
[upgrade/prepull] Prepulling image <span class="hljs-keyword">for</span> component kube-controller-manager.
[upgrade/prepull] Prepulling image <span class="hljs-keyword">for</span> component kube-scheduler.
[apiclient] Found 0 Pods <span class="hljs-keyword">for</span> label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 3 Pods <span class="hljs-keyword">for</span> label selector k8s-app=upgrade-prepull-kube-controller-manager
[apiclient] Found 3 Pods <span class="hljs-keyword">for</span> label selector k8s-app=upgrade-prepull-kube-apiserver
[apiclient] Found 0 Pods <span class="hljs-keyword">for</span> label selector k8s-app=upgrade-prepull-kube-scheduler
[apiclient] Found 3 Pods <span class="hljs-keyword">for</span> label selector k8s-app=upgrade-prepull-etcd
[apiclient] Found 3 Pods <span class="hljs-keyword">for</span> label selector k8s-app=upgrade-prepull-kube-scheduler
[upgrade/prepull] Prepulled image <span class="hljs-keyword">for</span> component etcd.
[upgrade/prepull] Prepulled image <span class="hljs-keyword">for</span> component kube-scheduler.
[apiclient] Error getting Pods with label selector <span class="hljs-string">"k8s-app=upgrade-prepull-kube-apiserver"</span> [etcdserver: request timed out]
[apiclient] Error getting Pods with label selector <span class="hljs-string">"k8s-app=upgrade-prepull-kube-controller-manager"</span> [etcdserver: request timed out]
[upgrade/prepull] Prepulled image <span class="hljs-keyword">for</span> component kube-controller-manager.
[upgrade/prepull] Prepulled image <span class="hljs-keyword">for</span> component kube-apiserver.
[upgrade/prepull] Successfully prepulled the images <span class="hljs-keyword">for</span> all the control plane components
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version <span class="hljs-string">"v1.18.0"</span>...
Static pod: kube-apiserver-ukm01 <span class="hljs-built_in">hash</span>: 7e7a0d385fce06636ac0fc223274d299
Static pod: kube-controller-manager-ukm01 <span class="hljs-built_in">hash</span>: fdad41767b3b13675192402b9a840a5c
Static pod: kube-scheduler-ukm01 <span class="hljs-built_in">hash</span>: 703c43ab97818f969f780a2cbf4d24b7
[upgrade/etcd] Upgrading to TLS <span class="hljs-keyword">for</span> etcd
[upgrade/etcd] Non fatal issue encountered during upgrade: the desired etcd version <span class="hljs-keyword">for</span> this Kubernetes version <span class="hljs-string">"v1.18.0"</span> is <span class="hljs-string">"3.4.3-0"</span>, but the current etcd version is <span class="hljs-string">"3.4.3"</span>. Won<span class="hljs-string">'t downgrade etcd, instead just continue</span>
<span class="hljs-string">[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests739015179"</span>
<span class="hljs-string">W0403 09:40:19.888673   21690 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"</span>
<span class="hljs-string">[upgrade/staticpods] Preparing for "kube-apiserver" upgrade</span>
<span class="hljs-string">[upgrade/staticpods] Renewing apiserver certificate</span>
<span class="hljs-string">[upgrade/staticpods] Renewing apiserver-kubelet-client certificate</span>
<span class="hljs-string">[upgrade/staticpods] Renewing front-proxy-client certificate</span>
<span class="hljs-string">[upgrade/staticpods] Renewing apiserver-etcd-client certificate</span>
<span class="hljs-string">[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-04-03-09-40-18/kube-apiserver.yaml"</span>
<span class="hljs-string">[upgrade/staticpods] Waiting for the kubelet to restart the component</span>
<span class="hljs-string">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span>
<span class="hljs-string">Static pod: kube-apiserver-ukm01 hash: 7e7a0d385fce06636ac0fc223274d299</span>
<span class="hljs-string">Static pod: kube-apiserver-ukm01 hash: 7e7a0d385fce06636ac0fc223274d299</span>
<span class="hljs-string">Static pod: kube-apiserver-ukm01 hash: 7e7a0d385fce06636ac0fc223274d299</span>
<span class="hljs-string">Static pod: kube-apiserver-ukm01 hash: 7e7a0d385fce06636ac0fc223274d299</span>
<span class="hljs-string">Static pod: kube-apiserver-ukm01 hash: 7e7a0d385fce06636ac0fc223274d299</span>
<span class="hljs-string">Static pod: kube-apiserver-ukm01 hash: 7e7a0d385fce06636ac0fc223274d299</span>
<span class="hljs-string">Static pod: kube-apiserver-ukm01 hash: f9ef3245bc9735275ed5508f8bbea7e1</span>
<span class="hljs-string">[apiclient] Found 3 Pods for label selector component=kube-apiserver</span>
<span class="hljs-string">[upgrade/staticpods] Component "kube-apiserver" upgraded successfully!</span>
<span class="hljs-string">[upgrade/staticpods] Preparing for "kube-controller-manager" upgrade</span>
<span class="hljs-string">[upgrade/staticpods] Renewing controller-manager.conf certificate</span>
<span class="hljs-string">[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-04-03-09-40-18/kube-controller-manager.yaml"</span>
<span class="hljs-string">[upgrade/staticpods] Waiting for the kubelet to restart the component</span>
<span class="hljs-string">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span>
<span class="hljs-string">Static pod: kube-controller-manager-ukm01 hash: fdad41767b3b13675192402b9a840a5c</span>
<span class="hljs-string">Static pod: kube-controller-manager-ukm01 hash: 24e6dad5d81a4c5257a7b29861a48543</span>
<span class="hljs-string">[apiclient] Found 3 Pods for label selector component=kube-controller-manager</span>
<span class="hljs-string">[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!</span>
<span class="hljs-string">[upgrade/staticpods] Preparing for "kube-scheduler" upgrade</span>
<span class="hljs-string">[upgrade/staticpods] Renewing scheduler.conf certificate</span>
<span class="hljs-string">[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-04-03-09-40-18/kube-scheduler.yaml"</span>
<span class="hljs-string">[upgrade/staticpods] Waiting for the kubelet to restart the component</span>
<span class="hljs-string">[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)</span>
<span class="hljs-string">Static pod: kube-scheduler-ukm01 hash: 703c43ab97818f969f780a2cbf4d24b7</span>
<span class="hljs-string">Static pod: kube-scheduler-ukm01 hash: b10b96b094242086df7de02b74f10de2</span>
<span class="hljs-string">[apiclient] Found 3 Pods for label selector component=kube-scheduler</span>
<span class="hljs-string">[upgrade/staticpods] Component "kube-scheduler" upgraded successfully!</span>
<span class="hljs-string">[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace</span>
<span class="hljs-string">[kubelet] Creating a ConfigMap "kubelet-config-1.18" in namespace kube-system with the configuration for the kubelets in the cluster</span>
<span class="hljs-string">[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.18" ConfigMap in the kube-system namespace</span>
<span class="hljs-string">[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"</span>
<span class="hljs-string">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span>
<span class="hljs-string">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span>
<span class="hljs-string">[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span>
<span class="hljs-string">[addons] Applied essential addon: CoreDNS</span>
<span class="hljs-string">[addons] Applied essential addon: kube-proxy</span>
<span class="hljs-string"></span>
<span class="hljs-string">[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.18.0". Enjoy!</span>
<span class="hljs-string"></span>
<span class="hljs-string">[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven'</span>t already <span class="hljs-keyword">done</span> so.</code></pre></div><h3 id="取消隔离"><a href="#取消隔离" class="headerlink" title="取消隔离"></a>取消隔离</h3><div class="hljs"><pre><code class="hljs bash"> kubectl uncordon ukm01
node/ukm01 uncordoned</code></pre></div><h3 id="升级-kubectl-和-kubelet"><a href="#升级-kubectl-和-kubelet" class="headerlink" title="升级 kubectl 和 kubelet"></a>升级 kubectl 和 kubelet</h3><div class="hljs"><pre><code class="hljs bash">apt-mark unhold kubelet kubectl &amp;&amp; \
 apt-get update &amp;&amp; apt-get install -y kubelet=1.18.0-00 kubectl=1.18.0-00 &amp;&amp; \
 apt-mark hold kubelet kubectl

或者
apt-get update &amp;&amp; \
 apt-get install -y --allow-change-held-packages kubelet=1.18.0-00 kubectl=1.18.0-00</code></pre></div><h3 id="升级其他master节点-kubeadm"><a href="#升级其他master节点-kubeadm" class="headerlink" title="升级其他master节点 kubeadm"></a>升级其他master节点 kubeadm</h3><div class="hljs"><pre><code class="hljs bash">ansible master:\!ans -m shell -a <span class="hljs-string">'apt-mark unhold kubeadm &amp;&amp; apt-get update &amp;&amp; apt-get install -y kubeadm=1.18.0-00 &amp;&amp; apt-mark hold kubeadm'</span>
ansible master:\!ans -m shell -a <span class="hljs-string">'apt-mark unhold kubelet kubectl &amp;&amp; apt-get update &amp;&amp; apt-get install -y kubelet=1.18.0-00 kubectl=1.18.0-00 &amp;&amp; apt-mark hold kubelet kubectl'</span>
ansible master:\!ans -m shell -a <span class="hljs-string">'kubeadm version'</span></code></pre></div><h3 id="升级-master-节点-kubernetes"><a href="#升级-master-节点-kubernetes" class="headerlink" title="升级 master 节点 kubernetes"></a>升级 master 节点 kubernetes</h3><div class="hljs"><pre><code class="hljs bash">kubectl drain ukm02 --ignore-daemonsets
ssh ukm02
kubeadm upgrade node
<span class="hljs-built_in">exit</span>
kubectl uncordon ukm02</code></pre></div><h3 id="升级-master-节点-kubectl-和-kubelet"><a href="#升级-master-节点-kubectl-和-kubelet" class="headerlink" title="升级 master 节点 kubectl 和 kubelet"></a>升级 master 节点 kubectl 和 kubelet</h3><div class="hljs"><pre><code class="hljs bash">ansible master -m shell -a <span class="hljs-string">'apt-mark unhold kubelet kubectl &amp;&amp; apt-get update &amp;&amp; apt-get install -y kubelet=1.18.0-00 kubectl=1.18.0-00 &amp;&amp; apt-mark hold kubelet kubectl'</span></code></pre></div><h3 id="重启-master-节点-kubelet"><a href="#重启-master-节点-kubelet" class="headerlink" title="重启 master 节点 kubelet"></a>重启 master 节点 kubelet</h3><div class="hljs"><pre><code class="hljs bash">ansible master -m shell -a <span class="hljs-string">'systemctl restart kubelet'</span></code></pre></div><h3 id="确认升级成功"><a href="#确认升级成功" class="headerlink" title="确认升级成功"></a>确认升级成功</h3><div class="hljs"><pre><code class="hljs bash"> kubectl get nodes
NAME    STATUS   ROLES    AGE   VERSION
uki01   Ready    infra    8d    v1.17.3
uki02   Ready    infra    8d    v1.17.3
uki03   Ready    infra    8d    v1.17.3
ukm01   Ready    master   8d    v1.18.0
ukm02   Ready    master   8d    v1.18.0
ukm03   Ready    master   8d    v1.18.0
ukn01   Ready    worker   8d    v1.17.3
ukn02   Ready    worker   8d    v1.17.3
ukn03   Ready    worker   8d    v1.17.3</code></pre></div><h3 id="升级-infra-和-worker-节点-kubeadm"><a href="#升级-infra-和-worker-节点-kubeadm" class="headerlink" title="升级 infra 和 worker 节点 kubeadm"></a>升级 infra 和 worker 节点 kubeadm</h3><div class="hljs"><pre><code class="hljs bash">ansible infra:worker -m shell -a <span class="hljs-string">'apt-mark unhold kubeadm &amp;&amp; apt-get update &amp;&amp; apt-get install -y kubeadm=1.18.0-00 &amp;&amp; apt-mark hold kubeadm'</span></code></pre></div><h3 id="升级-infra-和-worker-节点-kubernetes"><a href="#升级-infra-和-worker-节点-kubernetes" class="headerlink" title="升级 infra 和 worker 节点 kubernetes"></a>升级 infra 和 worker 节点 kubernetes</h3><div class="hljs"><pre><code class="hljs bash">kubectl drain uki01 --ignore-daemonsets
ssh uki01
kubeadm upgrade node
<span class="hljs-built_in">exit</span>
kubectl uncordon uki01</code></pre></div><h3 id="升级-infra-和-worker-节点-kubectl-和-kubelet"><a href="#升级-infra-和-worker-节点-kubectl-和-kubelet" class="headerlink" title="升级 infra 和 worker 节点 kubectl 和 kubelet"></a>升级 infra 和 worker 节点 kubectl 和 kubelet</h3><div class="hljs"><pre><code class="hljs bash">ansible infra:worker -m shell -a <span class="hljs-string">'apt-mark unhold kubelet kubectl &amp;&amp; apt-get update &amp;&amp; apt-get install -y kubelet=1.18.0-00 kubectl=1.18.0-00 &amp;&amp; apt-mark hold kubelet kubectl'</span></code></pre></div><h3 id="重启-infra-和-worker-节点-kubelet"><a href="#重启-infra-和-worker-节点-kubelet" class="headerlink" title="重启 infra 和 worker 节点 kubelet"></a>重启 infra 和 worker 节点 kubelet</h3><div class="hljs"><pre><code class="hljs bash">ansible infra:worker -m shell -a <span class="hljs-string">'systemctl restart kubelet'</span></code></pre></div><h3 id="检查-kubernetes-升级状态"><a href="#检查-kubernetes-升级状态" class="headerlink" title="检查 kubernetes 升级状态"></a>检查 kubernetes 升级状态</h3><div class="hljs"><pre><code class="hljs bash"> kubectl get nodes
NAME    STATUS   ROLES    AGE   VERSION
uki01   Ready    infra    8d    v1.18.0
uki02   Ready    infra    8d    v1.18.0
uki03   Ready    infra    8d    v1.18.0
ukm01   Ready    master   8d    v1.18.0
ukm02   Ready    master   8d    v1.18.0
ukm03   Ready    master   8d    v1.18.0
ukn01   Ready    worker   8d    v1.18.0
ukn02   Ready    worker   8d    v1.18.0
ukn03   Ready    worker   8d    v1.18.0</code></pre></div><h2 id="安装-router"><a href="#安装-router" class="headerlink" title="安装 router"></a><strong>安装 router</strong></h2><p>router 是 openshift 开源的 ingress 服务，router 服务默认安装在 default namespace 下面。</p><h3 id="配置-rbac-文件"><a href="#配置-rbac-文件" class="headerlink" title="配置 rbac 文件"></a>配置 rbac 文件</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; router/rbac.yaml  &lt;&lt;EOF
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: openshift-router
  namespace: default
rules:
- apiGroups: [<span class="hljs-string">""</span>]
  resources: [<span class="hljs-string">"namespaces"</span>, <span class="hljs-string">"services"</span>, <span class="hljs-string">"endpoints"</span>]
  verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>]
- apiGroups: [<span class="hljs-string">"route.openshift.io"</span>]
  resources: [<span class="hljs-string">"routes"</span>]
  verbs: [<span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>]
- apiGroups: [<span class="hljs-string">"route.openshift.io"</span>]
  resources: [<span class="hljs-string">"routes/status"</span>]
  verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"patch"</span>, <span class="hljs-string">"update"</span>]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: openshift-router
roleRef:
  apiGroup: <span class="hljs-string">""</span>
  kind: ClusterRole
  name: openshift-router
subjects:
- kind: ServiceAccount
  namespace: default
  name: router

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: openshift-router-auth-delegator
roleRef:
  apiGroup: <span class="hljs-string">""</span>
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  namespace: default
  name: router
EOF</code></pre></div><h3 id="配置-serviceaccount-文件"><a href="#配置-serviceaccount-文件" class="headerlink" title="配置 serviceaccount 文件"></a>配置 serviceaccount 文件</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; router/serviceaccount.yaml  &lt;&lt;EOF
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: router
  namespace: default
EOF</code></pre></div><h3 id="配置-crd-文件"><a href="#配置-crd-文件" class="headerlink" title="配置 crd 文件"></a>配置 crd 文件</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; router/crd.yaml  &lt;&lt;EOF
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
   name must match the spec fields below, and be <span class="hljs-keyword">in</span> the form: &lt;plural&gt;.&lt;group&gt;
  name: routes.route.openshift.io
  namespace: default
spec:
   group name to use <span class="hljs-keyword">for</span> REST API: /apis/&lt;group&gt;/&lt;version&gt;
  group: route.openshift.io
   list of versions supported by this CustomResourceDefinition
  versions:
    - name: v1
       Each version can be enabled/disabled by Served flag.
      served: <span class="hljs-literal">true</span>
       One and only one version must be marked as the storage version.
      storage: <span class="hljs-literal">true</span>
   either Namespaced or Cluster
  scope: Namespaced
  subresources:
     <span class="hljs-built_in">enable</span> spec/status
    status: &#123;&#125;
  names:
     plural name to be used <span class="hljs-keyword">in</span> the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;
    plural: routes
     singular name to be used as an <span class="hljs-built_in">alias</span> on the CLI and <span class="hljs-keyword">for</span> display
    singular: route
     kind is normally the CamelCased singular <span class="hljs-built_in">type</span>. Your resource manifests use this.
    kind: Route
  additionalPrinterColumns:
  - name: Host
    <span class="hljs-built_in">type</span>: string
    JSONPath: .status.ingress[0].host
  - name: Admitted
    <span class="hljs-built_in">type</span>: string
    JSONPath: .status.ingress[0].conditions[?(@.<span class="hljs-built_in">type</span>==<span class="hljs-string">"Admitted"</span>)].status
  - name: Service
    <span class="hljs-built_in">type</span>: string
    JSONPath: .spec.to.name
  - name: TLS
    <span class="hljs-built_in">type</span>: string
    JSONPath: .spec.tls.type
EOF</code></pre></div><h3 id="配置-router-安装文件"><a href="#配置-router-安装文件" class="headerlink" title="配置 router 安装文件"></a>配置 router 安装文件</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; router/deployment.yaml  &lt;&lt;EOF
---
apiVersion: apps/v1
kind: DaemonSet  Deployment DaemonSet
metadata:
  name: router
  namespace: default
  labels:
    k8s-app: router
spec:
  replicas: 3  如果是 DaemonSet，可以不设置副本数量。
  selector:
    matchLabels:
      k8s-app: router
  template:
    metadata:
      labels:
        k8s-app: router
    spec:
      serviceAccountName: router
      nodeSelector:
        node-role.kubernetes.io/infra: <span class="hljs-string">"true"</span>
      containers:
      - env:
        - name: ROUTER_LISTEN_ADDR
          value: 0.0.0.0:1936
        - name: ROUTER_METRICS_TYPE
          value: haproxy
        - name: ROUTER_SERVICE_HTTPS_PORT
          value: <span class="hljs-string">"443"</span>
        - name: ROUTER_SERVICE_HTTP_PORT
          value: <span class="hljs-string">"80"</span>
        - name: ROUTER_THREADS
          value: <span class="hljs-string">"4"</span>
        image: openshift/origin-haproxy-router:v4.0.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            host: localhost
            path: /healthz
            port: 1936
          initialDelaySeconds: 10
        name: router
        ports:
        - containerPort: 80
        - containerPort: 443
        - containerPort: 1936
          name: stats
          protocol: TCP
        readinessProbe:
          httpGet:
            host: localhost
            path: healthz/ready
            port: 1936
          initialDelaySeconds: 10
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
      hostNetwork: <span class="hljs-literal">true</span>
EOF</code></pre></div><h3 id="安装-router-服务"><a href="#安装-router-服务" class="headerlink" title="安装 router 服务"></a>安装 router 服务</h3><div class="hljs"><pre><code class="hljs bash">kubectl apply -f router/serviceaccount.yaml

kubectl apply -f router/rbac.yaml

kubectl apply -f router/deployment.yaml

kubectl apply -f router/crd.yaml</code></pre></div><h3 id="检查端口监听"><a href="#检查端口监听" class="headerlink" title="检查端口监听"></a>检查端口监听</h3><p>load blance 配置 TCP 80/443/1936 端口转发到 infra 节点服务器。</p><div class="hljs"><pre><code class="hljs bash">ansible infra -m shell -a <span class="hljs-string">'netstat -lntp|grep -E "80|443|1936"'</span></code></pre></div><h3 id="校验-route-服务"><a href="#校验-route-服务" class="headerlink" title="校验 route 服务"></a>校验 route 服务</h3><p>安装nginx</p><div class="hljs"><pre><code class="hljs bash">kubectl create deployment nginx --image nginx:alpine
kubectl expose deployment nginx --port=80</code></pre></div><h3 id="创建-nginx-route"><a href="#创建-nginx-route" class="headerlink" title="创建 nginx route"></a>创建 nginx route</h3><div class="hljs"><pre><code class="hljs bash"> kubectl apply -f - &lt;&lt;EOF
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  host: test.mokr.cn
  to:
    kind: Service
    name: nginx
    weight: 100
  wildcardPolicy: None
EOF</code></pre></div><h3 id="访问验证"><a href="#访问验证" class="headerlink" title="访问验证"></a>访问验证</h3><div class="hljs"><pre><code class="hljs bash"> curl -ik http://test.mokr.cn --resolve <span class="hljs-string">'test.mokr.cn:80:10.10.34.89'</span>
HTTP/1.1 200 OK
Server: nginx/1.17.9
Date: Tue, 24 Mar 2020 07:01:43 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 03 Mar 2020 17:36:53 GMT
ETag: <span class="hljs-string">"5e5e95b5-264"</span>
Accept-Ranges: bytes
Set-Cookie: 3bc5f7a8aec67b74b33e81d956f57cb9=5ffd533ac952a924e0ec8b6f1c601703; path=/; HttpOnly
Cache-control: private

&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body &#123;
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    &#125;
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=<span class="hljs-string">"http://nginx.org/"</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=<span class="hljs-string">"http://nginx.com/"</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you <span class="hljs-keyword">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;

 或者
 curl -ik http://10.10.34.89 -H <span class="hljs-string">'Host: test.mokr.cn'</span> -v</code></pre></div><h3 id="删除-router"><a href="#删除-router" class="headerlink" title="删除 router"></a>删除 router</h3><div class="hljs"><pre><code class="hljs bash">kubectl delete -f router.yaml</code></pre></div><h2 id="安装-kubernetes-dashboard-2-0"><a href="#安装-kubernetes-dashboard-2-0" class="headerlink" title="安装 kubernetes dashboard 2.0"></a><strong>安装 kubernetes dashboard 2.0</strong></h2><div class="hljs"><pre><code class="hljs bash">curl -L https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml -o kubernetes-dashboard.yaml</code></pre></div><h3 id="签发-https-访问证书"><a href="#签发-https-访问证书" class="headerlink" title="签发 https 访问证书"></a>签发 https 访问证书</h3><ul><li>Can’t load /root/.rnd into RNG</li></ul><div class="hljs"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root
openssl rand -writerand .rnd</code></pre></div><h3 id="dashboard-多域名配置"><a href="#dashboard-多域名配置" class="headerlink" title="dashboard 多域名配置"></a>dashboard 多域名配置</h3><div class="hljs"><pre><code class="hljs bash"> mkdir -p /certs

 cp /etc/ssl/openssl.cnf /certs
 vim /certs/openssl.cnf
...
[ req_distinguished_name ]
countryName                     = Country Name (2 letter code)
countryName_default             = CN
countryName_min                 = 2
countryName_max                 = 2
stateOrProvinceName             = State or Province Name (full name)
stateOrProvinceName_default     = GuangDong
localityName                    = Locality Name (eg, city)
localityName_default            = ShenZhen
0.organizationName              = Organization Name (eg, company)
0.organizationName_default      = MOKR Ltd
organizationalUnitName          = Organizational Unit Name (eg, section)
organizationalUnitName_default  = OPS
commonName                      = Common Name (e.g. server FQDN or YOUR name)
commonName_max                  = 64
emailAddress                    = Email Address
emailAddress_max                = 64
emailAddress_default            = seanzhau@gmail.com

[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = dashboard.mokr.cn
DNS.2 = kubernetes-dashboard.kubernetes-dashboard.svc
DNS.3 = kubernetes-dashboard.kubernetes-dashboard.svc.cluster.local
IP.1 = 10.104.71.164
IP.2 = 10.10.34.89
...</code></pre></div><h3 id="签发-dashboard-密钥"><a href="#签发-dashboard-密钥" class="headerlink" title="签发 dashboard 密钥"></a>签发 dashboard 密钥</h3><div class="hljs"><pre><code class="hljs bash">openssl req -nodes -newkey rsa:2048 \
 -keyout /certs/dashboard.key \
 -out /certs/dashboard.csr \
 -subj <span class="hljs-string">"/C=CN/ST=GuangDong/L=ShenZhen/O=Mokr LTD/OU=OPS"</span> \
 -config /certs/openssl.cnf \
 -extensions v3_req</code></pre></div><h3 id="签发-dashboard-证书"><a href="#签发-dashboard-证书" class="headerlink" title="签发 dashboard 证书"></a>签发 dashboard 证书</h3><div class="hljs"><pre><code class="hljs bash">私钥签发
openssl x509 -req -sha256 -days 365 \
 -<span class="hljs-keyword">in</span> /certs/dashboard.csr \
 -signkey /certs/dashboard.key \
 -out /certs/dashboard.crt \
 -extfile /certs/openssl.cnf \
 -extensions v3_req

ca 签发
openssl x509 -req -days 365 \
 -<span class="hljs-keyword">in</span> /certs/dashboard.csr \
 -CA /etc/kubernetes/pki/ca.crt \
 -CAkey /etc/kubernetes/pki/ca.key \
 -CAcreateserial \
 -out /certs/dashboard.crt \
 -extfile /certs/openssl.cnf \
 -extensions v3_req</code></pre></div><h3 id="验证-dashboard-证书"><a href="#验证-dashboard-证书" class="headerlink" title="验证 dashboard 证书"></a>验证 dashboard 证书</h3><div class="hljs"><pre><code class="hljs bash"> openssl x509 -text -noout -<span class="hljs-keyword">in</span> /certs/dashboard.crt
Certificate:
...
        X509v3 extensions:
            X509v3 Basic Constraints:
                CA:FALSE
            X509v3 Key Usage:
                Digital Signature, Non Repudiation, Key Encipherment
            X509v3 Subject Alternative Name:
                DNS:dashboard.mokr.cn, DNS:kubernetes-dashboard.kubernetes-dashboard.svc, DNS:kubernetes-dashboard.kubernetes-dashboard.svc.cluster.local, IP Address:10.104.71.164, IP Address:10.10.34.89
...</code></pre></div><h3 id="部署-dashboard"><a href="#部署-dashboard" class="headerlink" title="部署 dashboard"></a>部署 dashboard</h3><div class="hljs"><pre><code class="hljs bash">kubectl apply -f kubernetes-dashboard.yaml</code></pre></div><h3 id="为-dashboard-配置-secret-证书"><a href="#为-dashboard-配置-secret-证书" class="headerlink" title="为 dashboard 配置 secret 证书"></a>为 dashboard 配置 secret 证书</h3><div class="hljs"><pre><code class="hljs bash">kubectl create secret generic kubernetes-dashboard-certs --from-file=/certs/dashboard -n kubernetes-dashboard

可不配置
kubectl create secret tls kubernetes-dashboard-tls --cert=/certs/dashboard/dashboard.crt --key=/certs/dashboard/dashboard.key -n kubernetes-dashboard</code></pre></div><h3 id="校验-dashboard"><a href="#校验-dashboard" class="headerlink" title="校验 dashboard"></a>校验 dashboard</h3><div class="hljs"><pre><code class="hljs bash"> kubectl get pods,svc -n kubernetes-dashboard
NAME                                             READY   STATUS    RESTARTS   AGE
pod/dashboard-metrics-scraper-566cddb686-m5dzb   1/1     Running   0          22h
pod/kubernetes-dashboard-7b5bf5d559-dbrh2        1/1     Running   0          22h

NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/dashboard-metrics-scraper   ClusterIP   10.107.240.229   &lt;none&gt;        8000/TCP   22h
service/kubernetes-dashboard        ClusterIP   10.96.188.203    &lt;none&gt;        443/TCP    22h</code></pre></div><h3 id="创建-dashboard-route"><a href="#创建-dashboard-route" class="headerlink" title="创建 dashboard route"></a>创建 dashboard route</h3><div class="hljs"><pre><code class="hljs bash"> kubectl apply -f -  &lt;&lt;EOF
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: kubernetes-dashboard
  labels:
    app: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  host: dashboard.mokr.cn
  tls:
    insecureEdgeTerminationPolicy: Redirect <span class="hljs-comment">## None Allow Redirect</span>
    termination: reencrypt <span class="hljs-comment">## edge passthrough reencrypt</span>
  to:
    kind: Service
    name: kubernetes-dashboard
    weight: 100
  wildcardPolicy: None
EOF</code></pre></div><h3 id="curl-访问-dashboard"><a href="#curl-访问-dashboard" class="headerlink" title="curl 访问 dashboard"></a>curl 访问 dashboard</h3><div class="hljs"><pre><code class="hljs bash">curl -ik https://dashboard.mokr.cn --resolve <span class="hljs-string">'dashboard.mokr.cn:443:10.10.34.89'</span></code></pre></div><h3 id="或者"><a href="#或者" class="headerlink" title="或者"></a>或者</h3><div class="hljs"><pre><code class="hljs bash">curl -ik https://10.10.34.89 -H <span class="hljs-string">'Host: dashboard.mokr.cn'</span></code></pre></div><h3 id="创建-dashboard-用户"><a href="#创建-dashboard-用户" class="headerlink" title="创建 dashboard 用户"></a>创建 dashboard 用户</h3><div class="hljs"><pre><code class="hljs bash"> 创建用户
 kubectl create serviceaccount seanzhau -n kubernetes-dashboard

 添加用户授权，配置集群 admin 权限
<span class="hljs-comment">## ex: kubectl create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 --user=user2 --group=group1</span>
 kubectl create clusterrolebinding dashboard-admin --clusterrole cluster-admin --serviceaccount kubernetes-dashboard:seanzhau

 或者
 kubectl apply -f - &lt;&lt;EOF
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: seanzhau
  namespace: kubernetes-dashboard
  labels:
    kubernetes.io/cluster-service: <span class="hljs-string">"true"</span>
    addonmanager.kubernetes.io/mode: Reconcile

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dashboard-admin
  namespace: kubernetes-dashboard
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: <span class="hljs-string">"true"</span>
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: seanzhau
  namespace: kubernetes-dashboard
EOF</code></pre></div><h3 id="查看-secret-信息"><a href="#查看-secret-信息" class="headerlink" title="查看 secret 信息"></a>查看 secret 信息</h3><div class="hljs"><pre><code class="hljs bash">查看 secret 私钥或者 ca 信息
kubectl -n kubernetes-dashboard get secret -o jsonpath=<span class="hljs-string">'&#123;range .items[?(@.metadata.annotations.kubernetes\.io/service-account\.name=="seanzhau")].data&#125;&#123;.ca\.crt&#125;&#123;end&#125;'</span> | base64 -d

查看用户 token
kubectl -n kubernetes-dashboard get secret -o jsonpath=<span class="hljs-string">'&#123;range .items[?(@.metadata.annotations.kubernetes\.io/service-account\.name=="seanzhau")].data&#125;&#123;.token&#125;&#123;end&#125;'</span> | base64 -d</code></pre></div><h3 id="浏览器访问"><a href="#浏览器访问" class="headerlink" title="浏览器访问"></a>浏览器访问</h3><p>浏览器打开 <code>https://dashboard.mokr.cn</code> ，选择 Token ，输入获取的用户 token。</p><h3 id="使用-kubeconfig-登录"><a href="#使用-kubeconfig-登录" class="headerlink" title="使用 kubeconfig 登录"></a>使用 kubeconfig 登录</h3><p>下载 /etc/kubernetes/admin.conf 到本地，修改配置文件，在最底部添加 token 字段。</p><div class="hljs"><pre><code class="hljs bash"> vim admin.conf
...
users:
- name: kubernetes-admin
  user:
    client-certificate-data: xxxxxxxxxxxxxxxxxxxxx
    client-key-data: xxxxxxxxxxxxxxxxxxxxx
    token: xxxxxxxxxxxxxxxxxxxxx</code></pre></div><h2 id="安装-ceph-存储"><a href="#安装-ceph-存储" class="headerlink" title="安装 ceph 存储"></a><strong>安装 ceph 存储</strong></h2><h3 id="ceph-ansible-安装"><a href="#ceph-ansible-安装" class="headerlink" title="ceph ansible 安装"></a>ceph ansible 安装</h3><div class="hljs"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> -b stable-5.0 https://github.com/ceph/ceph-ansible.git
<span class="hljs-built_in">cd</span> ceph-ansible</code></pre></div><h3 id="安装-pip"><a href="#安装-pip" class="headerlink" title="安装 pip"></a>安装 pip</h3><div class="hljs"><pre><code class="hljs bash">apt install -y python-pip</code></pre></div><h3 id="配置-pip-源"><a href="#配置-pip-源" class="headerlink" title="配置 pip 源"></a>配置 pip 源</h3><div class="hljs"><pre><code class="hljs bash"> mkdir -p ~/.pip
 cat &gt; ~/.pip/pip.conf &lt;&lt;EOF
[global]
index-url = https://mirrors.aliyun.com/pypi/simple/

[install]
trusted-host=mirrors.aliyun.com
EOF</code></pre></div><h3 id="安装-ansible-以及依赖"><a href="#安装-ansible-以及依赖" class="headerlink" title="安装 ansible 以及依赖"></a>安装 ansible 以及依赖</h3><div class="hljs"><pre><code class="hljs bash">pip install -r requirements.txt</code></pre></div><h3 id="配置-ansible-资产文件"><a href="#配置-ansible-资产文件" class="headerlink" title="配置 ansible 资产文件"></a>配置 ansible 资产文件</h3><div class="hljs"><pre><code class="hljs bash"> cat  &gt; dummy-ansible-hosts &lt;&lt;EOF
[mons]
10.10.34.29
10.10.34.31
10.10.34.42

[osds]
10.10.34.29
10.10.34.31
10.10.34.42

[mdss]
10.10.34.29
10.10.34.31
10.10.34.42

[rgws]
10.10.34.29
10.10.34.31
10.10.34.42

[mgrs]
10.10.34.29
10.10.34.31
10.10.34.42

[grafana-server]
10.10.34.29

[clients]
10.10.34.29
10.10.34.31
10.10.34.42
EOF</code></pre></div><h3 id="配置-ceph-安装文件"><a href="#配置-ceph-安装文件" class="headerlink" title="配置 ceph 安装文件"></a>配置 ceph 安装文件</h3><div class="hljs"><pre><code class="hljs bash"> docker 安装
 cp site-container.yml.sample site-container.yml

 二进制文件安装
 cp site.yml.sample site.yml

 cat &gt; group_vars/all.yml &lt;&lt;EOF
---
cluster: ceph
generate_fsid: <span class="hljs-literal">true</span>
journal_size: 512
ceph_origin: repository
ceph_repository: community

 docker 安装
ceph_docker_image: <span class="hljs-string">"ceph/daemon"</span>
ceph_docker_image_tag: latest-octopus
containerized_deployment: <span class="hljs-literal">true</span>
ceph_docker_registry: docker.io
ceph_stable_repo: <span class="hljs-string">"&#123;&#123; ceph_mirror &#125;&#125;/debian-&#123;&#123; ceph_stable_release &#125;&#125;"</span>

 二进制安装
ceph_mirror: http://mirrors.aliyun.com/ceph
ceph_stable_key: http://mirrors.aliyun.com/ceph/keys/release.asc
ceph_stable_release: octopus

public_network: <span class="hljs-string">"10.10.34.0/24"</span>
cluster_network: <span class="hljs-string">"10.10.34.0/24"</span>
monitor_interface: eth0
osd_objectstore: bluestore
dmcrypt: <span class="hljs-literal">false</span>
devices:
  - /dev/vdb
  - /dev/vdc
radosgw_interface: eth0
radosgw_address: 0.0.0.0
radosgw_address_block: <span class="hljs-string">"10.10.34.0/24"</span>
grafana_admin_user: seanzhau
grafana_admin_password: iPaaS2020
dashboard_enabled: True
dashboard_admin_user: seanzhau
dashboard_admin_password: iPaaS2020
EOF</code></pre></div><h3 id="安装-ceph-集群"><a href="#安装-ceph-集群" class="headerlink" title="安装 ceph 集群"></a>安装 ceph 集群</h3><div class="hljs"><pre><code class="hljs bash">docker 安装
ansible-playbook -i dummy-ansible-hosts site-container.yml

二进制文件安装
ansible-playbook -i dummy-ansible-hosts site.yml</code></pre></div><h3 id="非集群节点安装-ceph-客户端"><a href="#非集群节点安装-ceph-客户端" class="headerlink" title="非集群节点安装 ceph 客户端"></a>非集群节点安装 ceph 客户端</h3><div class="hljs"><pre><code class="hljs bash">apt install -y ceph-common</code></pre></div><h3 id="配置客户端"><a href="#配置客户端" class="headerlink" title="配置客户端"></a>配置客户端</h3><div class="hljs"><pre><code class="hljs bash"> 配置 ceph.conf
 cat &gt; /etc/ceph/ceph.conf &lt;&lt;EOF
[global]
mon host = 10.10.34.29,10.10.34.31,10.10.34.42
EOF

 从 ceph 集群复制 admin key
 scp 10.10.34.29:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring</code></pre></div><h3 id="查看集群状态"><a href="#查看集群状态" class="headerlink" title="查看集群状态"></a>查看集群状态</h3><div class="hljs"><pre><code class="hljs bash">ceph -s
 cluster:
   id:     5ae77bc9-1831-42df-8ef2-956499ce685b
   health: HEALTH_WARN
           1 pools have too few placement groups

 services:
   mon: 3 daemons, quorum ceph01,ceph02,ceph03 (age 15m)
   mgr: ceph01(active, since 14m), standbys: ceph02, ceph03
   mds: cephfs:1 &#123;0=ceph01=up:active&#125; 2 up:standby
   osd: 6 osds: 6 up (since 19m), 6 <span class="hljs-keyword">in</span> (since 19m)
   rgw: 3 daemons active (ceph01.rgw0, ceph02.rgw0, ceph03.rgw0)

 task status:
   scrub status:
       mds.ceph01: idle

 data:
   pools:   7 pools, 121 pgs
   objects: 215 objects, 11 KiB
   usage:   6.1 GiB used, 2.9 TiB / 2.9 TiB avail
   pgs:     121 active+clean</code></pre></div><h3 id="查看磁盘状态"><a href="#查看磁盘状态" class="headerlink" title="查看磁盘状态"></a>查看磁盘状态</h3><div class="hljs"><pre><code class="hljs bash"> ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                STATUS  REWEIGHT  PRI-AFF
-1         2.92978  root default
-3         0.97659      host ceph01
 0    hdd  0.48830          osd.0                up   1.00000  1.00000
 3    hdd  0.48830          osd.3                up   1.00000  1.00000
-7         0.97659      host ceph02
 2    hdd  0.48830          osd.2                up   1.00000  1.00000
 5    hdd  0.48830          osd.5                up   1.00000  1.00000
-5         0.97659      host ceph03
 1    hdd  0.48830          osd.1                up   1.00000  1.00000
 4    hdd  0.48830          osd.4                up   1.00000  1.00000</code></pre></div><h3 id="挂载-ceph"><a href="#挂载-ceph" class="headerlink" title="挂载 ceph"></a>挂载 ceph</h3><div class="hljs"><pre><code class="hljs bash"> mount.ceph 10.10.34.29,10.10.34.31,10.10.34.42:/ /mnt/ -o name=admin,secret=admin-password   secretfile=/root/admin.secret

 df -h /mnt/
Filesystem                             Size  Used Avail Use% Mounted on
10.10.34.29,10.10.34.31,10.10.34.42:/  948G     0  948G   0% /mnt</code></pre></div><h3 id="删除集群"><a href="#删除集群" class="headerlink" title="删除集群"></a>删除集群</h3><div class="hljs"><pre><code class="hljs bash">docker 卸载
ansible-playbook -i dummy-ansible-hosts infrastructure-playbooks/purge-container-cluster.yml

二进制文件卸载
ansible-playbook -i dummy-ansible-hosts infrastructure-playbooks/purge-cluster.yml</code></pre></div><h3 id="删除磁盘分区信息"><a href="#删除磁盘分区信息" class="headerlink" title="删除磁盘分区信息"></a>删除磁盘分区信息</h3><p>RuntimeError: Unable to use device, already a member of LVM: /dev/vdb</p><div class="hljs"><pre><code class="hljs bash">ansible -i dummy-ansible-hosts osds -m shell -a <span class="hljs-string">'wipefs -a /dev/vdb --force'</span></code></pre></div><h2 id="配置-provisioner"><a href="#配置-provisioner" class="headerlink" title="配置 provisioner"></a><strong>配置 provisioner</strong></h2><h3 id="下载-ceph-images"><a href="#下载-ceph-images" class="headerlink" title="下载 ceph images"></a>下载 ceph images</h3><div class="hljs"><pre><code class="hljs bash">docker pull quay.io/external_storage/rbd-provisioner:latest
docker pull quay.io/external_storage/cephfs-provisioner:latest</code></pre></div><h3 id="创建-ceph-namespace"><a href="#创建-ceph-namespace" class="headerlink" title="创建 ceph namespace"></a>创建 ceph namespace</h3><div class="hljs"><pre><code class="hljs bash">kubectl create namespace ceph</code></pre></div><h3 id="安装-cephfs"><a href="#安装-cephfs" class="headerlink" title="安装 cephfs"></a>安装 cephfs</h3><h4 id="生成-cephfs-admin-密码"><a href="#生成-cephfs-admin-密码" class="headerlink" title="生成 cephfs admin 密码"></a>生成 cephfs admin 密码</h4><div class="hljs"><pre><code class="hljs bash">ceph auth get-key client.admin &gt; /tmp/key</code></pre></div><h4 id="创建-cephfs-secret"><a href="#创建-cephfs-secret" class="headerlink" title="创建 cephfs secret"></a>创建 cephfs secret</h4><div class="hljs"><pre><code class="hljs bash">kubectl create secret generic cephfs-secret-admin --from-file=/tmp/key --namespace=ceph --<span class="hljs-built_in">type</span>=kubernetes.io/cephfs</code></pre></div><h4 id="创建-cephfs-安装文件"><a href="#创建-cephfs-安装文件" class="headerlink" title="创建 cephfs 安装文件"></a>创建 cephfs 安装文件</h4><div class="hljs"><pre><code class="hljs bash">mkdir -p storageClass/cephfs</code></pre></div><h3 id="重新构建-image"><a href="#重新构建-image" class="headerlink" title="重新构建 image"></a>重新构建 image</h3><p>如果有权限问题，可以按操作重新制造 image。</p><div class="hljs"><pre><code class="hljs bash"> cat &gt; storageClass/cephfs/Dockerfile &lt;&lt;EOF
FROM quay.io/external_storage/cephfs-provisioner:latest

USER root

RUN sed -i <span class="hljs-string">'s|0o755|0o777|g'</span> /usr/lib/python2.7/site-packages/ceph_volume_client.py
EOF

 docker build -t quay.io/external_storage/cephfs-provisioner:new storageClass/cephfs/</code></pre></div><h4 id="创建-cephfs-deployment-安装文件"><a href="#创建-cephfs-deployment-安装文件" class="headerlink" title="创建 cephfs deployment 安装文件"></a>创建 cephfs deployment 安装文件</h4><div class="hljs"><pre><code class="hljs bash"> cat &gt; storageClass/cephfs/deployment.yaml &lt;&lt;EOF
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cephfs-provisioner
  namespace: ceph
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cephfs-provisioner
  strategy:
    <span class="hljs-built_in">type</span>: Recreate
  template:
    metadata:
      labels:
        app: cephfs-provisioner
    spec:
      nodeSelector:
        node-role.kubernetes.io/infra: <span class="hljs-string">"true"</span>
      containers:
      - name: cephfs-provisioner
        image: <span class="hljs-string">"quay.io/external_storage/cephfs-provisioner:latest"</span>
        imagePullPolicy: IfNotPresent
        env:
        - name: PROVISIONER_NAME
          value: ceph.com/cephfs
        - name: PROVISIONER_SECRET_NAMESPACE
          value: ceph
        <span class="hljs-built_in">command</span>:
        - <span class="hljs-string">"/usr/local/bin/cephfs-provisioner"</span>
        args:
        - <span class="hljs-string">"-id=cephfs-provisioner-1"</span>
      serviceAccount: cephfs-provisioner
EOF</code></pre></div><h4 id="创建-cephfs-serviceaccount-安装文件"><a href="#创建-cephfs-serviceaccount-安装文件" class="headerlink" title="创建 cephfs serviceaccount 安装文件"></a>创建 cephfs serviceaccount 安装文件</h4><div class="hljs"><pre><code class="hljs bash"> cat &gt; storageClass/cephfs/serviceaccount.yaml &lt;&lt;EOF
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cephfs-provisioner
  namespace: ceph
EOF</code></pre></div><h4 id="创建-cephfs-rbac-安装文件"><a href="#创建-cephfs-rbac-安装文件" class="headerlink" title="创建 cephfs rbac 安装文件"></a>创建 cephfs rbac 安装文件</h4><div class="hljs"><pre><code class="hljs bash"> cat &gt; storageClass/cephfs/rbac.yaml &lt;&lt;EOF
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cephfs-provisioner
  namespace: ceph
rules:
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"secrets"</span>]
    verbs: [<span class="hljs-string">"create"</span>, <span class="hljs-string">"get"</span>, <span class="hljs-string">"delete"</span>]
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"endpoints"</span>]
    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"create"</span>, <span class="hljs-string">"update"</span>, <span class="hljs-string">"patch"</span>]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cephfs-provisioner
  namespace: ceph
subjects:
- kind: ServiceAccount
  name: cephfs-provisioner
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cephfs-provisioner

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cephfs-provisioner
  namespace: ceph
rules:
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"persistentvolumes"</span>]
    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"create"</span>, <span class="hljs-string">"delete"</span>]
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"persistentvolumeclaims"</span>]
    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"update"</span>]
  - apiGroups: [<span class="hljs-string">"storage.k8s.io"</span>]
    resources: [<span class="hljs-string">"storageclasses"</span>]
    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>]
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"events"</span>]
    verbs: [<span class="hljs-string">"create"</span>, <span class="hljs-string">"update"</span>, <span class="hljs-string">"patch"</span>]
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"services"</span>]
    resourceNames: [<span class="hljs-string">"kube-dns"</span>,<span class="hljs-string">"coredns"</span>]
    verbs: [<span class="hljs-string">"list"</span>, <span class="hljs-string">"get"</span>]
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"endpoints"</span>]
    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"create"</span>, <span class="hljs-string">"update"</span>, <span class="hljs-string">"patch"</span>]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cephfs-provisioner
subjects:
  - kind: ServiceAccount
    name: cephfs-provisioner
    namespace: ceph
roleRef:
  kind: ClusterRole
  name: cephfs-provisioner
  apiGroup: rbac.authorization.k8s.io
EOF</code></pre></div><h4 id="创建-cephfs-storageclass-安装文件"><a href="#创建-cephfs-storageclass-安装文件" class="headerlink" title="创建 cephfs storageclass 安装文件"></a>创建 cephfs storageclass 安装文件</h4><div class="hljs"><pre><code class="hljs bash"> cat &gt; storageClass/cephfs/storageclass.yaml &lt;&lt;EOF
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: cephfs-provisioner
provisioner: ceph.com/cephfs
parameters:
    monitors: <span class="hljs-string">"10.10.34.29:6789,10.10.34.31:6789,10.10.34.42:6789"</span>
    adminId: admin
    adminSecretName: cephfs-secret-admin
    adminSecretNamespace: ceph
    claimRoot: /storage
EOF</code></pre></div><h4 id="安装-cephfs-provisioner"><a href="#安装-cephfs-provisioner" class="headerlink" title="安装 cephfs provisioner"></a>安装 cephfs provisioner</h4><div class="hljs"><pre><code class="hljs bash">kubectl apply -f storageClass/cephfs/serviceaccount.yaml

kubectl apply -f storageClass/cephfs/rbac.yaml

kubectl apply -f storageClass/cephfs/deployment.yaml

kubectl apply -f storageClass/cephfs/storageclass.yaml</code></pre></div><h4 id="查看-cephfs-provisioner"><a href="#查看-cephfs-provisioner" class="headerlink" title="查看 cephfs provisioner"></a>查看 cephfs provisioner</h4><div class="hljs"><pre><code class="hljs bash"> kubectl get storageclasses cephfs-provisioner
NAME                 PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
cephfs-provisioner   ceph.com/cephfs   Delete          Immediate           <span class="hljs-literal">false</span>                  17s</code></pre></div><h4 id="创建-cephfs-PVC"><a href="#创建-cephfs-PVC" class="headerlink" title="创建 cephfs PVC"></a>创建 cephfs PVC</h4><div class="hljs"><pre><code class="hljs bash"> cat &gt; storageClass/cephfs/pvc.yaml &lt;&lt;EOF
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cephfs-test
spec:
  storageClassName: cephfs-provisioner
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
EOF

 kubectl apply -f storageClass/cephfs/pvc.yaml -n kube-system
 kubectl apply -f storageClass/cephfs/pvc.yaml -n ceph
 kubectl apply -f storageClass/cephfs/pvc.yaml -n default</code></pre></div><h4 id="验证-cephfs-PVC"><a href="#验证-cephfs-PVC" class="headerlink" title="验证 cephfs PVC"></a>验证 cephfs PVC</h4><div class="hljs"><pre><code class="hljs bash"> kubectl get pvc --all-namespaces
NAMESPACE     NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE
ceph          cephfs-test   Bound    pvc-ae5ff9f8-4032-4690-9104-eeb563d45a1e   5Gi        RWX            cephfs-provisioner   8s
default       cephfs-test   Bound    pvc-85d47294-50a0-4f6c-b9ca-7a676cf3c34e   5Gi        RWX            cephfs-provisioner   5s
kube-system   cephfs-test   Bound    pvc-759c1b48-df22-4b72-ab22-490a2f3ebd5c   5Gi        RWX            cephfs-provisioner   13s</code></pre></div><h3 id="创建-rbd"><a href="#创建-rbd" class="headerlink" title="创建 rbd"></a>创建 rbd</h3><h4 id="生成-rbd-admin-密码"><a href="#生成-rbd-admin-密码" class="headerlink" title="生成 rbd admin 密码"></a>生成 rbd admin 密码</h4><div class="hljs"><pre><code class="hljs bash">ceph auth get-key client.admin &gt; /tmp/key</code></pre></div><h4 id="创建-admin-secret"><a href="#创建-admin-secret" class="headerlink" title="创建 admin secret"></a>创建 admin secret</h4><div class="hljs"><pre><code class="hljs bash">kubectl create secret generic rbd-secret-admin  --from-file=/tmp/key --namespace=ceph --<span class="hljs-built_in">type</span>=kubernetes.io/rbd</code></pre></div><h4 id="创建-pool-secret"><a href="#创建-pool-secret" class="headerlink" title="创建 pool secret"></a>创建 pool secret</h4><div class="hljs"><pre><code class="hljs bash">ceph osd pool create kube 8 8

ceph auth add client.kube mon <span class="hljs-string">'allow r'</span> osd <span class="hljs-string">'allow rwx pool=kube'</span>

ceph auth get-key client.kube &gt; /tmp/key

kubectl create secret generic kube-secret --from-file=/tmp/key --namespace=ceph --<span class="hljs-built_in">type</span>=kubernetes.io/rbd</code></pre></div><h4 id="创建-rbd-目录"><a href="#创建-rbd-目录" class="headerlink" title="创建 rbd 目录"></a>创建 rbd 目录</h4><div class="hljs"><pre><code class="hljs bash">mkdir -p storageClass/rbd</code></pre></div><h4 id="创建-rbd-deployment-安装文件"><a href="#创建-rbd-deployment-安装文件" class="headerlink" title="创建 rbd deployment 安装文件"></a>创建 rbd deployment 安装文件</h4><div class="hljs"><pre><code class="hljs bash"> cat &gt; storageClass/rbd/deployment.yaml &lt;&lt;EOF
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rbd-provisioner
  namespace: ceph
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rbd-provisioner
  strategy:
    <span class="hljs-built_in">type</span>: Recreate
  template:
    metadata:
      labels:
        app: rbd-provisioner
    spec:
      nodeSelector:
        node-role.kubernetes.io/infra: <span class="hljs-string">"true"</span>
      containers:
      - name: rbd-provisioner
        image: <span class="hljs-string">"quay.io/external_storage/rbd-provisioner:latest"</span>
        imagePullPolicy: IfNotPresent
        env:
        - name: PROVISIONER_NAME
          value: ceph.com/rbd
      serviceAccount: rbd-provisioner
EOF</code></pre></div><h4 id="创建-rbd-serviceaccount-安装文件"><a href="#创建-rbd-serviceaccount-安装文件" class="headerlink" title="创建 rbd serviceaccount 安装文件"></a>创建 rbd serviceaccount 安装文件</h4><div class="hljs"><pre><code class="hljs bash"> cat &gt; storageClass/rbd/serviceaccount.yaml &lt;&lt;EOF
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rbd-provisioner
  namespace: ceph
EOF</code></pre></div><h4 id="创建-rbd-rbac-安装文件"><a href="#创建-rbd-rbac-安装文件" class="headerlink" title="创建 rbd rbac 安装文件"></a>创建 rbd rbac 安装文件</h4><div class="hljs"><pre><code class="hljs bash"> cat &gt; storageClass/rbd/rbac.yaml &lt;&lt;EOF
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: rbd-provisioner
  namespace: ceph
rules:
- apiGroups: [<span class="hljs-string">""</span>]
  resources: [<span class="hljs-string">"secrets"</span>]
  verbs: [<span class="hljs-string">"get"</span>]
- apiGroups: [<span class="hljs-string">""</span>]
  resources: [<span class="hljs-string">"endpoints"</span>]
  verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"create"</span>, <span class="hljs-string">"update"</span>, <span class="hljs-string">"patch"</span>]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rbd-provisioner
  namespace: ceph
subjects:
- kind: ServiceAccount
  name: rbd-provisioner
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rbd-provisioner

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rbd-provisioner
  namespace: ceph
rules:
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"persistentvolumes"</span>]
    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"create"</span>, <span class="hljs-string">"delete"</span>]
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"persistentvolumeclaims"</span>]
    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"update"</span>]
  - apiGroups: [<span class="hljs-string">"storage.k8s.io"</span>]
    resources: [<span class="hljs-string">"storageclasses"</span>]
    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>]
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"events"</span>]
    verbs: [<span class="hljs-string">"create"</span>, <span class="hljs-string">"update"</span>, <span class="hljs-string">"patch"</span>]
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"services"</span>]
    resourceNames: [<span class="hljs-string">"kube-dns"</span>,<span class="hljs-string">"coredns"</span>]
    verbs: [<span class="hljs-string">"list"</span>, <span class="hljs-string">"get"</span>]
  - apiGroups: [<span class="hljs-string">""</span>]
    resources: [<span class="hljs-string">"endpoints"</span>]
    verbs: [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"create"</span>, <span class="hljs-string">"update"</span>, <span class="hljs-string">"patch"</span>]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: rbd-provisioner
subjects:
  - kind: ServiceAccount
    name: rbd-provisioner
    namespace: ceph
roleRef:
  kind: ClusterRole
  name: rbd-provisioner
  apiGroup: rbac.authorization.k8s.io
EOF</code></pre></div><h4 id="创建-rbd-storageclass-安装文件"><a href="#创建-rbd-storageclass-安装文件" class="headerlink" title="创建 rbd storageclass 安装文件"></a>创建 rbd storageclass 安装文件</h4><div class="hljs"><pre><code class="hljs bash"> cat &gt; storageClass/rbd/storageclass.yaml &lt;&lt;EOF
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rbd-provisioner
provisioner: ceph.com/rbd
parameters:
  monitors: <span class="hljs-string">"10.10.34.29:6789,10.10.34.31:6789,10.10.34.42:6789"</span>
  pool: kube
  adminId: admin
  adminSecretNamespace: ceph
  adminSecretName: rbd-secret-admin
  userId: kube
  userSecretNamespace: ceph
  userSecretName: ceph-secret
  imageFormat: <span class="hljs-string">"2"</span>
  imageFeatures: layering
EOF</code></pre></div><h4 id="安装-rbd-provisioner"><a href="#安装-rbd-provisioner" class="headerlink" title="安装 rbd provisioner"></a>安装 rbd provisioner</h4><div class="hljs"><pre><code class="hljs bash">kubectl apply -f storageClass/rbd/serviceaccount.yaml

kubectl apply -f storageClass/rbd/rbac.yaml

kubectl apply -f storageClass/rbd/deployment.yaml

kubectl apply -f storageClass/rbd/storageclass.yaml</code></pre></div><h4 id="查看-rbd-provisioner"><a href="#查看-rbd-provisioner" class="headerlink" title="查看 rbd provisioner"></a>查看 rbd provisioner</h4><div class="hljs"><pre><code class="hljs bash"> kubectl get storageclasses rbd-provisioner
NAME              PROVISIONER    RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
rbd-provisioner   ceph.com/rbd   Delete          Immediate           <span class="hljs-literal">false</span>                  7s</code></pre></div><h4 id="创建-rbd-PVC"><a href="#创建-rbd-PVC" class="headerlink" title="创建 rbd PVC"></a>创建 rbd PVC</h4><div class="hljs"><pre><code class="hljs bash"> cat &gt; storageClass/rbd/pvc.yaml &lt;&lt;EOF
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-test
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: rbd-provisioner
  resources:
    requests:
      storage: 10Gi
EOF

 kubectl apply -f storageClass/rbd/pvc.yaml -n kube-system
 kubectl apply -f storageClass/rbd/pvc.yaml -n ceph
 kubectl apply -f storageClass/rbd/pvc.yaml -n default</code></pre></div><h4 id="验证-rbd-PVC"><a href="#验证-rbd-PVC" class="headerlink" title="验证 rbd PVC"></a>验证 rbd PVC</h4><div class="hljs"><pre><code class="hljs bash"> kubectl get pvc --all-namespaces
NAMESPACE     NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE
ceph          rbd-test      Bound    pvc-4d85ad48-b24c-43b6-a7c2-89aec9c661a7   10Gi       RWO            rbd-provisioner      19s
default       rbd-test      Bound    pvc-9d93ccbd-b6ed-42fe-8f57-78fbff88baeb   10Gi       RWO            rbd-provisioner      11s
kube-system   rbd-test      Bound    pvc-5887166f-d9b4-4d40-b419-2e37ac81901d   10Gi       RWO            rbd-provisioner      31s</code></pre></div><h3 id="对象存储"><a href="#对象存储" class="headerlink" title="对象存储"></a>对象存储</h3><h4 id="s3-存储"><a href="#s3-存储" class="headerlink" title="s3 存储"></a>s3 存储</h4><div class="hljs"><pre><code class="hljs bash"> 配置 ceph s3 账号，保留 access_key 和 secret_key
 radosgw-admin user create --uid=<span class="hljs-string">"testuser"</span> --display-name=<span class="hljs-string">"test user"</span>
&#123;
    <span class="hljs-string">"user_id"</span>: <span class="hljs-string">"testuser"</span>,
    <span class="hljs-string">"display_name"</span>: <span class="hljs-string">"test user"</span>,
    <span class="hljs-string">"email"</span>: <span class="hljs-string">""</span>,
    <span class="hljs-string">"suspended"</span>: 0,
    <span class="hljs-string">"max_buckets"</span>: 1000,
    <span class="hljs-string">"subusers"</span>: [],
    <span class="hljs-string">"keys"</span>: [
        &#123;
            <span class="hljs-string">"user"</span>: <span class="hljs-string">"testuser"</span>,
            <span class="hljs-string">"access_key"</span>: <span class="hljs-string">"60DD3SXN58LIRC06ILQE"</span>,
            <span class="hljs-string">"secret_key"</span>: <span class="hljs-string">"zgFnP3lL3uPF3WsCeQtnESKvgPYHnW75cSJfkYeZ"</span>
        &#125;
    ],
    <span class="hljs-string">"swift_keys"</span>: [],
    <span class="hljs-string">"caps"</span>: [],
    <span class="hljs-string">"op_mask"</span>: <span class="hljs-string">"read, write, delete"</span>,
    <span class="hljs-string">"default_placement"</span>: <span class="hljs-string">""</span>,
    <span class="hljs-string">"default_storage_class"</span>: <span class="hljs-string">""</span>,
    <span class="hljs-string">"placement_tags"</span>: [],
    <span class="hljs-string">"bucket_quota"</span>: &#123;
        <span class="hljs-string">"enabled"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"check_on_raw"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"max_size"</span>: -1,
        <span class="hljs-string">"max_size_kb"</span>: 0,
        <span class="hljs-string">"max_objects"</span>: -1
    &#125;,
    <span class="hljs-string">"user_quota"</span>: &#123;
        <span class="hljs-string">"enabled"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"check_on_raw"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"max_size"</span>: -1,
        <span class="hljs-string">"max_size_kb"</span>: 0,
        <span class="hljs-string">"max_objects"</span>: -1
    &#125;,
    <span class="hljs-string">"temp_url_keys"</span>: [],
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"rgw"</span>,
    <span class="hljs-string">"mfa_ids"</span>: []
&#125;

 安装 s3 python 库
 pip install boto

 创建 s3 测试文件，使用上面创建的账号。
 cat &gt; s3.py &lt;&lt;EOF
from boto.s3.connection import S3Connection
from boto.s3.connection import OrdinaryCallingFormat


 add radosgw user <span class="hljs-keyword">in</span> ceph cluster
 radosgw-admin user create --uid=rgwuser --subuser=rgwuser:swift --display-name=s3 --access=full

conn = S3Connection(aws_access_key_id=<span class="hljs-string">'60DD3SXN58LIRC06ILQE'</span>,
                    aws_secret_access_key=<span class="hljs-string">'zgFnP3lL3uPF3WsCeQtnESKvgPYHnW75cSJfkYeZ'</span>,
                    host=<span class="hljs-string">'10.10.34.29'</span>,
                    port=8080,
                    is_secure=False,
                    calling_format=OrdinaryCallingFormat())

create_bucket = conn.create_bucket(<span class="hljs-string">'cephfs'</span>)

buckets_list = conn.get_all_buckets()
<span class="hljs-keyword">for</span> bucket <span class="hljs-keyword">in</span> buckets_list:
     <span class="hljs-built_in">print</span>(<span class="hljs-string">'%s\t%s'</span> % (bucket.name, bucket.creation_date))

delete_bucket = conn.delete_bucket(<span class="hljs-string">'cephfs'</span>)
EOF

 测试验证
 python s3.py
cephfs  2020-04-10T08:16:15.647Z</code></pre></div><h4 id="swift"><a href="#swift" class="headerlink" title="swift"></a>swift</h4><div class="hljs"><pre><code class="hljs bash"> 创建账号
 radosgw-admin subuser create --uid=testuser --subuser=testuser:swift --access=full
&#123;
    <span class="hljs-string">"user_id"</span>: <span class="hljs-string">"testuser"</span>,
    <span class="hljs-string">"display_name"</span>: <span class="hljs-string">"test user"</span>,
    <span class="hljs-string">"email"</span>: <span class="hljs-string">""</span>,
    <span class="hljs-string">"suspended"</span>: 0,
    <span class="hljs-string">"max_buckets"</span>: 1000,
    <span class="hljs-string">"subusers"</span>: [
        &#123;
            <span class="hljs-string">"id"</span>: <span class="hljs-string">"testuser:swift"</span>,
            <span class="hljs-string">"permissions"</span>: <span class="hljs-string">"full-control"</span>
        &#125;
    ],
    <span class="hljs-string">"keys"</span>: [
        &#123;
            <span class="hljs-string">"user"</span>: <span class="hljs-string">"testuser"</span>,
            <span class="hljs-string">"access_key"</span>: <span class="hljs-string">"60DD3SXN58LIRC06ILQE"</span>,
            <span class="hljs-string">"secret_key"</span>: <span class="hljs-string">"zgFnP3lL3uPF3WsCeQtnESKvgPYHnW75cSJfkYeZ"</span>
        &#125;
    ],
    <span class="hljs-string">"swift_keys"</span>: [
        &#123;
            <span class="hljs-string">"user"</span>: <span class="hljs-string">"testuser:swift"</span>,
            <span class="hljs-string">"secret_key"</span>: <span class="hljs-string">"b59Aw59eiStnhibukAm7fN3vQRtTU2BwNio2dHnl"</span>
        &#125;
    ],
    <span class="hljs-string">"caps"</span>: [],
    <span class="hljs-string">"op_mask"</span>: <span class="hljs-string">"read, write, delete"</span>,
    <span class="hljs-string">"default_placement"</span>: <span class="hljs-string">""</span>,
    <span class="hljs-string">"default_storage_class"</span>: <span class="hljs-string">""</span>,
    <span class="hljs-string">"placement_tags"</span>: [],
    <span class="hljs-string">"bucket_quota"</span>: &#123;
        <span class="hljs-string">"enabled"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"check_on_raw"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"max_size"</span>: -1,
        <span class="hljs-string">"max_size_kb"</span>: 0,
        <span class="hljs-string">"max_objects"</span>: -1
    &#125;,
    <span class="hljs-string">"user_quota"</span>: &#123;
        <span class="hljs-string">"enabled"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"check_on_raw"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"max_size"</span>: -1,
        <span class="hljs-string">"max_size_kb"</span>: 0,
        <span class="hljs-string">"max_objects"</span>: -1
    &#125;,
    <span class="hljs-string">"temp_url_keys"</span>: [],
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"rgw"</span>,
    <span class="hljs-string">"mfa_ids"</span>: []
&#125;

 重置密码
 radosgw-admin key create --subuser=testuser:swift --key-type=swift --gen-secret

 安装 swift 客户端
 pip install python-swiftclient

 测试
 swift -A http://10.10.34.29:8080/auth/1.0 -U testuser:swift -K <span class="hljs-string">'b59Aw59eiStnhibukAm7fN3vQRtTU2BwNio2dHnl'</span> list</code></pre></div><h2 id="安装-istio"><a href="#安装-istio" class="headerlink" title="安装 istio"></a><strong>安装 istio</strong></h2><h3 id="使用-demo-安装"><a href="#使用-demo-安装" class="headerlink" title="使用 demo 安装"></a>使用 demo 安装</h3><ul><li>参考 <code>https://istio.io/docs/setup/additional-setup/config-profiles/</code></li></ul><div class="hljs"><pre><code class="hljs bash">istioctl manifest apply --<span class="hljs-built_in">set</span> profile=demo --<span class="hljs-built_in">set</span> values.gateways.istio-ingressgateway.type=NodePort</code></pre></div><h3 id="自定义参数安装"><a href="#自定义参数安装" class="headerlink" title="自定义参数安装"></a>自定义参数安装</h3><ul><li>参数 <code>https://istio.io/zh/docs/reference/config/installation-options/</code></li></ul><div class="hljs"><pre><code class="hljs bash">istioctl manifest apply \
 --namespace istio-system \
 --<span class="hljs-built_in">set</span> values.global.hub=docker.io/istio \
 --<span class="hljs-built_in">set</span> values.global.tag=1.5.0 \
 --<span class="hljs-built_in">set</span> values.global.logging.level=<span class="hljs-string">"default:info"</span> \
 --<span class="hljs-built_in">set</span> values.global.imagePullPolicy=IfNotPresent \
 --<span class="hljs-built_in">set</span> values.global.k8sIngress.enabled=<span class="hljs-literal">false</span> \
 --<span class="hljs-built_in">set</span> values.global.k8sIngress.enableHttps=<span class="hljs-literal">false</span> \
 --<span class="hljs-built_in">set</span> values.global.proxy.image=proxyv2 \
 --<span class="hljs-built_in">set</span> values.global.proxy.clusterDomain=<span class="hljs-string">"cluster.local"</span> \
 --<span class="hljs-built_in">set</span> values.global.proxy.privileged=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.global.proxy.enableCoreDump=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.global.proxy.autoInject=enabled \
 --<span class="hljs-built_in">set</span> values.global.proxy.tracer=zipkin \
 --<span class="hljs-built_in">set</span> values.global.controlPlaneSecurityEnabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.global.mtls.enabled=<span class="hljs-literal">false</span> \
 --<span class="hljs-built_in">set</span> values.global.outboundTrafficPolicy.mode=ALLOW_ANY \
 --<span class="hljs-built_in">set</span> values.global.sds.enabled=<span class="hljs-literal">false</span> \
 --<span class="hljs-built_in">set</span> values.certmanager.enabled=<span class="hljs-literal">false</span> \
 --<span class="hljs-built_in">set</span> values.galley.enabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.gateways.enabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.gateways.istio-ingressgateway.enabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.gateways.istio-ingressgateway.type=NodePort  NodePort, ClusterIP or LoadBalancer \
 --<span class="hljs-built_in">set</span> values.gateways.istio-egressgateway.enabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.grafana.enabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.istio_cni.enabled=<span class="hljs-literal">false</span> \
 --<span class="hljs-built_in">set</span> values.istiocoredns.enabled=<span class="hljs-literal">false</span> \
 --<span class="hljs-built_in">set</span> values.kiali.enabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.kiali.createDemoSecret=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.kiali.hub=docker.io/kiali \
 --<span class="hljs-built_in">set</span> values.mixer.policy.enabled=<span class="hljs-literal">false</span> \
 --<span class="hljs-built_in">set</span> values.mixer.policy.autoscaleEnabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.nodeagent.enabled=<span class="hljs-literal">false</span> \
 --<span class="hljs-built_in">set</span> values.pilot.enabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.pilot.sidecar=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.prometheus.enabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.security.enabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.sidecarInjectorWebhook.enabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.sidecarInjectorWebhook.enableNamespacesByDefault=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.tracing.enabled=<span class="hljs-literal">true</span> \
 --<span class="hljs-built_in">set</span> values.tracing.jaeger.hub=docker.io/jaegertracing \
 --<span class="hljs-built_in">set</span> values.tracing.jaeger.tag=1.14</code></pre></div><h3 id="查看-ingress-端口监听情况"><a href="#查看-ingress-端口监听情况" class="headerlink" title="查看 ingress 端口监听情况"></a>查看 ingress 端口监听情况</h3><div class="hljs"><pre><code class="hljs bash"> kubectl get svc -n istio-system | grep ingress
istio-ingressgateway        NodePort    10.105.16.129    &lt;none&gt;        15020:30796/TCP,80:32179/TCP,443:31133/TCP,15029:31338/TCP,15030:30980/TCP,15031:31675/TCP,15032:32296/TCP,31400:31821/TCP,15443:32679/TCP   30s

 ansible ins -m shell -a <span class="hljs-string">'netstat -lntp | grep -E "32179|31133"'</span>
10.10.34.92 | CHANGED | rc=0 &gt;&gt;
tcp6       0      0 :::32179                :::*                    LISTEN      21780/kube-proxy
tcp6       0      0 :::31133                :::*                    LISTEN      21780/kube-proxy
10.10.34.94 | CHANGED | rc=0 &gt;&gt;
tcp6       0      0 :::32179                :::*                    LISTEN      3093/kube-proxy
tcp6       0      0 :::31133                :::*                    LISTEN      3093/kube-proxy
10.10.34.100 | CHANGED | rc=0 &gt;&gt;
tcp6       0      0 :::32179                :::*                    LISTEN      14998/kube-proxy
tcp6       0      0 :::31133                :::*                    LISTEN      14998/kube-proxy
10.10.34.99 | CHANGED | rc=0 &gt;&gt;
tcp6       0      0 :::31133                :::*                    LISTEN      3031/kube-proxy
tcp6       0      0 :::32179                :::*                    LISTEN      3031/kube-proxy
10.10.34.95 | CHANGED | rc=0 &gt;&gt;
tcp6       0      0 :::32179                :::*                    LISTEN      23912/kube-proxy
tcp6       0      0 :::31133                :::*                    LISTEN      23912/kube-proxy
10.10.34.96 | CHANGED | rc=0 &gt;&gt;
tcp6       0      0 :::32179                :::*                    LISTEN      26951/kube-proxy
tcp6       0      0 :::31133                :::*                    LISTEN      26951/kube-proxy
10.10.34.93 | CHANGED | rc=0 &gt;&gt;
tcp6       0      0 :::31133                :::*                    LISTEN      31622/kube-proxy
tcp6       0      0 :::32179                :::*                    LISTEN      31622/kube-proxy
10.10.34.97 | CHANGED | rc=0 &gt;&gt;
tcp6       0      0 :::32179                :::*                    LISTEN      4244/kube-proxy
tcp6       0      0 :::31133                :::*                    LISTEN      4244/kube-proxy
10.10.34.98 | CHANGED | rc=0 &gt;&gt;
tcp6       0      0 :::31133                :::*                    LISTEN      8077/kube-proxy
tcp6       0      0 :::32179                :::*                    LISTEN      8077/kube-proxy</code></pre></div><h3 id="配置-load-blance"><a href="#配置-load-blance" class="headerlink" title="配置 load blance"></a>配置 load blance</h3><ul><li>LB 80 端口的 TCP 协议请求转发到集群 node 节点的 32179 端口</li><li>LB 443 端口的 TCP 协议请求转发到集群 node 节点的 31133 端口</li></ul><h3 id="应用测试"><a href="#应用测试" class="headerlink" title="应用测试"></a>应用测试</h3><p>创建 namespace 并打开自动注入</p><div class="hljs"><pre><code class="hljs bash">kubectl create namespace book

kubectl label namespace book istio-injection=enabled</code></pre></div><h3 id="创建-demo"><a href="#创建-demo" class="headerlink" title="创建 demo"></a>创建 demo</h3><div class="hljs"><pre><code class="hljs bash">kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -n book

kubectl get services -n book

kubectl get pods -n book

kubectl <span class="hljs-built_in">exec</span> -n book \
 -it $(kubectl get pod  -n book -l app=ratings -o jsonpath=<span class="hljs-string">'&#123;.items[0].metadata.name&#125;'</span>) -c ratings \
 -- curl productpage:9080/productpage | grep -o <span class="hljs-string">"&lt;title&gt;.*&lt;/title&gt;"</span></code></pre></div><h3 id="创建-http-gateway"><a href="#创建-http-gateway" class="headerlink" title="创建 http gateway"></a>创建 http gateway</h3><div class="hljs"><pre><code class="hljs bash"> kubectl apply -f - &lt;&lt;EOF
--
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: book
  namespace: book
spec:
  hosts:
  - <span class="hljs-string">"book.mokr.cn"</span>
  gateways:
  - istio-system/ingressgateway
  http:
  - match:
    - uri:
        exact: /productpage
    - uri:
        prefix: /static
    - uri:
        exact: /login
    - uri:
        exact: /<span class="hljs-built_in">logout</span>
    - uri:
        prefix: /api/v1/products
    route:
    - destination:
        host: productpage
        port:
          number: 9080
EOF</code></pre></div><h3 id="验证-http-gateway"><a href="#验证-http-gateway" class="headerlink" title="验证 http gateway"></a>验证 http gateway</h3><div class="hljs"><pre><code class="hljs bash">curl -ik http://book.mokr.cn --resolve <span class="hljs-string">'book.mokr.cn:443:10.10.34.89'</span> -v</code></pre></div><h3 id="创建-https-gateway"><a href="#创建-https-gateway" class="headerlink" title="创建 https gateway"></a>创建 https gateway</h3><div class="hljs"><pre><code class="hljs bash"> 创建根证书
 openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout mokr.cn.key -out mokr.cn.crt -subj <span class="hljs-string">"/C=CN/ST=Guangdong/L=Shenzhen/O=mokr/OU=ops/CN=mokr.cn"</span>

 创建域名 key
 openssl req -out book.mokr.cn.csr -newkey rsa:2048 -nodes -keyout book.mokr.cn.key -subj <span class="hljs-string">"/C=CN/ST=Guangdong/L=Shenzhen/O=mokr/OU=ops/CN=book.mokr.cn"</span>

 创建域名证书
 openssl x509 -req -days 365 -CA mokr.cn.crt -CAkey mokr.cn.key -set_serial 0 -<span class="hljs-keyword">in</span> book.mokr.cn.csr -out book.mokr.cn.crt

 创建 tls secret
 kubectl create secret tls ingressgateway-book-certs --key book.mokr.cn.key --cert book.mokr.cn.crt  -n istio-system

 创建 https gateway
 kubectl apply -f - &lt;&lt;EOF
--
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: book-gateway
  namespace: istio-system
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - book.mokr.cn
    tls:
      httpsRedirect: <span class="hljs-literal">true</span>
  - port:
      number: 443
      name: https
      protocol: HTTPS   HTTP | HTTPS | GRPC | HTTP2 | MONGO | TCP | TLS
    tls:
      mode: SIMPLE    PASSTHROUGH | SIMPLE | MUTUAL | AUTO_PASSTHROUGH | ISTIO_MUTUAL
      credentialName: ingressgateway-book-certs
    hosts:
    - book.mokr.cn
EOF

 创建 https VirtualService
 kubectl apply -f - &lt;&lt;EOF
--
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: book
  namespace: book
spec:
  hosts:
  - <span class="hljs-string">"book.mokr.cn"</span>
  gateways:
  - istio-system/book-gateway
  http:
  - match:
    - uri:
        exact: /productpage
    - uri:
        prefix: /static
    - uri:
        exact: /login
    - uri:
        exact: /<span class="hljs-built_in">logout</span>
    - uri:
        prefix: /api/v1/products
    route:
    - destination:
        host: productpage
        port:
          number: 9080
EOF</code></pre></div><h3 id="查看安装情况"><a href="#查看安装情况" class="headerlink" title="查看安装情况"></a>查看安装情况</h3><div class="hljs"><pre><code class="hljs bash"> kubectl get gateway -n book
NAME               AGE
bookinfo-gateway   23s

 kubectl get virtualservice -n book
NAME       GATEWAYS             HOSTS                       AGE
bookinfo   [bookinfo-gateway]   [book.mokr.cn]   36s</code></pre></div><h3 id="验证-https-gateway"><a href="#验证-https-gateway" class="headerlink" title="验证 https gateway"></a>验证 https gateway</h3><div class="hljs"><pre><code class="hljs bash">curl -ik https://book.mokr.cn --resolve <span class="hljs-string">'book.mokr.cn:443:10.10.34.89'</span> -v</code></pre></div><h3 id="删除-demo"><a href="#删除-demo" class="headerlink" title="删除 demo"></a>删除 demo</h3><div class="hljs"><pre><code class="hljs bash">kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml -n book

kubectl delete namespace book --force --grace-period=0</code></pre></div><h3 id="删除-istioctl-集群"><a href="#删除-istioctl-集群" class="headerlink" title="删除 istioctl 集群"></a>删除 istioctl 集群</h3><div class="hljs"><pre><code class="hljs bash">istioctl manifest generate | kubectl delete -f -</code></pre></div><h2 id="部署-registry"><a href="#部署-registry" class="headerlink" title="部署 registry"></a>部署 registry</h2><h3 id="创建-pvc-部署文件"><a href="#创建-pvc-部署文件" class="headerlink" title="创建 pvc 部署文件"></a>创建 pvc 部署文件</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; registry/pvc.yaml &lt;&lt;EOF
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: registry-storage
spec:
  storageClassName: cephfs-provisioner
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
EOF</code></pre></div><h3 id="创建-service-部署文件"><a href="#创建-service-部署文件" class="headerlink" title="创建 service 部署文件"></a>创建 service 部署文件</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; registry/service.yaml &lt;&lt;EOF
---
apiVersion: v1
kind: Service
metadata:
  name: docker-registry
  namespace: default
spec:
  ports:
  - port: 5000
    protocol: TCP
    targetPort: 5000
  selector:
    k8s-app: docker-registry
  sessionAffinity: None
  <span class="hljs-built_in">type</span>: ClusterIP
EOF</code></pre></div><h3 id="创建-registry-部署文件"><a href="#创建-registry-部署文件" class="headerlink" title="创建 registry 部署文件"></a>创建 registry 部署文件</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; registry/deployment.yaml &lt;&lt;EOF
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    k8s-app: docker-registry
  name: docker-registry
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: docker-registry
  template:
    metadata:
      labels:
        k8s-app: docker-registry
    spec:
      nodeSelector:
        node-role.kubernetes.io/infra: <span class="hljs-string">"true"</span>
      containers:
      - env:
        - name: REGISTRY_HTTP_ADDR
          value: :5000
        - name: REGISTRY_HTTP_TLS_CERTIFICATE
          value: /etc/secrets/tls.crt
        - name: REGISTRY_HTTP_TLS_KEY
          value: /etc/secrets/tls.key
        - name: REGISTRY_STORAGE_DELETE_ENABLED
          value: <span class="hljs-string">"true"</span>
        name: registry
        image: registry:2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 5000
          protocol: TCP
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
        volumeMounts:
        - name: registry-data
          mountPath: /var/lib/registry
        - name: registry-certificates
          mountPath: /etc/secrets
      serviceAccountName: docker-registry
      volumes:
      - name: registry-data
        persistentVolumeClaim:
          claimName: registry-storage
      - name: registry-certificates
        secret:
          defaultMode: 420
          secretName: registry-certificates
EOF</code></pre></div><h3 id="创建-service"><a href="#创建-service" class="headerlink" title="创建 service"></a>创建 service</h3><div class="hljs"><pre><code class="hljs bash">kubectl apply -f registry/service.yaml

kubectl get svc docker-registry -o custom-columns=clusterIP:.spec.clusterIP</code></pre></div><h3 id="配置多域名"><a href="#配置多域名" class="headerlink" title="配置多域名"></a>配置多域名</h3><div class="hljs"><pre><code class="hljs bash"> mkdir -p registry/

 cp /etc/ssl/openssl.cnf registry/
 vim registry/openssl.cnf
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = docker-registry.mokr.cn
DNS.2 = docker-registry.default.svc
DNS.3 = docker-registry.default.svc.cluster.local</code></pre></div><h3 id="签发-registry-密钥"><a href="#签发-registry-密钥" class="headerlink" title="签发 registry 密钥"></a>签发 registry 密钥</h3><div class="hljs"><pre><code class="hljs bash">openssl req -nodes -newkey rsa:2048 \
 -keyout registry/docker-registry.key \
 -out registry/docker-registry.csr \
 -subj <span class="hljs-string">"/C=CN/ST=GuangDong/L=ShenZhen/O=Mokr LTD/OU=OPS"</span> \
 -config registry/openssl.cnf \
 -extensions v3_req</code></pre></div><h3 id="签发-registry-证书"><a href="#签发-registry-证书" class="headerlink" title="签发 registry 证书"></a>签发 registry 证书</h3><div class="hljs"><pre><code class="hljs bash">私钥签发
openssl x509 -req -sha256 -days 365 \
 -<span class="hljs-keyword">in</span> registry/docker-registry.csr \
 -signkey registry/docker-registry.key \
 -out registry/docker-registry.crt \
 -extfile registry/openssl.cnf \
 -extensions v3_req

ca 签发
openssl x509 -req -days 365 \
 -<span class="hljs-keyword">in</span> registry/docker-registry.csr \
 -CA /etc/kubernetes/pki/ca.crt \
 -CAkey /etc/kubernetes/pki/ca.key \
 -CAcreateserial \
 -out registry/docker-registry.crt \
 -extfile registry/openssl.cnf \
 -extensions v3_req</code></pre></div><h3 id="验证-registry-证书"><a href="#验证-registry-证书" class="headerlink" title="验证 registry 证书"></a>验证 registry 证书</h3><div class="hljs"><pre><code class="hljs bash"> openssl x509 -text -noout -<span class="hljs-keyword">in</span> registry/docker-registry.crt
Certificate:
...
        X509v3 extensions:
            X509v3 Basic Constraints:
                CA:FALSE
            X509v3 Key Usage:
                Digital Signature, Non Repudiation, Key Encipherment
            X509v3 Subject Alternative Name:
                DNS:docker-registry.mokr.cn, DNS:docker-registry.default.svc, DNS:docker-registry.default.svc.cluster.local
...</code></pre></div><h3 id="为-registry-配置-secret-证书"><a href="#为-registry-配置-secret-证书" class="headerlink" title="为 registry 配置 secret 证书"></a>为 registry 配置 secret 证书</h3><div class="hljs"><pre><code class="hljs bash">kubectl create secret tls registry-certificates --cert=registry/docker-registry.crt --key=registry/docker-registry.key</code></pre></div><h3 id="创建-registry"><a href="#创建-registry" class="headerlink" title="创建 registry"></a>创建 registry</h3><div class="hljs"><pre><code class="hljs bash">kubectl apply -f registry/pvc.yaml

kubectl apply -f registry/deployment.yaml</code></pre></div><h3 id="创建-registry-route"><a href="#创建-registry-route" class="headerlink" title="创建 registry route"></a>创建 registry route</h3><div class="hljs"><pre><code class="hljs bash"> cat &gt; registry/docker-registry-route.yaml &lt;&lt;EOF
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: docker-registry
  labels:
    app: docker-registry
spec:
  host: docker-registry.mokr.cn
  tls:
    insecureEdgeTerminationPolicy: Redirect <span class="hljs-comment">## None Allow Redirect</span>
    termination: passthrough <span class="hljs-comment">## edge passthrough reencrypt</span>
  to:
    kind: Service
    name: docker-registry
    weight: 100
  wildcardPolicy: None
EOF

 kubectl apply -f registry/docker-registry-route.yaml</code></pre></div><h3 id="配置-docker"><a href="#配置-docker" class="headerlink" title="配置 docker"></a>配置 docker</h3><div class="hljs"><pre><code class="hljs bash"> vim /etc/docker/daemon.json
&#123;
...
  <span class="hljs-string">"registry-mirrors"</span>: [
    <span class="hljs-string">"https://docker-registry.mokr.cn"</span>,
    <span class="hljs-string">"https://docker-registry.default.svc:5000"</span>,
    <span class="hljs-string">"https://docker-registry.default.svc.cluster.local:5000"</span>
  ],
  <span class="hljs-string">"insecure-registries"</span>: [
    <span class="hljs-string">"docker-registry.mokr.cn"</span>,
    <span class="hljs-string">"docker-registry.default.svc:5000"</span>,
    <span class="hljs-string">"docker-registry.default.svc.cluster.local:5000"</span>
  ],
...
&#125;

 systemctl restart docker

 docker info
...
 Insecure Registries:
  docker-registry.default.svc.cluster.local:5000
  docker-registry.default.svc:5000
  docker-registry.mokr.cn
  127.0.0.0/8
 Registry Mirrors:
  https://docker-registry.mokr.cn/
  https://docker-registry.default.svc:5000/
  https://docker-registry.default.svc.cluster.local:5000/
 Live Restore Enabled: <span class="hljs-literal">false</span></code></pre></div><h3 id="下载-image-测试"><a href="#下载-image-测试" class="headerlink" title="下载 image 测试"></a>下载 image 测试</h3><div class="hljs"><pre><code class="hljs bash">docker pull busybox:latest

docker tag busybox:latest docker-registry.default.svc:5000/busybox:latest
docker tag busybox:latest docker-registry.default.svc.cluster.local:5000/busybox:latest
docker tag busybox:latest docker-registry.mokr.cn/default/busybox:latest

docker push docker-registry.default.svc:5000/busybox:latest
docker push docker-registry.default.svc.cluster.local:5000/busybox:latest
docker push docker-registry.mokr.cn/default/busybox:latest</code></pre></div><h3 id="验证-registry"><a href="#验证-registry" class="headerlink" title="验证 registry"></a>验证 registry</h3><div class="hljs"><pre><code class="hljs bash"> curl -ik https://docker-registry.mokr.cn/v2/_catalog --resolve <span class="hljs-string">'docker-registry.mokr.cn:443:10.10.34.89'</span> -v
* Added docker-registry.mokr.cn:443:10.10.34.89 to DNS cache
* Hostname docker-registry.mokr.cn was found <span class="hljs-keyword">in</span> DNS cache
*   Trying 10.10.34.89...
* TCP_NODELAY <span class="hljs-built_in">set</span>
* Connected to docker-registry.mokr.cn (10.10.34.89) port 443 (0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully <span class="hljs-built_in">set</span> certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256
* ALPN, server accepted to use h2
* Server certificate:
*  subject: C=CN; ST=GuangDong; L=ShenZhen; O=Mokr LTD; OU=OPS
*  start date: Apr 11 15:05:20 2020 GMT
*  expire date: Apr 11 15:05:20 2021 GMT
*  issuer: CN=kubernetes
*  SSL certificate verify result: unable to get <span class="hljs-built_in">local</span> issuer certificate (20), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data <span class="hljs-keyword">in</span> stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x556a671559a0)
&gt; GET /v2/_catalog HTTP/2
&gt; Host: docker-registry.mokr.cn
&gt; User-Agent: curl/7.58.0
&gt; Accept: */*
&gt;
* Connection state changed (MAX_CONCURRENT_STREAMS updated)!
&lt; HTTP/2 200
HTTP/2 200
&lt; content-type: application/json; charset=utf-8
content-type: application/json; charset=utf-8
&lt; docker-distribution-api-version: registry/2.0
docker-distribution-api-version: registry/2.0
&lt; x-content-type-options: nosniff
x-content-type-options: nosniff
&lt; content-length: 47
content-length: 47
&lt; date: Sat, 11 Apr 2020 15:57:22 GMT
date: Sat, 11 Apr 2020 15:57:22 GMT

&lt;
&#123;<span class="hljs-string">"repositories"</span>:[<span class="hljs-string">"busybox"</span>,<span class="hljs-string">"default/busybox"</span>]&#125;
* Connection 0 to host docker-registry.mokr.cn left intact</code></pre></div><h3 id="清理-registry-images"><a href="#清理-registry-images" class="headerlink" title="清理 registry images"></a>清理 registry images</h3><div class="hljs"><pre><code class="hljs bash"> 登录 docker-registry
 kubectl <span class="hljs-built_in">exec</span> -it $(kubectl get pods -l k8s-app=docker-registry -o jsonpath=&#123;.items[0].metadata.name&#125;) sh

/  registry garbage-collect /etc/docker/registry/config.yml --delete-untagged=<span class="hljs-literal">true</span></code></pre></div><h2 id="安装部署-cert-manager"><a href="#安装部署-cert-manager" class="headerlink" title="安装部署 cert-manager"></a>安装部署 cert-manager</h2><h3 id="下载安装脚本"><a href="#下载安装脚本" class="headerlink" title="下载安装脚本"></a>下载安装脚本</h3><div class="hljs"><pre><code class="hljs bash">mkdir -p cert-manager

curl -L https://github.com/jetstack/cert-manager/releases/download/v0.14.1/cert-manager.yaml -o cert-manager/cert-manager.yaml

sed -i <span class="hljs-string">'s/namespace: "cert-manager"/namespace: kube-system/g'</span> cert-manager/cert-manager.yaml

sed -i <span class="hljs-string">'s/namespace: cert-manager/namespace: kube-system/g'</span> cert-manager/cert-manager.yaml

sed -i <span class="hljs-string">'s/secret: "cert-manager/secret: kube-system/g'</span> cert-manager/cert-manager.yaml

sed -i <span class="hljs-string">'s/secret: cert-manager/secret: kube-system/g'</span> cert-manager/cert-manager.yaml

注释 5878 - 5882 行，关闭
sed -i <span class="hljs-string">'5878,5882s/^//g'</span> cert-manager/cert-manager.yaml</code></pre></div><h3 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h3><div class="hljs"><pre><code class="hljs bash">kubectl apply -f cert-manager/cert-manager.yaml</code></pre></div><h3 id="检查安装"><a href="#检查安装" class="headerlink" title="检查安装"></a>检查安装</h3><div class="hljs"><pre><code class="hljs bash"> kubectl get pods -n kube-system | grep cert-manager
cert-manager-557f8d6944-zr5jh              1/1     Running   0          2m1s
cert-manager-cainjector-6999bdd97-nfsrn    1/1     Running   0          2m1s
cert-manager-webhook-c579c9f47-g6rd5       1/1     Running   0          2m1s

 kubectl get crd -o custom-columns=crd-name:.metadata.name|grep cert-manager
certificaterequests.cert-manager.io
certificates.cert-manager.io
challenges.acme.cert-manager.io
clusterissuers.cert-manager.io
issuers.cert-manager.io
orders.acme.cert-manager.io</code></pre></div></article><hr><div><div class="post-metas mb-3"></div><p class="note note-warning">本博客所有文章除特别声明外，均采用: <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际协议</a>，转载请保留原文链接及作者。</p><div class="post-prevnext row"><div class="post-prev col-6"><a href="/post/66cb4f6f.html"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Ansible简明指北</span> <span class="visible-mobile">上一篇</span></a></div><div class="post-next col-6"><a href="/post/c21ba7a3.html"><span class="hidden-mobile">ansible 部署zk、kafka、nacos集群</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></div></div></div><div class="comments" id="comments"><div id="vcomments"></div><script defer src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="/js/Valine.min.js"></script><script type="text/javascript">var notify=!1,verify=!1,oldLoadVa=window.onload;window.onload=function(){oldLoadVa&&oldLoadVa(),new Valine({lang:"zh-cn",el:"#vcomments",notify:notify,verify:verify,app_id:"dXhxgFWthuLPDnYYuAJbIe6X-MdYXbMMI",app_key:"In8P9esI1pt8dBUc7JbFCvLa",placeholder:"说点什么",avatar:"/retro",meta:["nick","mail","link"],pageSize:"10",emoticon_url:"/emoji",emoticon_list:["动耳朵.gif","抖脚脚.gif","抖眼.gif","抖眼镜.gif","风吹秀发.gif","慌张.gif","夹住.gif","开车.gif","哭唧唧.gif","绿色的.gif","跳舞.gif","跳着走.gif","小花花.gif","眼睛转.gif","摇头.gif","眨眼.gif","吃东西.gif","弹肚皮.gif","动次打次.gif","比心心.gif","吹风.gif","打篮球.gif","都是小心心.gif","吐.png","喷血.png","狂汗.png","不说话.png","汗.png","坐等.png","献花.png","不高兴.png","中刀.png","害羞.png","皱眉.png","小眼睛.png","中指.png","尴尬.png","瞅你.png","想一想.png","中枪.png","得意.png","肿包.png","扇耳光.png","亲亲.png","惊喜.png","脸红.png","无所谓.png","便便.png","愤怒.png","蜡烛.png","献黄瓜.png","内伤.png","投降.png","观察.png","看不见.png","击掌.png","抠鼻.png","邪恶.png","看热闹.png","口水.png","抽烟.png","锁眉.png","装大款.png","吐舌.png","无奈.png","长草.png","赞一个.png","呲牙.png","无语.png","阴暗.png","不出所料.png","咽气.png","期待.png","高兴.png","吐血倒地.png","哭泣.png","欢呼.png","黑线.png","喜极而泣.png","喷水.png","深思.png","鼓掌.png","暗地观察.png"]})}</script></div></div></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div id="tocbot"></div></div></div></div></div><div class="col-lg-7 mx-auto nopadding-md"><div class="container custom post-content mx-auto"><img src="/images/wechatpay.webp" class="rounded mx-auto d-block mt-5" style="width:120px;height:150px;vertical-align:-webkit-baseline-middle"><br><div style="text-align:center">买个卤蛋，吃根冰棒</div></div></div></main><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div><footer class="mt-5"><div class="text-center py-3"><div><svg t="1586793095516" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3553" width="16" height="16"><path d="M512 0c282.7776 0 512 229.2224 512 512s-229.2224 512-512 512S0 794.7776 0 512 229.2224 0 512 0z m0 71.424a440.576 440.576 0 1 0 0 881.152 440.576 440.576 0 0 0 0-881.152z m8.192 166.7072c55.296 0 108.3904 15.5136 153.7536 44.2368a35.7376 35.7376 0 0 1-38.144 60.3648 215.6032 215.6032 0 0 0-115.5584-33.1264c-116.6336 0-210.6368 90.88-210.6368 202.3936s94.0032 202.3936 210.6368 202.3936c41.0112 0 80.2304-11.264 113.8688-32.1024a35.7376 35.7376 0 0 1 37.632 60.7744 287.1296 287.1296 0 0 1-151.552 42.8032c-155.4944 0-282.0608-122.368-282.0608-273.8688 0-151.552 126.5664-273.8688 282.112-273.8688z" fill="#8a8a8a" p-id="3554"></path></svg> <span itemprop="copyrightYear" id="cori">2021</span><br><svg t="1584375698087" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1913" width="23" height="14"><path d="M513.984496 513.984496c141.383442 0 256-114.616558 256-256S655.367938 1.984496 513.984496 1.984496 257.984496 116.601054 257.984496 257.984496s114.616558 256 256 256z m0-59.534884c-108.50431 0-196.465116-87.960806-196.465116-196.465116s87.960806-196.465116 196.465116-196.465116 196.465116 87.960806 196.465116 196.465116-87.960806 196.465116-196.465116 196.465116z" fill="#8a8a8a" p-id="1914"></path><path d="M1020.031008 992.248062c0-281.647628-227.423256-510.015504-508.031008-510.015504S3.968992 710.600434 3.968992 992.248062a29.767442 29.767442 0 1 0 59.534884 0c0-248.820093 200.827039-450.48062 448.496124-450.48062s448.496124 201.660527 448.496124 450.48062a29.767442 29.767442 0 1 0 59.534884 0z" fill="#8a8a8a" p-id="1915"></path></svg> <span class="author" id="cori" itemprop="copyrightHolder">Bryce Huang</span></div></div><script type="text/javascript" src="/js/commentTyping.min.js"></script></footer><script src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js"></script><script src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js"></script><script src="/js/main.js"></script><script src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js"></script><script>$(document).ready(function(){var t=$("#board-ctn").offset().top;tocbot.init({tocSelector:"#tocbot",contentSelector:".post-content",headingSelector:"h1,h2,h3,h4,h5,h6",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",collapseDepth:6,scrollSmooth:!0,headingsOffset:-t}),0<$(".toc-list-item").length&&$("#toc").css("visibility","visible")})</script><script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js"></script><script src="/js/clipboard-use.js"></script><script src="/js/custom.js"></script><script src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js"></script><script>var typed=new Typed("#subtitle",{strings:["  ","kubernetes 安装&nbsp;"],cursorChar:"_",typeSpeed:70,loop:!1});typed.stop(),$(document).ready(function(){$(".typed-cursor").addClass("h2"),typed.start()})</script><script src="/js/local-search.js"></script><script>var path="/local-search.xml",inputArea=document.querySelector("#local-search-input");inputArea.onclick=function(){searchFunc(path,"local-search-input","local-search-result"),this.onclick=null}</script><script src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js"></script><link rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css"><script>$("#post img:not(.no-zoom img, img[no-zoom]), img[zoom]").each(function(){var t=document.createElement("a");$(t).attr("data-fancybox","images"),$(t).attr("href",$(this).attr("src")),$(this).wrap(t)})</script></body></html>